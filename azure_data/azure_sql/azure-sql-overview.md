# Azure SQL - Complete Overview

## Table of Contents

- [Introduction](#introduction)
- [Azure SQL Product Family](#azure-sql-product-family)
- [Azure SQL Database](#azure-sql-database)
  - [Deployment Options](#deployment-options)
  - [Purchasing Models](#purchasing-models)
  - [Service Tiers](#service-tiers)
  - [Compute Tiers](#compute-tiers)
  - [High Availability and Zone Redundancy](#high-availability-and-zone-redundancy)
  - [Multi-Region Write Support](#multi-region-write-support)
  - [Geo-Replication and Failover Options](#geo-replication-and-failover-options)
- [Azure SQL Managed Instance](#azure-sql-managed-instance)
- [SQL Server on Azure VMs](#sql-server-on-azure-vms)
  - [High Availability Options for SQL Server on Azure VMs](#high-availability-options-for-sql-server-on-azure-vms)
  - [Network Name Options: VNN vs DNN](#network-name-options-vnn-vs-dnn)
  - [Disk Caching Best Practices](#disk-caching-best-practices-for-sql-server-on-azure-vms)
- [Pricing Tiers Comparison](#pricing-tiers-comparison)
- [Feature Comparison](#feature-comparison)
- [Decision Guide](#decision-guide)
- [Best Practices](#best-practices)
  - [Security Best Practices](#security-best-practices)
    - [Always Encrypted](#always-encrypted-column-level-encryption)
    - [Transparent Data Encryption (TDE)](#transparent-data-encryption-tde)
    - [Dynamic Data Masking](#dynamic-data-masking)
- [Migration to Azure SQL](#migration-to-azure-sql)
- [Common Scenarios and Recommendations](#common-scenarios-and-recommendations)
- [Key Insights for Exams](#key-insights-for-exams)
- [References](#references)

> üìñ **For comprehensive migration information**, see the dedicated **[Azure SQL Migration Guide](./azure-sql-migration-guide.md)**

## Introduction

**Azure SQL** is a family of managed SQL Server database services running in the Azure cloud. It provides a range of deployment options from fully managed PaaS databases to infrastructure-as-a-service (IaaS) virtual machines, catering to different migration scenarios, workload requirements, and management preferences.

### Key Benefits

- **Fully Managed**: Automated patching, backups, and high availability
- **Built-in Intelligence**: Performance recommendations and automatic tuning
- **Scalability**: Scale up/down or scale out based on demand
- **Security**: Built-in advanced security features
- **Cost Optimization**: Pay only for what you use with flexible pricing models
- **High Availability**: Built-in 99.99%+ SLA

## Azure SQL Product Family

Azure SQL consists of three main products:

| Product | Type | Best For | Management Level |
|---------|------|----------|------------------|
| **Azure SQL Database** | PaaS | Modern cloud applications | Fully managed |
| **Azure SQL Managed Instance** | PaaS | Lift-and-shift migrations | Nearly 100% SQL Server compatibility |
| **SQL Server on Azure VMs** | IaaS | Full SQL Server control | Self-managed |

### Product Selection Flow

```
Need full SQL Server control? 
‚îî‚îÄ YES ‚Üí SQL Server on Azure VMs
‚îî‚îÄ NO ‚Üí Need instance-scoped features?
    ‚îî‚îÄ YES ‚Üí SQL Managed Instance
    ‚îî‚îÄ NO ‚Üí SQL Database
```

## Azure SQL Database

**Azure SQL Database** is a fully managed platform as a service (PaaS) database engine that handles database management functions including upgrading, patching, backups, and monitoring without user involvement.

### Deployment Options

#### 1. Single Database
**Isolated database with dedicated resources**

- Fully isolated resources
- Independent scaling and management
- Best for modern cloud applications
- Supports serverless compute tier
- Per-second billing available

**Use Cases**:
- Modern SaaS applications
- Microservices architectures
- Development/test environments
- Applications with predictable or unpredictable workloads (serverless)

#### 2. Elastic Pool
**Collection of databases sharing a set of resources**

- Shared resource pool across multiple databases
- Cost-effective for multiple databases
- Pool-level resource management
- Databases can have varying usage patterns

**Use Cases**:
- Multi-tenant SaaS applications
- Multiple databases with complementary usage patterns
- Cost optimization across database portfolio
- Unpredictable usage per database, but predictable aggregate

### Purchasing Models

Azure SQL Database offers two purchasing models:

#### vCore-Based Model (Recommended)

**Virtual Core (vCore)** model provides granular control over compute and storage resources.

**Characteristics**:
- Choose number of vCores (virtual cores)
- Choose memory (tied to vCore count)
- Choose storage independently
- Choose generation (Gen5, newer hardware)
- Better cost optimization
- Hybrid Benefit eligibility

**Components**:
```
Compute (vCores) + Storage (GB) + Backup Storage = Total Cost
```

**vCore Options**:
- 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 24, 32, 40, 64, 80, 128 vCores
- Memory: ~5.1 GB per vCore (Gen5)

#### DTU-Based Model

**Database Transaction Unit (DTU)** is a blended measure of compute, memory, and I/O.

**Characteristics**:
- Simplified pricing - bundled resources
- Fixed ratios of CPU, memory, I/O
- Less flexibility
- Easier to understand for beginners
- No Hybrid Benefit

**DTU Tiers**:
- Basic: 5 DTUs
- Standard: 10, 20, 50, 100, 200, 400, 800, 1600, 3000 DTUs
- Premium: 125, 250, 500, 1000, 1750, 4000 DTUs

### Service Tiers

Azure SQL Database offers three service tiers with different performance and pricing characteristics:

#### General Purpose (Recommended for most workloads)

**Budget-oriented balanced compute and storage**

**Characteristics**:
- Standard availability: 99.99% SLA
- Remote storage (Azure Premium Storage)
- Storage: 1 GB - 4 TB (depends on vCore)
- Latency: 5-10ms
- Backup retention: 7-35 days

**Pricing (vCore)**:
- 2 vCores: ~$360/month
- 4 vCores: ~$720/month
- 8 vCores: ~$1,440/month

**Use Cases**:
- Most business workloads
- Development and test
- Web applications
- Small to medium-sized databases

#### Business Critical (High performance requirements)

**Highest performance and availability**

**Characteristics**:
- Premium availability: 99.99% SLA (99.995% zone-redundant)
- Local SSD storage
- Storage: 1 GB - 4 TB
- Latency: <2ms
- Built-in read replicas (1 free)
- Zone redundancy available

**Pricing (vCore)**:
- 2 vCores: ~$900/month
- 4 vCores: ~$1,800/month
- 8 vCores: ~$3,600/month

**Use Cases**:
- Mission-critical applications
- Low-latency requirements
- High transaction rate
- Read scale-out scenarios

#### Hyperscale (Massive databases)

**Highly scalable storage and compute**

**Characteristics**:
- Storage: Up to 100 TB
- Fast scaling (minutes, not hours)
- Multiple read replicas (up to 4)
- Fast backup and restore
- Independent compute and storage scaling

**Pricing (vCore)**:
- 2 vCores: ~$500/month
- 4 vCores: ~$1,000/month
- Plus storage costs (~$0.12/GB/month)

**Use Cases**:
- Very large databases (> 4 TB)
- Applications requiring rapid scale-up/down
- Need for multiple read replicas
- Fast backup/restore requirements

### Compute Tiers

#### Provisioned Compute

**Fixed resources allocated continuously**

**Characteristics**:
- Fixed compute size (always on)
- Predictable performance
- Billed per hour
- Best for steady workloads

**Pricing Example (General Purpose, 4 vCores)**:
```
Compute: 4 vCores √ó 24 hours √ó 30 days = $720/month
Storage: 100 GB √ó $0.12/GB = $12/month
Total: ~$732/month
```

**Use Cases**:
- Production applications
- Consistent usage patterns
- 24/7 availability requirements

#### Serverless Compute ‚≠ê

**Auto-scaling compute with auto-pause**

**Characteristics**:
- Auto-scales between min and max vCores
- Auto-pauses during inactivity
- Per-second billing
- Automatic resume on connection

**Configuration**:
```
Min vCores: 0.5 - 40
Max vCores: 0.5 - 40
Auto-pause delay: 1 hour - 7 days (or disabled)
```

**Pricing Example (General Purpose, 0.5-4 vCores)**:
```
Active time: 8 hours/day at avg 2 vCores
Paused time: 16 hours/day

Compute: (8 √ó 2 vCores √ó 30 days) = ~$144/month
Storage: 100 GB √ó $0.12/GB = $12/month
Total: ~$156/month (vs $732 provisioned)
```

**Cost Savings**: Up to 80% for intermittent workloads

**Use Cases**:
- Development and test environments
- Unpredictable or intermittent workloads
- Infrequently used applications
- New applications with unknown patterns
- Cost optimization

**Limitations**:
- Only available in General Purpose tier
- Not available in Hyperscale (preview support varies)
- Resume delay (few seconds) on first connection after pause

### Serverless vs Hyperscale: Understanding the Difference

A common confusion is between **Serverless** and **Hyperscale**. They are fundamentally different concepts:

| Aspect | Serverless | Hyperscale |
|--------|------------|------------|
| **What it is** | **Compute Tier** (billing model) | **Service Tier** (architecture) |
| **Scaling Type** | Auto-scales compute based on workload demand | Fast manual/scheduled scaling of compute |
| **Auto-Pause** | ‚úÖ Yes - pauses when idle, **no compute charges** | ‚ùå No - always running, always billed |
| **Billing Model** | Per-second for actual usage | Per-hour for provisioned resources |
| **Max Storage** | 4 TB (General Purpose limit) | 100 TB |
| **Best For** | Intermittent/unpredictable workloads | Very large databases needing rapid scale |

#### The Critical Difference: Auto-Pause & Billing

**Serverless (Compute Tier):**
- Automatically **pauses** during inactivity ‚Üí **$0 compute cost** when paused
- Automatically **resumes** on connection (few seconds delay)
- Scales between configured min/max vCores based on actual load
- Only pay for compute seconds actually used
- Best for: bursty workloads, dev/test, apps used occasionally, monthly processing

**Hyperscale (Service Tier):**
- Can scale compute quickly (minutes vs hours for other tiers)
- But compute is **always provisioned and running**
- You pay for compute 24/7 regardless of activity
- Designed for massive databases that can't fit in 4 TB limit
- Best for: very large databases (>4TB), need multiple read replicas, rapid scale-up/down

#### Cost Impact Example

```
Scenario: Database used 8 hours/day for monthly payroll processing

Serverless (General Purpose, 4 vCores max):
‚îú‚îÄ‚îÄ Active: 8 hours √ó 30 days = 240 hours billed
‚îú‚îÄ‚îÄ Paused: 16 hours √ó 30 days = 480 hours FREE
‚îú‚îÄ‚îÄ Compute: ~$150/month
‚îî‚îÄ‚îÄ Storage: ~$12/month (100 GB)
    Total: ~$162/month

Hyperscale (Provisioned, 4 vCores):
‚îú‚îÄ‚îÄ Compute: 24 hours √ó 30 days = 720 hours billed
‚îú‚îÄ‚îÄ Compute: ~$1,000/month
‚îî‚îÄ‚îÄ Storage: ~$12/month (100 GB)
    Total: ~$1,012/month

Savings with Serverless: ~84%
```

> **Exam Tip:** When questions mention **"minimize costs"** combined with **"automatic scaling"** or **"bursty workloads"**, the answer is **Serverless (General Purpose)** - NOT Hyperscale. The key differentiator is **auto-pause** which eliminates compute charges during idle periods. Hyperscale scales fast but doesn't auto-pause, so you pay 24/7.

### Service Tier Comparison Table

| Feature | General Purpose | Business Critical | Hyperscale |
|---------|----------------|-------------------|------------|
| **Use Case** | Most workloads | Mission-critical | Very large databases |
| **Storage Type** | Remote (Premium) | Local SSD | Multi-tier storage |
| **Max Storage** | 4 TB | 4 TB | 100 TB |
| **IOPS** | Up to 7,000 | Up to 200,000+ | Depends on replicas |
| **Latency** | 5-10ms | 1-2ms | 5-10ms (primary) |
| **Read Replicas** | ‚ùå | ‚úÖ 1 included | ‚úÖ Up to 4 |
| **Zone Redundancy** | ‚úÖ Optional | ‚úÖ Optional | ‚úÖ Available |
| **Backup Retention** | 7-35 days | 7-35 days | 7-35 days |
| **Restore Time** | Depends on size | Depends on size | Fast (minutes) |
| **Serverless Compute** | ‚úÖ **Supported** | ‚ùå **Not Supported** | ‚ö†Ô∏è Preview only |
| **Always Encrypted** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **Price (4 vCore)** | ~$720/mo | ~$1,800/mo | ~$1,000/mo + storage |

> **Important:** When automatic scaling (Serverless) is required, **General Purpose is the only production-ready option**. Business Critical does NOT support Serverless compute tier.

### High Availability and Zone Redundancy

Azure SQL Database provides built-in high availability features that vary by service tier. Understanding these capabilities is crucial for designing business continuity solutions.

#### High Availability Architecture by Tier

**vCore-Based Service Tiers (General Purpose, Business Critical, Hyperscale)**:

##### General Purpose (vCore)
- **Standard Availability**: 99.99% SLA
- **Architecture**: Remote storage with compute node redundancy
- **Failover**: Automatic failover to standby replica
- **Zone Redundancy**: ‚úÖ Available (optional configuration)
- **RPO (Recovery Point Objective)**: Typically < 30 seconds
- **RTO (Recovery Time Objective)**: Typically < 30 seconds
- **Data Loss During Failover**: Minimal (transactions in flight may be lost)

##### Business Critical (vCore)
- **Premium Availability**: 99.99% SLA (99.995% with zone redundancy)
- **Architecture**: Always On availability groups with local SSD storage
- **Failover**: Automatic failover with zero data loss
- **Zone Redundancy**: ‚úÖ Available (optional configuration)
- **RPO (Recovery Point Objective)**: **0 (zero data loss)**
- **RTO (Recovery Time Objective)**: Typically < 30 seconds
- **Built-in Read Replicas**: 1 included for read scale-out
- **Data Loss During Failover**: **None** - fully synchronous replication

##### Hyperscale (vCore)
- **Availability**: 99.99% SLA (99.995% with zone redundancy)
- **Architecture**: Multi-tier storage with compute nodes
- **Failover**: Fast failover to standby compute nodes
- **Zone Redundancy**: ‚úÖ Available (optional configuration)
- **RPO**: Typically < 30 seconds
- **RTO**: Typically < 30 seconds
- **Read Replicas**: Up to 4 named replicas
- **Note**: Does not support automatic failover with zero data loss like Business Critical

**DTU-Based Service Tiers (Basic, Standard, Premium)**:

##### Basic (DTU)
- **Availability**: 99.99% SLA
- **Architecture**: Single database instance with basic redundancy
- **Failover**: Automatic but slower recovery
- **Zone Redundancy**: ‚ùå Not supported
- **RPO**: Up to 60 seconds
- **RTO**: Can be several minutes
- **Use Case**: Non-critical, small workloads only
- **Limitations**: No zone redundancy, no active geo-replication

##### Standard (DTU)
- **Availability**: 99.99% SLA
- **Architecture**: Standard redundancy with remote storage
- **Failover**: Automatic failover to standby
- **Zone Redundancy**: ‚ùå Not supported
- **RPO**: Typically < 30 seconds
- **RTO**: Typically < 30 seconds
- **Data Loss During Failover**: Possible (transactions in flight)
- **Limitations**: No zone redundancy support

##### Premium (DTU)
- **Premium Availability**: 99.99% SLA (99.995% with zone redundancy)
- **Architecture**: Always On availability groups (similar to Business Critical)
- **Failover**: Automatic failover with zero data loss
- **Zone Redundancy**: ‚úÖ **Supported**
- **RPO (Recovery Point Objective)**: **0 (zero data loss)**
- **RTO (Recovery Time Objective)**: Typically < 30 seconds
- **Active Geo-Replication**: ‚úÖ Supported
- **Built-in Read Replicas**: 1 included for read scale-out
- **Data Loss During Failover**: **None** - fully synchronous replication

#### Zone Redundancy Support Summary

| Tier | Zone Redundancy | Zero Data Loss Failover | Cost Efficiency |
|------|----------------|-------------------------|-----------------|
| **Basic (DTU)** | ‚ùå No | ‚ùå No | Lowest cost, minimal HA |
| **Standard (DTU)** | ‚ùå No | ‚ùå No | Low cost, basic HA |
| **Premium (DTU)** | ‚úÖ **Yes** | ‚úÖ **Yes** | ‚úÖ **Best for HA + cost** |
| **General Purpose (vCore)** | ‚úÖ Yes | ‚ùå No | Medium cost |
| **Business Critical (vCore)** | ‚úÖ Yes | ‚úÖ Yes | Highest cost |
| **Hyperscale (vCore)** | ‚úÖ Yes | ‚ùå No | High cost + scale |

#### Choosing the Right Tier for High Availability

**Scenario: Zero Data Loss + Zone Redundancy + Cost Optimization**

**Requirements**:
1. ‚úÖ Failover between replicas must occur without any data loss (RPO = 0)
2. ‚úÖ Database must remain available during a zone outage
3. ‚úÖ Minimize costs

**Solution**: **Azure SQL Database Premium (DTU)** ‚≠ê

**Why Premium is the Correct Choice**:

1. **Zero Data Loss (RPO = 0)**:
   - Uses Always On availability groups
   - Synchronous replication to secondary replicas
   - Automatic failover with no data loss

2. **Zone Redundancy**:
   - Supports zone-redundant configuration
   - Distributes replicas across availability zones
   - Database remains available during zone failures

3. **Cost Optimization**:
   - Less expensive than Business Critical (vCore)
   - Comparable high availability features
   - Suitable for most mission-critical workloads

**Why Other Tiers Don't Meet Requirements**:

‚ùå **Basic (DTU)**:
- No zone redundancy support
- Cannot guarantee zero data loss
- Not suitable for mission-critical workloads

‚ùå **Standard (DTU)**:
- No zone redundancy support
- Possible data loss during failover
- Not designed for high availability scenarios

‚ùå **Business Critical (vCore)**:
- ‚úÖ Meets all technical requirements
- ‚ùå More expensive than Premium DTU
- ‚ùå Does not minimize costs (violates requirement #3)

‚ùå **Hyperscale (vCore)**:
- ‚úÖ Supports zone redundancy
- ‚ùå Does not guarantee zero data loss (RPO not 0)
- ‚ùå Optimized for scale, not zero-data-loss HA
- ‚ùå Higher cost for features not needed

‚ùå **General Purpose (vCore)**:
- ‚úÖ Supports zone redundancy
- ‚ùå Does not guarantee zero data loss
- ‚ùå Uses remote storage, not Always On

#### Key Insights for Business Continuity Solutions

**For Zero Data Loss Requirements**:
- Choose **Premium (DTU)** or **Business Critical (vCore)**
- Both use Always On availability groups
- Both provide synchronous replication
- Both support zone redundancy

**For Zone Outage Protection**:
- Enable zone-redundant configuration
- Available in: Premium, General Purpose (vCore), Business Critical (vCore), Hyperscale
- NOT available in: Basic, Standard

**For Cost Optimization with High Availability**:
- **Premium (DTU)** offers the best balance
- Provides enterprise-grade HA at lower cost than Business Critical
- Suitable for most mission-critical workloads

**For Active Geo-Replication**:
- Supported in Premium and Business Critical
- Enables read-scale and disaster recovery across regions
- Automatic failover groups provide seamless failover

#### Multi-Region Write Support

> ‚ö†Ô∏è **Important: Azure SQL does NOT support multi-region writes (active-active writes)**

Unlike Azure Cosmos DB which offers native multi-region write capabilities, **Azure SQL Database and Azure SQL Managed Instance do not support simultaneous writes to multiple regions**. The replication architecture is based on:

| Aspect | Azure SQL Behavior | Cosmos DB Comparison |
|--------|-------------------|---------------------|
| **Write Region** | Single primary region only | Multiple regions can accept writes |
| **Replication Type** | Active Geo-Replication (async) | Multi-master replication |
| **Secondary Regions** | Read-only replicas | Read-write capable |
| **Write Conflicts** | N/A (single writer) | Automatic conflict resolution |
| **Failover for Writes** | Manual or automatic failover required | No failover needed |

**Why Azure SQL Doesn't Support Multi-Region Writes**:
1. **SQL Server Architecture**: Based on traditional RDBMS with single-primary design
2. **ACID Compliance**: Strict transactional consistency requires single write endpoint
3. **Conflict Resolution**: SQL databases lack built-in multi-master conflict resolution
4. **Schema Enforcement**: Complex schema changes require coordination that multi-master complicates

**Available Multi-Region Options in Azure SQL**:

| Option | Description | Use Case |
|--------|-------------|----------|
| **Active Geo-Replication** | Up to 4 readable secondaries in different regions | Read scale-out, disaster recovery |
| **Auto-Failover Groups** | Automatic failover with DNS endpoint | Business continuity with minimal RTO |
| **Readable Secondary** | Read workloads on secondary replicas | Offload reporting queries |

**If You Need Multi-Region Writes**:
- ‚úÖ **Azure Cosmos DB**: Native multi-region write support with automatic conflict resolution
- ‚ö†Ô∏è **Application-Level Sharding**: Partition data by region at application level (complex)
- ‚ö†Ô∏è **Distributed Transactions**: Use Azure SQL with application coordination (high latency)

**Key Exam Point**: When a question asks about "multi-region writes" or "active-active database writes", Azure SQL is **NOT** the correct answer. Choose **Azure Cosmos DB** for true multi-region write scenarios.

#### Geo-Replication and Failover Options

Azure SQL Database provides several mechanisms for disaster recovery and geographic distribution of data. Understanding the differences between these options is critical for selecting the right solution.

##### Active Geo-Replication

**Active Geo-Replication** provides asynchronous replication of data to **up to 4 readable secondary replicas** in different Azure regions.

**Key Characteristics**:
- ‚úÖ Supports **multiple secondary read-only replicas** (up to 4)
- ‚úÖ **Automatic replication** between primary and secondary replicas (asynchronous)
- ‚úÖ **RTO (Recovery Time Objective)**: Less than 30 seconds
- ‚úÖ **RPO (Recovery Point Objective)**: Less than 5 seconds
- ‚úÖ Secondary replicas are **readable** and available for read workloads
- ‚úÖ Supports **manual or automatic failover**
- ‚úÖ Each secondary can be in a different region
- ‚úÖ Independent scale and pricing for each replica

**Best Use Cases**:
- Applications requiring **multiple read-only replicas** for read scale-out
- Disaster recovery scenarios with **low RTO requirements** (< 30 seconds)
- Geographic distribution of read workloads
- Scenarios requiring **granular control** over failover decisions

##### Auto-Failover Groups

**Auto-Failover Groups** provide automatic failover capability with a DNS-based endpoint for transparent connection handling.

**Key Characteristics**:
- ‚úÖ **Automatic failover** with DNS endpoint management
- ‚úÖ Supports **single secondary replica only** (not multiple)
- ‚úÖ **RTO**: 1 hour (typical, can be longer)
- ‚úÖ **RPO**: 1 hour (typical)
- ‚úÖ Transparent connection string (no application changes needed after failover)
- ‚úÖ Supports both Azure SQL Database and Azure SQL Managed Instance
- ‚ö†Ô∏è Does **NOT support multiple secondary replicas**

**Best Use Cases**:
- Business continuity with **minimal application changes**
- Scenarios where **automatic failover** is required without manual intervention
- Organizations preferring **simplified failover management**

##### Standard Geo-Replication (Not Recommended)

**Standard Geo-Replication** is a legacy option with limited capabilities.

**Key Characteristics**:
- ‚ö†Ô∏è Supports **only one secondary replica**
- ‚ö†Ô∏è Secondary replica is **offline** and not readable
- ‚ö†Ô∏è Higher RTO and RPO compared to Active Geo-Replication
- ‚ùå **Not recommended** for new deployments

##### Comparison: Active Geo-Replication vs Auto-Failover Groups

| Feature | Active Geo-Replication | Auto-Failover Groups | Standard Geo-Replication |
|---------|----------------------|---------------------|-------------------------|
| **Secondary Replicas** | Up to 4 | 1 only | 1 only |
| **Secondary Readable** | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No (offline) |
| **Automatic Replication** | ‚úÖ Yes (async) | ‚úÖ Yes (async) | ‚úÖ Yes |
| **RTO** | < 30 seconds | ~1 hour | Higher |
| **RPO** | < 5 seconds | ~1 hour | Higher |
| **Automatic Failover** | ‚ùå No (manual/programmatic) | ‚úÖ Built-in | ‚ùå Manual only |
| **DNS Endpoint** | ‚ùå No | ‚úÖ Yes | ‚ùå No |
| **Best For** | Multiple read replicas, low RTO | Automatic failover, simplicity | Legacy scenarios |

##### Key Exam Point: Selecting the Right Replication Mechanism

> üìù **When to choose Active Geo-Replication**:
> - Requirement for **multiple secondary read-only replicas**
> - Strict **RTO requirement of 15 minutes or less** (Active Geo-Replication has RTO < 30 seconds)
> - Need for **automatic replication** between replicas (data sync is automatic, failover is NOT)
> - **Manual failover is acceptable** (increases administrative overhead)

> üìù **When to choose Auto-Failover Groups**:
> - Need for **automatic failover** with minimal configuration
> - Requirement for **transparent DNS failover** (no app changes)
> - Only **one secondary replica** is needed
> - **Cross-region business continuity** with minimal administrative effort
> - Supports **user-initiated backups**
> - Ideal for **minimizing administrative overhead** for disaster recovery

> üìù **When to choose Zone-Redundant Deployment**:
> - High availability **within a single region** is sufficient
> - Protection against **availability zone failures**
> - **Cross-region replication is NOT required**
> - Does NOT provide multi-region disaster recovery

> ‚ö†Ô∏è **Avoid Standard Geo-Replication** for new deployments as it doesn't support readable secondaries and has limited capabilities.

##### Resiliency Solutions Comparison: Cross-Region vs Single-Region

| Feature | Auto-Failover Groups | Active Geo-Replication | Zone-Redundant Deployment |
|---------|---------------------|----------------------|---------------------------|
| **Scope** | Cross-region | Cross-region | Single region |
| **Automatic Failover** | ‚úÖ Yes (built-in) | ‚ùå No (manual/programmatic only) | ‚úÖ Yes (within zone) |
| **Supports User-Initiated Backups** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **Multiple Replicas Across Regions** | ‚ùå 1 secondary only | ‚úÖ Up to 4 | ‚ùå No cross-region |
| **Administrative Effort** | ‚úÖ Minimal | ‚ö†Ô∏è Higher (manual failover required) | ‚úÖ Minimal |
| **DNS Endpoint Management** | ‚úÖ Automatic | ‚ùå Manual | N/A |
| **Best For** | Cross-region DR with auto-failover | Read scale-out + granular control | HA within single region |

> üí° **Key Decision Point**: For cross-region business continuity with **automatic failover** and **minimal administrative effort**, choose **Auto-Failover Groups**. Active Geo-Replication requires **manual or programmatic failover initiation**, which increases administrative overhead. Per Microsoft docs: *"The geo-failover is initiated programmatically by the application or manually by the user."*

#### References
- [Azure SQL Database High Availability](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla-local-zone-redundancy?view=azuresql&tabs=azure-powershell)
- [Active Geo-Replication Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql&tabs=tsql)
- [Active Geo-Replication Configuration](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-configure-portal?view=azuresql&tabs=portal)
- [Auto-Failover Groups Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/auto-failover-group-sql-db?view=azuresql)
- [Azure SQL Database Service Tiers (DTU)](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)
- [Azure SQL Database Hyperscale](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)

## Azure SQL Managed Instance

**Azure SQL Managed Instance** is a fully managed instance of SQL Server providing near 100% compatibility with SQL Server Enterprise Edition, making it ideal for lift-and-shift migrations.

### Key Features

#### Instance-Scoped Features
- SQL Server Agent
- Service Broker
- Database Mail
- Distributed transactions
- CLR (Common Language Runtime)
- Linked servers
- Cross-database queries

#### Networking
- Deployed in Azure VNet
- Private IP address
- Supports hybrid connectivity (VPN, ExpressRoute)
- Can join managed domain

#### Compatibility
- Near 100% compatibility with SQL Server
- Most SQL Server features supported
- Online migration tools available

### Service Tiers

#### General Purpose

**Cost-effective for most workloads**

**Characteristics**:
- Remote storage (Azure Premium Storage)
- 99.99% availability SLA
- Standard performance

**Pricing**:
- 4 vCores: ~$1,000/month
- 8 vCores: ~$2,000/month
- 16 vCores: ~$4,000/month

#### Business Critical

**High performance and availability**

**Characteristics**:
- Local SSD storage
- 99.99% availability SLA
- Built-in Always On Availability Group
- Free read replica

**Pricing**:
- 4 vCores: ~$2,500/month
- 8 vCores: ~$5,000/month
- 16 vCores: ~$10,000/month

### Use Cases

‚úÖ **Use SQL Managed Instance When**:
- Migrating from SQL Server on-premises
- Need instance-scoped features (SQL Agent, Service Broker)
- Cross-database queries required
- Need managed domain integration
- Lift-and-shift with minimal changes

‚ùå **Don't Use When**:
- Modern cloud-native application
- Don't need SQL Server specific features
- Cost is primary concern
- Simpler management preferred

## SQL Server on Azure VMs

**SQL Server on Azure Virtual Machines** provides full control over SQL Server and OS, running on IaaS infrastructure.

### Key Features

- Full SQL Server control
- Full OS control
- Any SQL Server version
- Flexible licensing (BYOL, PAYG)
- Custom extensions and configurations
- Windows or Linux

### VM Sizes and Pricing

#### Small (Development/Test)
- **Standard_D2s_v3**: 2 vCores, 8 GB RAM
- SQL Server Standard: ~$190/month
- SQL Server Enterprise: ~$515/month

#### Medium (Production)
- **Standard_D8s_v3**: 8 vCores, 32 GB RAM
- SQL Server Standard: ~$760/month
- SQL Server Enterprise: ~$2,060/month

#### Large (High Performance)
- **Standard_E16s_v3**: 16 vCores, 128 GB RAM
- SQL Server Standard: ~$1,950/month
- SQL Server Enterprise: ~$4,120/month

### Use Cases

‚úÖ **Use SQL Server on VMs When**:
- Need full SQL Server control
- Need OS-level access
- Custom SQL Server configuration required
- Specific SQL Server version needed
- Third-party tools requiring specific setup

‚ùå **Don't Use When**:
- Want minimal management overhead
- Don't need OS access
- Want automated patching/backups
- PaaS benefits desired

### High Availability Options for SQL Server on Azure VMs

When migrating on-premises SQL Server to Azure VMs, implementing high availability (HA) is critical for production workloads. Azure provides two main HA options for SQL Server on VMs: **Always On Availability Groups (AG)** and **Always On Failover Cluster Instances (FCI)**.

#### Overview of High Availability Options

| Feature | Always On Availability Group (AG) | Always On Failover Cluster Instance (FCI) |
|---------|-----------------------------------|-------------------------------------------|
| **Level** | Database-level protection | Instance-level protection |
| **Storage** | Local storage on each node (independent) | Shared storage (Premium File Share or shared disks) |
| **Failover Unit** | Individual databases | Entire SQL Server instance |
| **Readable Secondary** | Yes (read replicas supported) | No |
| **SQL Server Edition** | Enterprise (multi-database), Standard (single DB) | Enterprise or Standard |
| **Cost** | Lower (no shared storage required) | Higher (shared storage costs) |

#### Network Name Options: VNN vs DNN

When configuring high availability for SQL Server on Azure VMs, you must choose between **Virtual Network Name (VNN)** and **Distributed Network Name (DNN)** for client connectivity.

##### Virtual Network Name (VNN)

**VNN** is the traditional approach requiring an Azure Load Balancer for traffic routing.

**How VNN Works**:
1. A single virtual IP is assigned to the availability group listener or FCI
2. An Azure Load Balancer routes traffic to the primary node
3. Health probes periodically check node availability
4. On failover, the load balancer redirects traffic to the new primary

**VNN Characteristics**:
- Requires Azure Load Balancer configuration
- Health probes introduce failover detection delays
- Single IP address for client connections
- Traditional connectivity model (compatible with older applications)
- Additional complexity in provisioning

##### Distributed Network Name (DNN)

**DNN** is a newer approach that eliminates the Azure Load Balancer dependency.

**How DNN Works**:
1. When a DNN resource is created, the cluster binds the DNS name to the IP addresses of **all nodes** in the cluster
2. Clients receive all node IP addresses when resolving the DNS name
3. With `MultiSubnetFailover=True` in the connection string, the client tries all IP addresses **in parallel**
4. The first responding (active) node establishes the connection

**DNN Characteristics**:
- No Azure Load Balancer required
- Faster failover (parallel connection attempts)
- Simplified provisioning
- Requires connection string modification (`MultiSubnetFailover=True`)
- Lower operational complexity

#### Comparison: VNN vs DNN

| Aspect | Virtual Network Name (VNN) | Distributed Network Name (DNN) |
|--------|---------------------------|--------------------------------|
| **Load Balancer** | Required | Not required |
| **Failover Time** | Slower (health probe delays) | Faster (parallel IP attempts) |
| **Provisioning Complexity** | Higher | Lower |
| **Connection String** | Standard | Requires `MultiSubnetFailover=True` |
| **Cost** | Higher (load balancer costs) | Lower |
| **Legacy App Support** | Better | May require app modifications |

#### Recommended HA Configuration for Minimal Cost and Failover Time

**Scenario**: Migrating an on-premises LOB application with SQL Server backend to Azure VMs, requiring:
- Minimize costs
- Minimize failover time if a single server fails

**Recommended Solution**: **Always On Availability Group with Premium Storage Disks and DNN**

| Requirement | Why AG + DNN is Best |
|-------------|---------------------|
| **Minimize Cost** | AG uses local storage (no shared storage costs), DNN eliminates load balancer costs |
| **Minimize Failover Time** | DNN enables parallel connection attempts, eliminating load balancer health probe delays |

**Why Other Options Are Less Suitable**:

| Option | Why Not Recommended |
|--------|-------------------|
| **FCI with VNN + Premium File Share** | VNN requires load balancer (adds failover delay), FCI requires shared storage (adds cost) |
| **FCI with VNN + Standard File Share** | VNN adds failover delay, standard file share has lower performance |
| **AG with VNN + Premium Storage** | VNN requires load balancer which introduces failover delays due to health probes |

#### Configuration Best Practices

##### For Always On Availability Groups with DNN

1. **Storage**: Use Premium SSD or Ultra Disks for data and log files
2. **Network**: Configure DNN listener for faster failover
3. **Connection String**: Use `MultiSubnetFailover=True`
4. **Synchronous Commit**: For zero data loss within same region
5. **Quorum**: Use Cloud Witness for cost-effective cluster quorum

**Example Connection String with DNN**:
```
Server=ag-dnn-listener.domain.com;Database=MyDB;MultiSubnetFailover=True;...
```

##### For Always On Failover Cluster Instances

1. **Storage Options**:
   - **Premium File Share**: Recommended for most FCI deployments
   - **Azure Shared Disks**: For Ultra Disk or Premium SSD v2 requirements
2. **Network**: Choose DNN when possible for faster failover
3. **Placement**: Use Availability Sets or Availability Zones for hardware fault tolerance

#### Storage Options for FCI

| Storage Type | Description | Best For |
|--------------|-------------|----------|
| **Premium File Share** | Azure Files with premium tier | Most FCI deployments, cost-effective |
| **Standard File Share** | Azure Files with standard tier | Dev/test only (not recommended for production) |
| **Azure Shared Disks** | Managed disks shared across VMs | High I/O requirements |
| **Storage Spaces Direct (S2D)** | Software-defined storage | High performance, complex setup |

#### Exam Tips - SQL Server HA on Azure VMs

| Scenario | Recommended Solution |
|----------|---------------------|
| Minimize failover time + minimize cost | **AG with DNN + Premium Storage** |
| Database-level protection with read replicas | **Always On Availability Group** |
| Instance-level protection (entire SQL Server) | **Always On Failover Cluster Instance** |
| Legacy apps can't use MultiSubnetFailover | VNN with Azure Load Balancer |
| Shared storage required | FCI with Premium File Share |

**Key Points to Remember**:
- **DNN > VNN** for faster failover (no load balancer health probe delays)
- **AG > FCI** for cost optimization (no shared storage required)
- DNN requires `MultiSubnetFailover=True` in connection strings
- VNN requires Azure Load Balancer configuration
- Premium storage recommended for production workloads

**Reference Links**:
- [Windows Server Failover Cluster Overview - VNN](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/hadr-windows-server-failover-cluster-overview?view=azuresql#virtual-network-name-vnn)
- [Windows Server Failover Cluster Overview - DNN](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/hadr-windows-server-failover-cluster-overview?view=azuresql#distributed-network-name-dnn)
- [Availability Group Overview](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/availability-group-overview?view=azuresql)
- [Failover Cluster Instance Overview](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/failover-cluster-instance-overview?view=azuresql)

### Disk Caching Best Practices for SQL Server on Azure VMs

When configuring SQL Server on Azure VMs, proper disk caching configuration is critical for optimal performance and data integrity. Azure offers three host caching policies for managed disks:

#### Caching Options

| Caching Policy | Description | Use Case |
|----------------|-------------|----------|
| **None** | No caching | Write-intensive workloads, transaction logs |
| **ReadOnly** | Read caching only | Read-intensive workloads, data files |
| **ReadWrite** | Read and write caching | OS disks (not recommended for SQL) |

#### Recommended Configuration by Disk Type

| SQL Server Disk Type | Recommended Caching | Rationale |
|---------------------|--------------------|-----------|
| **Data disk** | **ReadOnly** | Improves read performance while maintaining data integrity. SQL Server workloads typically involve frequent read operations from data files. Enables faster retrieval from host cache. |
| **Transaction log disk** | **None** | No performance benefit for sequential write operations. Caching can degrade write performance and decrease available cache for data disk reads. |
| **Operating OS disk** | **ReadWrite** (default) | Default OS configuration is optimal. Not recommended to change. |
| **tempdb disk** | **ReadOnly** | If tempdb cannot fit on ephemeral drive (D:\), place on separate data disk with ReadOnly caching. |

#### Detailed Recommendations

**Data Disk (ReadOnly Caching)**:
- **Enable ReadOnly caching** for the disks hosting SQL Server data files
- Reads from cache will be faster than uncached reads from the data disk
- Uncached IOPS and throughput plus Cached IOPS and throughput yield the total possible performance available from the VM within the VM's limits
- Actual performance varies based on the workload's ability to use the cache (cache hit ratio)
- **Best for**: Read-intensive database operations, OLTP workloads

**Transaction Log Disk (None Caching)**:
- **Set the caching policy to None** for disks hosting the transaction log
- There's no performance benefit to enabling caching for the transaction log disk
- Having either ReadOnly or ReadWrite caching enabled on the log drive can degrade performance of the writes against the drive
- Caching reduces the amount of cache available for reads on the data drive
- **Best for**: Sequential write operations that don't benefit from caching

**Operating OS Disk (ReadWrite - Default)**:
- The default caching policy is **ReadWrite** for the OS drive
- **It is not recommended to change** the caching level of the OS drive
- Standard OS workloads benefit from the default configuration

**tempdb Configuration**:
- If tempdb **can't be placed on the ephemeral drive D:\** due to capacity reasons:
  - Either resize the VM to get a larger ephemeral drive
  - Or place tempdb on a separate data drive with **ReadOnly caching** configured
- The VM cache and ephemeral drive both use the local SSD
- Keep this in mind when sizing as tempdb I/O will count against the cached IOPS and throughput VM limits when hosted on the ephemeral drive

#### Performance Impact Examples

**Scenario 1: Data Disk with ReadOnly Caching (Optimal)**
```
P40 Disk: 7,500 IOPS, 250 MB/s
VM Cache: Additional cached reads
Result: Improved query performance for read operations
Cache Hit Ratio: 70% ‚Üí 70% of reads served from fast cache
```

**Scenario 2: Transaction Log with None Caching (Optimal)**
```
P40 Disk: 7,500 IOPS, 250 MB/s
No caching overhead
Result: Maximum write throughput for transaction log
Benefit: More cache available for data disk reads
```

**Scenario 3: Data Disk with ReadWrite Caching (Not Recommended)**
```
Risk: Data corruption during unexpected failures
Issue: Writes cached in host memory may be lost
Result: Unsuitable for transactional systems like SQL Server
```

#### Key Considerations

‚úÖ **Do**:
- Use ReadOnly caching for data disks to optimize read performance
- Use None caching for transaction log disks to optimize write performance
- Place tempdb on ephemeral drive (D:\) when possible for best performance
- Monitor cache hit ratios to validate effectiveness
- Use Premium SSD (P-series) or Ultra Disks for SQL Server workloads

‚ùå **Don't**:
- Use ReadWrite caching for SQL Server data or log disks (risk of data corruption)
- Enable any caching on transaction log disks (degrades performance)
- Change OS disk caching from default ReadWrite
- Place transaction log on the ephemeral drive (data loss risk on VM restart)

**Reference**: [SQL Server on Azure VMs - Storage Performance Best Practices](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices-storage?view=azuresql#data-file-caching-policies)

## Pricing Tiers Comparison

### Single Database - General Purpose (Serverless vs Provisioned)

| Configuration | Provisioned | Serverless (50% active) |
|---------------|-------------|-------------------------|
| 2 vCores | $360/month | ~$90/month |
| 4 vCores | $720/month | ~$180/month |
| 8 vCores | $1,440/month | ~$360/month |

**Savings**: Up to 75% for intermittent workloads

### Cross-Product Comparison (8 vCores, General Purpose)

| Product | Monthly Cost | Management | Best For |
|---------|--------------|------------|----------|
| **SQL Database (Provisioned)** | ~$1,440 | Fully managed | Modern apps |
| **SQL Database (Serverless avg)** | ~$360 | Fully managed | Intermittent use |
| **SQL Managed Instance** | ~$2,000 | Fully managed | Migrations |
| **SQL VM (Standard)** | ~$760 | Self-managed | Full control |
| **SQL VM (Enterprise)** | ~$2,060 | Self-managed | Full control + features |

### Service Tier Comparison (SQL Database, 4 vCores)

| Tier | Monthly Cost | IOPS | Max Storage | Best For |
|------|--------------|------|-------------|----------|
| **General Purpose** | ~$720 | ~7,000 | 4 TB | Most workloads |
| **Business Critical** | ~$1,800 | ~200,000 | 4 TB | Mission-critical |
| **Hyperscale** | ~$1,000+ | Varies | 100 TB | Large databases |

## Feature Comparison

### Azure SQL Products Feature Matrix

| Feature | SQL Database | SQL Managed Instance | SQL on VMs |
|---------|--------------|---------------------|------------|
| **Management** | Fully managed | Fully managed | Self-managed |
| **SQL Compatibility** | ~95% | ~100% | 100% |
| **Auto-scaling** | ‚úÖ Serverless | ‚ùå | ‚ö†Ô∏è Manual |
| **Per-second billing** | ‚úÖ Serverless | ‚ùå | ‚ùå |
| **Elastic pools** | ‚úÖ | ‚úÖ Instance pools | ‚ùå |
| **VNet injection** | ‚ö†Ô∏è Private endpoint | ‚úÖ Native | ‚úÖ Native |
| **SQL Agent** | ‚ùå | ‚úÖ | ‚úÖ |
| **Cross-DB queries** | ‚ö†Ô∏è Elastic queries | ‚úÖ | ‚úÖ |
| **Linked servers** | ‚ùå | ‚úÖ Limited | ‚úÖ Full |
| **Service Broker** | ‚ùå | ‚úÖ | ‚úÖ |
| **CLR** | ‚ùå | ‚úÖ | ‚úÖ |
| **Backup control** | ‚ùå Automated | ‚ö†Ô∏è Limited | ‚úÖ Full |
| **Patching control** | ‚ùå Automated | ‚ùå Automated | ‚úÖ Full |
| **OS access** | ‚ùå | ‚ùå | ‚úÖ |
| **Max DB size** | 4 TB (100 TB Hyperscale) | 16 TB | Unlimited |
| **Geo-replication** | ‚úÖ Active | ‚úÖ Failover groups | ‚ö†Ô∏è Manual |

### Deployment Options Comparison

| Deployment | Resource Sharing | Scaling | Billing | Best For |
|------------|-----------------|---------|---------|----------|
| **Single Database** | Dedicated | Per database | Per database | Individual apps |
| **Elastic Pool** | Shared pool | Pool level | Per pool | Multi-tenant SaaS |
| **Managed Instance** | Instance | Instance level | Per instance | Instance migration |

## Decision Guide

### Question-Based Selection

#### Q1: Do you need full SQL Server and OS control?
- **YES** ‚Üí SQL Server on Azure VMs
- **NO** ‚Üí Continue to Q2

#### Q2: Do you need instance-scoped features (SQL Agent, Service Broker, cross-DB queries)?
- **YES** ‚Üí Azure SQL Managed Instance
- **NO** ‚Üí Continue to Q3

#### Q3: Is it a new cloud-native application?
- **YES** ‚Üí Azure SQL Database (Single or Elastic Pool)
- **NO** ‚Üí Continue to Q4

#### Q4: Do you have unpredictable or intermittent workload?
- **YES** ‚Üí Azure SQL Database (Serverless)
- **NO** ‚Üí Azure SQL Database (Provisioned)

#### Q5: Do you have multiple databases with varying usage?
- **YES** ‚Üí Azure SQL Database (Elastic Pool)
- **NO** ‚Üí Azure SQL Database (Single)

### Service Tier Selection (SQL Database)

#### Choose General Purpose when:
- ‚úÖ Standard business workload
- ‚úÖ Budget-conscious
- ‚úÖ Storage < 4 TB
- ‚úÖ Latency tolerance (5-10ms)

#### Choose Business Critical when:
- ‚úÖ Mission-critical application
- ‚úÖ Low latency required (<2ms)
- ‚úÖ High transaction rate
- ‚úÖ Need read replicas
- ‚úÖ Zone redundancy needed

#### Choose Hyperscale when:
- ‚úÖ Database > 4 TB
- ‚úÖ Need rapid scaling
- ‚úÖ Multiple read replicas needed
- ‚úÖ Fast backup/restore critical

### Compute Tier Selection

#### Choose Provisioned when:
- ‚úÖ Consistent 24/7 usage
- ‚úÖ Production workload
- ‚úÖ Predictable performance required
- ‚úÖ No tolerance for cold start delay

#### Choose Serverless when:
- ‚úÖ Development/test environments
- ‚úÖ Intermittent usage patterns
- ‚úÖ Cost optimization priority
- ‚úÖ Unpredictable workload
- ‚úÖ Can tolerate resume delay (seconds)

## Best Practices

### Cost Optimization

#### 1. Right-Size Resources
```
‚ùå Over-provisioned: 8 vCores, 20% avg CPU ‚Üí $1,440/month
‚úÖ Right-sized: 4 vCores, 60% avg CPU ‚Üí $720/month
Savings: $720/month (50%)
```

#### 2. Use Serverless for Dev/Test
```
‚ùå Provisioned 4 vCores 24/7 ‚Üí $720/month
‚úÖ Serverless 4 vCores, 8hr/day ‚Üí ~$180/month
Savings: $540/month (75%)
```

#### 3. Leverage Azure Hybrid Benefit
```
‚ùå Without Hybrid Benefit ‚Üí $720/month
‚úÖ With Hybrid Benefit (existing license) ‚Üí $207/month
Savings: $513/month (71%)
```

#### 4. Use Elastic Pools for Multiple DBs
```
‚ùå 10 databases √ó $360 each ‚Üí $3,600/month
‚úÖ Elastic pool for 10 databases ‚Üí $1,800/month
Savings: $1,800/month (50%)
```

#### 5. Reserved Capacity
```
‚ùå Pay-as-you-go: $720/month √ó 12 = $8,640/year
‚úÖ 1-year reserved: ~$6,500/year
‚úÖ 3-year reserved: ~$4,800/year
Savings: Up to 44%
```

### Performance Optimization

‚úÖ **Use appropriate indexes** - Reduces I/O and improves query performance  
‚úÖ **Enable automatic tuning** - AI-powered performance recommendations  
‚úÖ **Use query performance insights** - Identify slow queries  
‚úÖ **Scale up temporarily** - For known peak periods  
‚úÖ **Use read replicas** - Offload read workloads (Business Critical)  
‚úÖ **Implement connection pooling** - Reduce connection overhead  
‚úÖ **Monitor DTU/vCore usage** - Identify bottlenecks  

### High Availability

> üìñ **For detailed high availability and zone redundancy information**, see the [High Availability and Zone Redundancy](#high-availability-and-zone-redundancy) section above.

‚úÖ **Choose the right tier for your HA requirements**:
  - **Zero data loss required**: Premium (DTU) or Business Critical (vCore)
  - **Zone redundancy required**: Premium, General Purpose, Business Critical, or Hyperscale
  - **Cost optimization with HA**: Premium (DTU) offers best balance
  
‚úÖ **Enable zone-redundancy** - Protect against datacenter/zone failures  
‚úÖ **Enable geo-replication** - Disaster recovery across regions  
‚úÖ **Configure failover groups** - Automatic failover capability with zone redundancy
‚úÖ **Test failover procedures** - Ensure recovery processes work  
‚úÖ **Monitor backup status** - Verify backups are successful  
‚úÖ **Understand RPO/RTO requirements** - Select tier that meets objectives  

### Security Best Practices

‚úÖ **Enable Azure AD authentication** - Centralized identity management  
‚úÖ **Use private endpoints** - Eliminate public internet exposure  
‚úÖ **Enable Transparent Data Encryption (TDE)** - Encrypt data at rest  
‚úÖ **Use Always Encrypted** - Protect sensitive data (see details below)  
‚úÖ **Enable Advanced Threat Protection** - Detect anomalous activities  
‚úÖ **Implement row-level security** - Fine-grained access control  
‚úÖ **Enable auditing** - Track database activities  
‚úÖ **Regular security assessments** - Identify vulnerabilities

#### Azure SQL Database Auditing

**Azure SQL Database Auditing** tracks database events and writes them to an audit log. It helps maintain regulatory compliance, understand database activity, and gain insights into potential security violations.

**Key Requirements and Constraints**:

1. **Regional Constraint for Storage Accounts**
   - The storage account used for storing audit logs **must be in the same region** as the SQL server
   - Example: If your SQL server is in East US, the storage account must also be in East US
   - Cross-region storage is not supported for audit log storage

2. **Service Tier Support**
   - Auditing is supported across all service tiers (Basic, Standard, Premium)
   - Available for all purchasing models (DTU-based and vCore-based)

3. **Storage Options**
   - **Azure Storage Account**: Store audit logs in blob storage
   - **Log Analytics Workspace**: Integrate with Azure Monitor for advanced querying
   - **Event Hub**: Stream audit logs to SIEM tools or external systems

4. **Audit Scope**
   - Database-level auditing: Applies to a specific database
   - Server-level auditing: Applies to all databases on the server

**Example Scenario**:

Given the following Azure resources:

| Resource | Type | Region | Details |
|----------|------|--------|---------|
| SQLsvr1 | SQL Server | East US | - |
| SQLsvr2 | SQL Server | West US | - |
| SQLdb1 | SQL Database | East US | Standard tier on SQLsvr1 |
| SQLdb2 | SQL Database | East US | Standard tier on SQLsvr1 |
| SQLdb3 | SQL Database | West US | Premium tier on SQLsvr2 |
| storage1 | Storage Account | East US | StorageV2 |
| storage2 | Storage Account | Central US | BlobStorage |

**Auditing Compatibility**:
- ‚úÖ SQLdb1 ‚Üí storage1: **Supported** (same region: East US)
- ‚úÖ SQLdb2 ‚Üí storage1: **Supported** (same region: East US)
- ‚ùå SQLdb1 ‚Üí storage2: **Not supported** (different regions: East US vs Central US)
- ‚ùå SQLdb3 ‚Üí storage1: **Not supported** (different regions: West US vs East US)

**Best Practices**:
- Enable auditing at the server level to audit all databases automatically
- Use Log Analytics for long-term retention and advanced analysis
- Configure retention policies based on compliance requirements
- Monitor audit logs regularly for suspicious activities

**Reference**: [Azure SQL Database Auditing Overview](https://learn.microsoft.com/azure/azure-sql/database/auditing-overview)  

#### Always Encrypted (Column-Level Encryption)

**Always Encrypted** is a feature designed to protect sensitive data, such as Personally Identifiable Information (PII), credit card numbers, and national identification numbers. It ensures data remains encrypted at rest, in transit, and during query processing.

**Key Characteristics**:

| Feature | Description |
|---------|-------------|
| **Encryption Location** | Client-side encryption - data is encrypted before leaving the application |
| **Key Management** | Column Master Key (CMK) stored in Azure Key Vault or Windows Certificate Store |
| **Tier Availability** | **All Azure SQL Database tiers** (Basic, Standard, Premium, General Purpose, Business Critical, Hyperscale) |
| **Encryption Types** | Deterministic (allows equality comparisons) and Randomized (more secure) |
| **Performance Impact** | Minimal - encryption/decryption handled by client driver |

**How It Works**:
```
[Application] --> [Client Driver encrypts data] --> [Azure SQL Database (encrypted columns)]
                                                              |
[Application] <-- [Client Driver decrypts data] <-- [Query returns encrypted data]
```

**Always Encrypted vs. Other Encryption Methods**:

| Method | Encryption Level | Use Case | Key Location |
|--------|-----------------|----------|--------------|
| **Always Encrypted** | Column-level | Sensitive columns (PII, SSN) | Client-side (Key Vault/Cert Store) |
| **Transparent Data Encryption (TDE)** | Database-level | Entire database at rest | Server-side (Azure managed) |
| **Dynamic Data Masking** | Display-level | Mask data in query results | N/A (not encryption) |
| **Row-Level Security** | Row-level | Filter rows by user | N/A (access control) |

**When to Use Always Encrypted**:
- ‚úÖ Storing PII (names, SSN, addresses, phone numbers)
- ‚úÖ Storing financial data (credit card numbers, salaries)
- ‚úÖ Compliance requirements (GDPR, HIPAA, PCI-DSS)
- ‚úÖ Need to prevent DBA access to sensitive data
- ‚úÖ Multi-tenant applications with data isolation requirements

**Limitations**:
- ‚ùå Cannot perform range queries on encrypted columns
- ‚ùå Limited SQL operations on encrypted data (no LIKE, ORDER BY on randomized)
- ‚ùå Requires application changes to use Always Encrypted-enabled drivers
- ‚ùå Encryption keys must be accessible to applications

> **Exam Tip:** When a scenario mentions encrypting **specific columns** containing sensitive/PII data, **Always Encrypted** is the answer. It's available across **all Azure SQL Database service tiers**, making it the go-to solution regardless of whether you're using General Purpose, Business Critical, or even Basic tier.

**Reference**: [Always Encrypted Documentation](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine)

#### Transparent Data Encryption (TDE)

**Transparent Data Encryption (TDE)** protects Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics against the threat of malicious offline activity by encrypting data at rest. It performs real-time encryption and decryption of the database, associated backups, and transaction log files at rest without requiring changes to the application.

**Key Characteristics**:

| Feature | Description |
|---------|-------------|
| **Encryption Level** | Database-level - encrypts entire database at rest |
| **Encryption Algorithm** | AES 256-bit for the Database Encryption Key (DEK) |
| **Default State** | Enabled by default for all new Azure SQL databases |
| **Performance Impact** | Minimal - transparent to applications |
| **Key Management** | Service-managed or Customer-managed (BYOK) |

**TDE Protector Options**:

| Option | Description | Key Location |
|--------|-------------|--------------|
| **Service-Managed TDE** | Microsoft manages the encryption key | Azure-managed |
| **Customer-Managed TDE (BYOK)** | You manage the encryption key | Azure Key Vault |

**Customer-Managed TDE (Bring Your Own Key - BYOK)**:

When implementing customer-managed TDE, you store your TDE protector (asymmetric key) in Azure Key Vault. This provides:
- Full control over key lifecycle
- Key rotation capabilities
- Ability to revoke database access by removing key permissions
- Centralized key management

**Supported Key Types and Lengths for TDE Protector**:

| Key Type | Key Length | Supported for TDE |
|----------|------------|-------------------|
| **RSA 2048** | 2048-bit | ‚úÖ Yes |
| **RSA 3072** | 3072-bit | ‚úÖ Yes (Recommended for maximum supported strength) |
| **RSA 4096** | 4096-bit | ‚ùå No - Not supported |
| **AES 256** | 256-bit | ‚ùå No - Symmetric, cannot be used as TDE protector |

> ‚ö†Ô∏è **Important**: The TDE protector must be an **asymmetric RSA key** stored in Azure Key Vault. AES is a symmetric algorithm and is used internally to encrypt the Database Encryption Key (DEK), but it **cannot** be used as the TDE protector itself.

**Maximum Encryption Strength for TDE Protector**:
- **RSA 3072** is the **strongest supported option** for customer-managed TDE
- While RSA 4096 offers higher theoretical encryption strength, it is **not supported** by Azure SQL Database and Azure SQL Managed Instance for TDE protectors
- RSA 2048 is supported but provides lower encryption strength than RSA 3072

**How TDE Works**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Azure Key Vault                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  TDE Protector (RSA 2048/3072 asymmetric key)           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Encrypts the Database Encryption Key (DEK)           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº Encrypts
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Database Encryption Key (DEK)                 ‚îÇ
‚îÇ  - Symmetric AES 256-bit key                                    ‚îÇ
‚îÇ  - Stored encrypted in database boot record                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº Encrypts
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Database Content                           ‚îÇ
‚îÇ  - Data files (.mdf)                                            ‚îÇ
‚îÇ  - Log files (.ldf)                                             ‚îÇ
‚îÇ  - Backup files                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Migration Scenario - Enabling Customer-Managed TDE**:

When migrating an on-premises SQL Server database to Azure SQL Managed Instance and you need to enable customer-managed TDE with maximum encryption strength:

1. **Create Azure Key Vault** with soft-delete and purge protection enabled
2. **Generate RSA 3072 key** in Azure Key Vault (maximum supported strength)
3. **Grant permissions** to the SQL Managed Instance identity on the key
4. **Set the TDE protector** to the customer-managed key
5. **Enable TDE** on the database (if not already enabled)

**Best Practices for Customer-Managed TDE**:
- ‚úÖ Use **RSA 3072** for maximum supported encryption strength
- ‚úÖ Enable **soft-delete** and **purge protection** on Key Vault
- ‚úÖ Implement **key rotation** policies
- ‚úÖ Use **Azure RBAC** for key access management
- ‚úÖ Configure **backup and disaster recovery** for keys
- ‚ùå Never use RSA 4096 (not supported, will cause deployment errors)
- ‚ùå Don't delete keys without understanding the impact on database accessibility

> **Exam Tip:** When asked about enabling customer-managed TDE with **maximum encryption strength**, the answer is **RSA 3072**. RSA 4096 is theoretically stronger but is **not supported** for TDE protectors in Azure SQL Database and Azure SQL Managed Instance. AES 256 is incorrect because TDE protectors must be asymmetric (RSA) keys, not symmetric (AES) keys.

**References**: 
- [TDE with Customer-Managed Keys](https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview)
- [Azure Key Vault - Supported Key Types and Sizes](https://learn.microsoft.com/en-us/azure/key-vault/keys/about-keys#supported-key-types-and-sizes)
- [TDE Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview)

#### Dynamic Data Masking

**Dynamic Data Masking (DDM)** limits sensitive data exposure by masking it to non-privileged users at the query level. It is a policy-based security feature that hides sensitive data in the result set of a query over designated database fields, while the data in the database is not changed.

**Key Characteristics**:

| Feature | Description |
|---------|-------------|
| **Masking Level** | Query result level - masks data returned from queries |
| **Data Storage** | Actual data remains unchanged in the database |
| **Access Control** | Non-privileged users see masked values; privileged users see actual data |
| **Configuration** | Define masking rules on specific columns |
| **Performance Impact** | Minimal - masking applied at query time |

**Built-in Masking Functions**:

| Function | Description | Example |
|----------|-------------|------|
| **Default** | Full masking based on data type | `XXXX` for strings, `0` for numbers |
| **Email** | Exposes first letter and domain suffix | `aXXX@XXXX.com` |
| **Random** | Random number within specified range | Numeric value between min and max |
| **Custom String** | Exposes prefix and suffix with custom padding | `[prefix][padding][suffix]` |

**How Dynamic Data Masking Works**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Azure SQL Database                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Table: Customers                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Name     ‚îÇ Email           ‚îÇ CreditCard           ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ John Doe ‚îÇ john@email.com  ‚îÇ 4532-1234-5678-9012  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚ñº                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Privileged User Query      ‚îÇ     ‚îÇ  Non-Privileged User Query  ‚îÇ
‚îÇ  (UNMASK permission)        ‚îÇ     ‚îÇ  (No UNMASK permission)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  John Doe                   ‚îÇ     ‚îÇ  XXXX                       ‚îÇ
‚îÇ  john@email.com             ‚îÇ     ‚îÇ  jXXX@XXXX.com              ‚îÇ
‚îÇ  4532-1234-5678-9012        ‚îÇ     ‚îÇ  XXXX-XXXX-XXXX-9012        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Granting UNMASK Permission**:

```sql
-- Grant UNMASK permission to a specific user
GRANT UNMASK TO [DataAnalyst];

-- Grant UNMASK on specific columns only (SQL Server 2022 / Azure SQL)
GRANT UNMASK ON Schema.Table(Column) TO [LimitedAnalyst];

-- Revoke UNMASK permission
REVOKE UNMASK FROM [DataAnalyst];
```

**When to Use Dynamic Data Masking**:
- ‚úÖ Protecting PII (names, addresses, phone numbers) from non-privileged users
- ‚úÖ Call center applications where agents need partial data visibility
- ‚úÖ Development/test environments needing production-like data
- ‚úÖ Compliance scenarios requiring data minimization
- ‚úÖ Multi-tier applications with different user access levels

**Limitations**:
- ‚ùå Users with elevated permissions can still query underlying data
- ‚ùå Not a replacement for encryption (data is stored unmasked)
- ‚ùå Can be bypassed through inference attacks with sufficient queries
- ‚ùå Does not protect data at rest or in transit

**Comparison: Dynamic Data Masking vs. Other Data Protection Methods**:

| Scenario | Best Solution | Why |
|----------|---------------|-----|
| **Restrict PII visibility to non-privileged users** | **Dynamic Data Masking** | Masks data at query level; privileged users can see actual values |
| **Identify and label sensitive data** | Data Discovery and Classification | Governance tool for discovering and tagging sensitive data |
| **Protect data at rest from physical theft** | Transparent Data Encryption (TDE) | Encrypts entire database but doesn't restrict query access |
| **Control access to database objects** | Role-Based Access Control (RBAC) | Database-level access control, not fine-grained PII masking |
| **Protect sensitive columns end-to-end** | Always Encrypted | Client-side encryption for maximum protection |

**Important Distinctions**:

| Method | Controls PII Visibility at Query Level? | Protects Data at Rest? | Fine-Grained Column Control? |
|--------|----------------------------------------|------------------------|-----------------------------|
| **Dynamic Data Masking** | ‚úÖ Yes | ‚ùå No | ‚úÖ Yes |
| **Data Discovery & Classification** | ‚ùå No (discovery only) | ‚ùå No | N/A |
| **Transparent Data Encryption (TDE)** | ‚ùå No | ‚úÖ Yes | ‚ùå No (entire database) |
| **Role-Based Access Control (RBAC)** | ‚ùå No (object-level only) | ‚ùå No | ‚ùå No |

> **Exam Tip:** When a scenario asks how to ensure that **only privileged users can view PII** while allowing other users to query the data, **Dynamic Data Masking** is the correct answer. TDE protects data at rest but doesn't prevent users from seeing data when querying. RBAC controls access at the database/table level but doesn't mask individual column values. Data Discovery and Classification identifies sensitive data but doesn't restrict access to it.

**References**: 
- [Dynamic Data Masking Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview)
- [TDE Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview)
- [Azure RBAC Overview](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)

### Migration to Azure SQL

For comprehensive information about migrating SQL Server databases to Azure, including detailed tool comparisons, best practices, and step-by-step workflows, see the dedicated **[Azure SQL Migration Guide](./azure-sql-migration-guide.md)**.

**Quick Migration Tool Reference**:

| Your Requirement | Recommended Tool |
|------------------|------------------|
| **Offline migration + Minimize admin effort** | Azure Database Migration Service |
| **Online migration + Minimize downtime** | Azure Data Studio (with Azure SQL Migration extension) |
| **Assessment before migration** | Azure Migrate or Data Migration Assistant |
| **Migrating from Oracle/MySQL** | SQL Server Migration Assistant |

For detailed explanations, real-world scenarios, and migration workflows, refer to the [Azure SQL Migration Guide](./azure-sql-migration-guide.md).

### Migration Tools Comparison (Quick Summary)

**Azure Database Migration Service (DMS)**: Fully managed service for offline migrations with minimal administrative effort. Best for large-scale migrations (50+ databases).

**Azure Data Studio**: Cross-platform tool with Azure SQL Migration extension for online migrations with minimal downtime. Ideal for production databases requiring continuous availability.

**Azure Migrate**: Assessment and discovery platform for migration planning, cost estimation, and compatibility analysis.

**Data Migration Assistant (DMA)**: Assessment-only tool for identifying compatibility issues before migration.

**SQL Server Migration Assistant (SSMA)**: For heterogeneous migrations from Oracle, MySQL, DB2 to SQL Server/Azure SQL.

> üìö **For detailed tool comparisons, workflows, and real-world scenarios, see the [Azure SQL Migration Guide](./azure-sql-migration-guide.md)**

## Common Scenarios and Recommendations

### Scenario 1: New Modern Web Application
**Requirements**: Auto-scaling, cost optimization, simple management

**Recommendation**: 
- **Azure SQL Database** (Single Database)
- **Serverless** compute tier
- **General Purpose** service tier

**Why**: Modern cloud-native design, per-second billing, auto-scaling, minimal management

### Scenario 2: Multi-Tenant SaaS Application
**Requirements**: Multiple databases, cost efficiency, varying usage patterns

**Recommendation**:
- **Azure SQL Database** (Elastic Pool)
- **Provisioned** compute tier
- **General Purpose** service tier

**Why**: Resource sharing across tenants, cost-effective, pool-level management

### Scenario 3: Migrating SQL Server Enterprise
**Requirements**: Minimal code changes, SQL Agent, cross-database queries

**Recommendation**:
- **Azure SQL Managed Instance**
- **General Purpose** service tier

**Why**: Near 100% SQL Server compatibility, instance-scoped features, lift-and-shift

### Scenario 4: Mission-Critical OLTP Application
**Requirements**: Low latency, high availability, read scale-out

**Recommendation**:
- **Azure SQL Database** (Single Database)
- **Provisioned** compute tier
- **Business Critical** service tier
- **Zone redundancy** enabled

**Why**: <2ms latency, local SSD, built-in read replica, 99.995% SLA

### Scenario 5: Large Data Warehouse
**Requirements**: 10 TB database, multiple read workloads, fast scaling

**Recommendation**:
- **Azure SQL Database** (Hyperscale)
- **Provisioned** compute tier
- **4 read replicas**

**Why**: Up to 100 TB storage, fast scaling, multiple read replicas, cost-effective for large DBs

### Scenario 6: Development and Test Environment
**Requirements**: Used 8 hours/day, cost optimization, temporary databases

**Recommendation**:
- **Azure SQL Database** (Single Database)
- **Serverless** compute tier (0.5-4 vCores)
- **General Purpose** service tier
- **Auto-pause enabled** (60 min)

**Why**: Up to 80% cost savings, pay per second of use, auto-pause during inactivity

### Scenario 7: Enterprise Migration with High Resiliency Requirements (Litware Inc. Case Study)
**Requirements**: 
- Migrating on-premises databases (DB1 and DB2) to Azure
- Must maintain availability if two availability zones in the region fail
- Minimal I/O latency for optimal performance
- Minimal administrative effort and operational complexity
- Near 100% SQL Server feature compatibility
- Support for advanced features like automatic failover and geo-replication

**Recommendation**:
- **Azure SQL Managed Instance**
- **Business Critical** service tier (for optimal I/O latency)
- **Zone-redundant configuration** enabled
- **Geo-replication** for disaster recovery

**Why**: 
- **Resiliency**: Azure SQL Managed Instance supports zone-redundant configuration and automatic failover, ensuring availability even if two availability zones fail. Geo-replication provides additional disaster recovery capabilities.
- **Performance**: Business Critical tier uses local SSD storage, delivering minimal I/O latency required for DB1 and DB2.
- **Minimal Administrative Effort**: As a fully managed PaaS service, it offloads tasks such as automated backups, patching, and high availability setup, significantly reducing operational complexity.
- **SQL Server Compatibility**: Near 100% compatibility with on-premises SQL Server Enterprise Edition, allowing lift-and-shift migrations with minimal code changes.
- **Advanced Features**: Supports enterprise-grade capabilities like SQL Server Agent, Service Broker, cross-database queries, and distributed transactions.

**Why Other Options Are Incorrect**:
- **Single Azure SQL Database**: While it can be made zone-redundant in premium tiers, it lacks the broad SQL Server feature compatibility and enterprise-grade scalability required for complex workloads like DB1 and DB2. It's best suited for standalone or less complex databases.
- **Azure SQL Database Elastic Pool**: Designed for managing multiple databases with varying usage patterns in a cost-effective manner. It does not offer the resiliency features (zone redundancy across multiple zones) or low-latency performance needed for mission-critical databases that must remain available during zone failures.

**Reference Links**:
- [Azure SQL High Availability SLA](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla)
- [Azure SQL Managed Instance PaaS Overview](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview?view=azuresql)

### Scenario 8: Multiple Databases with Varying Usage Patterns
**Requirements**: 
- 20 databases that will be 20 GB each
- Varying usage patterns across databases
- 99.99% uptime SLA
- Dynamic compute resource scaling
- Reserved capacity support
- Minimize compute charges

**Recommendation**:
- **Azure SQL Database Elastic Pool**
- **General Purpose** service tier (or higher as needed)
- **vCore-based** purchasing model (for reserved capacity)
- Pool size configured based on aggregate usage patterns

**Why**: 
- **Resource Sharing**: Elastic pools allow multiple databases to share a pool of compute and storage resources, making them ideal for scenarios with varying usage patterns across databases. Databases can consume resources as needed from the shared pool, optimizing cost efficiency.
- **99.99% SLA**: Built-in high availability meets the uptime requirement.
- **Dynamic Scaling**: Elastic pools can automatically scale compute within the pool based on actual usage, addressing the need for dynamic resource allocation.
- **Reserved Capacity**: Supports reserved capacity through the vCore purchasing model, providing cost savings for predictable workloads.
- **Cost Minimization**: Pooling resources instead of provisioning separate resources for each database significantly reduces compute charges, especially when databases have complementary usage patterns (when some databases are busy, others may be idle).

**Why Other Options Are Incorrect**:
- **20 databases on a Microsoft SQL server on an Azure VM in an availability set**: While this provides control and high availability through availability sets, it does not support dynamic compute scaling and typically results in higher operational overhead and cost compared to PaaS offerings like Azure SQL. It also requires manual management of infrastructure, which goes against the goal of minimizing compute charges and ensuring simplicity.
- **20 instances of Azure SQL Database serverless**: Although serverless can scale compute automatically and pause during inactivity to save costs, it does NOT support reserved capacity pricing, which is a requirement in this scenario. Serverless is best suited for intermittent workloads but not when reserved compute capacity is needed. Additionally, managing 20 separate serverless databases would be more complex and potentially more expensive than using a single elastic pool.
- **20 databases on Microsoft SQL server on an Azure VM**: This option lacks dynamic scaling capabilities, does not support reserved PaaS-level pricing benefits, and requires managing infrastructure. This violates the requirement to minimize compute charges and ensure operational simplicity.

**Reference Links**:
- [Elastic Pool Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview)
- [Azure SQL Database Reserved Capacity](https://learn.microsoft.com/en-us/azure/azure-sql/database/reserved-capacity-overview)
- [Serverless Tier Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview)

### Scenario 9: SQL Server VM Storage Configuration
**Requirements**: 
- Virtual machine running Microsoft SQL Server
- Two P40 managed disks: one for log files, one for data files
- Need best overall performance
- Must preserve integrity of SQL data and logs

**Recommendation**:
- **SQL Server on Azure VM** (IaaS)
- **Data disk**: ReadOnly host caching
- **Transaction log disk**: None host caching
- **Premium SSD** (P40) managed disks

**Why**: 
- **Data Disk (ReadOnly Caching)**: Offers performance benefits through host caching while maintaining data integrity. SQL Server workloads typically involve frequent read operations from data files. Enabling ReadOnly caching allows Azure to cache data in the host machine's memory for faster retrieval, improving overall read performance. This setting is commonly recommended by Microsoft for SQL Server data disks as it optimizes read-intensive operations without risking corruption or integrity issues. Reads from cache will be faster than uncached reads, and the cache hit ratio determines actual performance gains.

- **Transaction Log Disk (None Caching)**: Disables host caching completely for optimal write performance. While it's safe in terms of data integrity, sequential write operations to transaction logs don't benefit from caching. In fact, enabling caching on transaction log disks can degrade performance of writes and decrease the amount of cache available for reads on the data drive. Setting caching to None ensures maximum throughput for transaction log writes and preserves cache resources for data disk operations.

**Why Other Options Are Incorrect**:
- **None caching for data disk**: While safe for data integrity, it results in slower read performance for database queries that access data files frequently. This doesn't provide the best overall performance, even though it maintains integrity.

- **ReadWrite caching for data disk**: Introduces the risk of data corruption, especially with transactional systems like SQL Server. Caching writes in host memory can lead to data loss during unexpected failures (such as VM crashes or power outages), making it unsuitable for SQL data or log disks where write consistency and durability are critical. This violates the requirement to preserve data integrity.

- **ReadOnly or ReadWrite caching for transaction log disk**: Provides no performance benefit for sequential write operations characteristic of transaction logs. Can actually degrade write performance and reduce available cache for data disk reads.

**Performance Impact**:
- Data disk with ReadOnly: Faster query performance for read-heavy workloads
- Transaction log with None: Maximum write throughput, no caching overhead
- Combined configuration: Optimal balance of read and write performance
- Cache hit ratio of 70%+ on data disk provides significant performance gains

**Reference Links**:
- [SQL Server on Azure VMs - Storage Performance Best Practices](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices-storage?view=azuresql#data-file-caching-policies)
- [Azure Managed Disks - Host Caching](https://learn.microsoft.com/en-us/azure/virtual-machines/premium-storage-performance#disk-caching)

---

### Scenario 10: Web App with PII Data and Auto-Scaling Requirements
**Requirements**: 
- Azure web app storing Personally Identifiable Information (PII) of employees
- Maintain availability in the event of a single datacenter outage
- Support encryption of specific columns containing PII
- Automatically scale up during payroll operations (bursty workloads)
- Minimize costs

**Recommendation**:
- **Azure SQL Database** (Single Database)
- **General Purpose** service tier
- **Serverless** compute tier
- **Zone-redundant configuration** enabled
- **Always Encrypted** for column-level PII protection

**Why**: 
- **Datacenter Outage Protection**: The General Purpose tier supports **Zone-Redundant Configuration (ZRC)**, which distributes database replicas across Availability Zones within a region, ensuring resilience against a single datacenter failure.

- **Column-Level Encryption for PII**: **All Azure SQL Database tiers support Always Encrypted**, which enables column-level encryption. This is ideal for protecting Personally Identifiable Information (PII) such as social security numbers, salaries, and other sensitive employee data. Always Encrypted ensures data remains encrypted at rest, in transit, and during query processing.

- **Automatic Scaling for Payroll Operations**: The **Serverless compute tier** allows automatic scaling of compute resources based on workload demand. It supports auto-pausing during periods of inactivity and auto-resuming when connections are made. This is perfect for bursty workloads like monthly payroll processing where demand spikes temporarily.

- **Cost Minimization**: Serverless with the General Purpose tier is the most cost-effective option among the viable choices:
  - Per-second billing for actual compute used
  - Auto-pause reduces costs during idle periods
  - General Purpose is cheaper than Business Critical or Hyperscale
  - Designed for intermittent or variable workloads

**Why Other Options Are Incorrect**:

‚ùå **Business Critical service tier and Serverless compute tier**:
- **Business Critical does NOT support Serverless compute**. This combination is invalid.
- Business Critical is designed for mission-critical workloads with low-latency, high-throughput needs using local SSD storage.
- It's more expensive and exceeds the cost requirements for this scenario.
- While it provides zone redundancy and supports Always Encrypted, the lack of Serverless support and higher costs make it unsuitable.

‚ùå **Hyperscale service tier and Provisioned compute tier**:
- **Hyperscale does NOT support Serverless compute** (only limited preview support).
- Hyperscale is designed for massive, high-ingestion workloads with databases up to 100 TB.
- It's not cost-effective for typical web app workloads.
- While it supports zone redundancy and Always Encrypted, the provisioned compute model doesn't provide automatic scaling, and the tier is unnecessary for this scenario.

**Service Tier and Serverless Compatibility**:

| Service Tier | Serverless Support | Zone Redundancy | Always Encrypted |
|--------------|-------------------|-----------------|------------------|
| **General Purpose** | ‚úÖ **Yes** | ‚úÖ Yes | ‚úÖ Yes |
| **Business Critical** | ‚ùå **No** | ‚úÖ Yes | ‚úÖ Yes |
| **Hyperscale** | ‚ö†Ô∏è Preview only | ‚úÖ Yes | ‚úÖ Yes |

> **Exam Tip:** When a scenario requires automatic scaling (Serverless compute) AND cost minimization, **General Purpose with Serverless** is typically the correct answer. Remember:
> - Serverless is **only available in General Purpose tier**
> - **All tiers support Always Encrypted** for column-level encryption
> - **Zone redundancy is available in General Purpose**, not just Business Critical
> - Business Critical and Hyperscale are more expensive and don't support Serverless

**Reference Links**:
- [Azure SQL Database Service Tiers (vCore)](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)
- [Azure SQL Database Serverless](https://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?view=azuresql)
- [Always Encrypted](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine)
- [Zone-Redundant Configuration](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla-local-zone-redundancy?view=azuresql)

---

### Scenario 11: Intermittent Monthly Workload Migration
**Requirements**: 
- Migrate on-premises SQL Server databases to Azure with the following sizes:

| Name | Storage |
|------|---------|
| DB1  | 450 GB  |
| DB2  | 250 GB  |
| DB3  | 300 GB  |
| DB4  | 50 GB   |
| **Total** | **1,050 GB** |

- Application and data used only on the first day of each month
- Data growth expected to be less than 3% annually
- Need to minimize costs while ensuring database availability for monthly usage

**Recommendation**:
- **Azure SQL Database** (Single Database)
- **vCore-based General Purpose** service tier
- **Serverless** compute tier
- Auto-pause delay configured appropriately (e.g., 1-2 hours after inactivity)

**Why**: 
- **Serverless Compute with Auto-Pause**: The serverless compute tier supports automatic pausing and resuming of the database based on activity. Since the requirement specifies the database is only used on the first day of each month, the serverless capability enables the database to be paused for the remaining ~29 days, significantly reducing costs. During the paused state, you're only billed for storage, not compute, which aligns perfectly with the intermittent usage pattern.
- **Cost Optimization**: With serverless, you can save up to 80% compared to provisioned compute for intermittent workloads. The database automatically resumes when accessed on the first day of the month and pauses again after the configured inactivity period.
- **Scalability**: General Purpose with serverless can handle the database size requirements (supports up to 4 TB) and the minimal annual growth of 3%.
- **Per-Second Billing**: You only pay for compute resources when the database is active, making it ideal for monthly usage patterns.

**Cost Example**:
```
Active usage: 1 day/month (~24 hours)
Paused: 29 days/month
Average compute: 4 vCores during active time

Compute cost: 24 hours √ó 4 vCores √ó ~$0.20/vCore-hour = ~$19.20/month
Storage cost (1 TB): 1024 GB √ó $0.12/GB = ~$122.88/month
Total: ~$142/month (vs ~$720/month with provisioned compute)
```

**Why Other Options Are Incorrect**:
- **DTU-based Basic**: Incorrect because it supports databases only up to 2 GB in size, whereas the combined database size from the scenario exceeds 1 TB. This tier cannot accommodate the storage requirements.
- **DTU-based Standard**: Incorrect because although it supports larger database sizes (up to 1 TB per database), DTUs are always provisioned and billed regardless of actual usage. Since the app is only used once a month, this model would result in paying for 29 days of unused compute resources, leading to unnecessary costs. DTU-based models do not support the serverless auto-pause capability.
- **vCore-based Business Critical**: Incorrect because it is designed for mission-critical applications requiring the highest performance, lowest latency (<2ms), and built-in high availability through multiple replicas. The scenario does not indicate the need for such performance or availability requirements. Additionally, Business Critical does not support the serverless compute tier, making it unable to pause during inactive periods. This tier would be unnecessarily expensive for an intermittent monthly workload.

**Key Considerations**:
- **Auto-Pause Configuration**: Set an appropriate auto-pause delay (1-2 hours) to ensure the database pauses after completion of monthly processing
- **Resume Time**: First connection after pause will have a brief delay (few seconds) while the database resumes
- **Serverless Availability**: Only available in General Purpose tier, not Business Critical
- **Storage Billing**: Storage is billed continuously even when paused, but at a much lower rate than compute

**Reference Links**:
- [Azure SQL Database Serverless Tier Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?view=azuresql&tabs=general-purpose)
- [Azure SQL Database General Purpose Service Tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-general-purpose)
- [Azure SQL Database DTU Service Tiers](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu)
- [Azure SQL Database DTU Resource Limits](https://learn.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-single-databases?view=azuresql)

**Domain**: Design data storage solutions

---

### Scenario 11: Data Warehouse for Reporting with High Concurrent Read Operations
**Requirements**: 
- Design a data storage solution to support reporting
- Ingest high volumes of JSON data using Azure Event Hubs
- Data organized in directories by date and time
- Stored data must be queryable directly, transformed into summarized tables, and stored in a data warehouse
- Data warehouse must store **50 TB of relational data**
- Support **200-300 concurrent read operations**

**Recommendation**:
- **Azure SQL Database Hyperscale** for the data warehouse

**Why**: 
- **Large-Scale Storage**: Azure SQL Database Hyperscale is specifically designed to handle large relational datasets, supporting up to **100 TB of storage**, which comfortably accommodates the 50 TB requirement.
- **Independent Compute and Storage Scaling**: Hyperscale offers flexibility and cost efficiency by allowing compute and storage to scale independently, which is ideal for workloads with varying read demands.
- **High Concurrent Read Support**: The Hyperscale tier can handle 200-300 concurrent read operations efficiently, making it suitable for enterprise-grade reporting workloads.
- **High Performance**: Delivers fast backups and high performance, suitable for enterprise-grade data workloads.

**Why Other Options Are Incorrect**:
- **Azure Cosmos DB Cassandra API**: Incorrect because it is intended for NoSQL column-family workloads and not designed for storing or querying large volumes of relational, structured data. It does not support analytical queries efficiently and is not cost-effective for 50 TB data storage.
- **Azure Cosmos DB SQL API**: Incorrect because although it supports JSON-based document storage with flexible schema, it is not a relational database and is unsuitable for warehouse-scale storage or complex SQL-based analytics at the scale described.
- **Azure Synapse Analytics dedicated SQL pools**: Incorrect in this context because while it is designed for large-scale analytical processing, it is more suitable when working with pre-aggregated or summarized data from a data lake rather than being used as a primary data warehouse for high-volume relational storage with frequent concurrent read access. When relational consistency, transactional support, and high concurrent access are required, Hyperscale is more appropriate.

**Architecture Pattern**:
```
Azure Event Hubs (JSON ingestion) 
    ‚Üí Azure Data Lake Storage (organized by date/time directories)
    ‚Üí Data transformation (Azure Data Factory / Databricks)
    ‚Üí Azure SQL Database Hyperscale (data warehouse for reporting)
```

**Reference Links**:
- [Azure SQL Database Hyperscale](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)
- [Azure SQL Managed Instance Resource Limits](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/resource-limits?view=azuresql)
- [Azure Cosmos DB Introduction](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)
- [Azure Synapse Analytics Compute Management](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-compute-overview)

**Domain**: Design data storage solutions

---

### Scenario 12: Large-Scale Database Deployment with License Mobility
**Requirements**: 
- Deploy 50 databases to Azure
- Microsoft Volume Licensing customer with License Mobility through Software Assurance
- Support automatic scaling
- Minimize Microsoft SQL Server licensing costs

**Recommendation**:
- **Azure SQL Database** with **vCore-based** purchasing model
- **General Purpose** or **Business Critical** service tier (based on performance needs)
- **Serverless** compute tier (for automatic scaling requirements)
- **Azure Hybrid Benefit** enabled (to leverage existing licenses)

**Why**: 
- **License Mobility Support**: The vCore (virtual core) purchase model in Azure SQL Database supports License Mobility through Software Assurance. Since the customer is a Microsoft Volume Licensing customer participating in Software Assurance, they are eligible for the **Azure Hybrid Benefit**, allowing them to bring their own SQL Server licenses to Azure and significantly reduce SQL licensing costs (up to 55% savings on compute costs).
- **Automatic Scaling**: The vCore model supports automatic scaling through the serverless compute tier, which automatically scales compute based on workload demand and can pause during periods of inactivity.
- **Transparent Resource Control**: The vCore model offers transparent control over compute, memory, and I/O resources, making it ideal for deploying and managing a large number of databases efficiently.
- **Cost Optimization**: Combining Azure Hybrid Benefit with reserved capacity can provide up to 80% savings compared to pay-as-you-go pricing.

**Why Other Options Are Incorrect**:
- **DTU (Database Transaction Unit) model**: Incorrect because the DTU model is a bundled compute model with less flexibility. It **does not support Azure Hybrid Benefit or License Mobility through Software Assurance**, and it lacks detailed visibility and control over resource consumption. This leads to higher licensing costs for customers eligible for bring-your-own-license (BYOL) models.
- **Azure reserved virtual machine instances**: Incorrect because while reserved VM instances help reduce costs for SQL Server on Azure VMs, they apply to **IaaS-based deployments**, not PaaS-based Azure SQL Databases. Additionally, managing 50 databases on VMs would significantly increase operational overhead and not support automatic scaling as efficiently as the PaaS model under vCore.

**Azure Hybrid Benefit Savings Example**:
```
Without Azure Hybrid Benefit (Pay-as-you-go):
50 databases √ó 4 vCores √ó $720/month = $36,000/month

With Azure Hybrid Benefit (BYOL):
50 databases √ó 4 vCores √ó $324/month = $16,200/month

Savings: $19,800/month (55%)

With Azure Hybrid Benefit + 3-year Reserved Capacity:
50 databases √ó 4 vCores √ó $145/month = $7,250/month

Total Savings: $28,750/month (80%)
```

**Key Considerations**:
- ‚úÖ vCore model is required for Azure Hybrid Benefit
- ‚úÖ License Mobility allows using existing SQL Server licenses in Azure
- ‚úÖ Serverless tier provides automatic scaling within vCore model
- ‚úÖ Elastic pools can further optimize costs for 50 databases with varying usage
- ‚ùå DTU model does not support Azure Hybrid Benefit
- ‚ùå Azure VMs increase operational overhead compared to PaaS

**Reference Links**:
- [vCore-based Purchasing Model](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)
- [Azure SQL Database Purchasing Models](https://learn.microsoft.com/en-us/azure/azure-sql/database/purchasing-models?view=azuresql)
- [Azure Hybrid Benefit for Azure SQL](https://learn.microsoft.com/en-us/azure/azure-sql/azure-hybrid-benefit?view=azuresql)
- [Azure SQL Database Reserved Capacity](https://learn.microsoft.com/en-us/azure/azure-sql/database/reserved-capacity-overview)

**Domain**: Design data storage solutions

---

### Scenario 13: Multiple Database Deployment with License Mobility and Cost Optimization
**Requirements**: 
- Deploy 50 databases to Azure
- Microsoft Volume Licensing customer with License Mobility through Software Assurance
- Support automatic scaling
- Minimize Microsoft SQL Server licensing costs
- Databases have varying and unpredictable usage patterns

**Recommendation**:
- **Azure SQL Database Elastic Pool** with **vCore-based** purchasing model
- **General Purpose** service tier (or Business Critical based on performance needs)
- **Azure Hybrid Benefit** enabled (to leverage existing licenses)

**Why**: 
- **Resource Sharing and Cost Efficiency**: Elastic pools allow you to deploy and manage multiple databases (such as 50 in this case) that share the same compute and storage resources. This enables cost efficiency by pooling resources across databases with varying and unpredictable usage patterns.
- **Automatic Scaling**: Elastic pools support automatic scaling based on individual database usage patterns within the pool. When some databases are busy while others are idle, resources are dynamically allocated, helping minimize overall licensing and compute costs.
- **License Mobility Support**: Since Contoso uses License Mobility through Software Assurance, they can take advantage of the **Azure Hybrid Benefit** under the vCore-based model within elastic pools, further reducing SQL Server licensing costs.
- **Simplified Management**: Managing 50 databases in an elastic pool is significantly simpler than managing them individually or on managed instances.

**Why Other Options Are Incorrect**:
- **Azure SQL Managed Instance**: Incorrect because while it supports license mobility and is suitable for migrating on-premises SQL Server instances, it is **not optimized for managing a large number of small or variable-load databases**. Deploying 50 separate databases on managed instances would be more complex and expensive compared to using an elastic pool. Managed Instance is better suited for lift-and-shift scenarios requiring instance-scoped features.
- **SQL Server Always On availability group**: Incorrect because it applies to **IaaS-based SQL Server deployments** running on Azure VMs, which involve higher licensing costs, manual scaling, and increased management overhead. It does not natively support automatic scaling or cost optimization across multiple databases like elastic pools do.

**Elastic Pool Cost Optimization Example**:
```
Without Elastic Pool (50 separate databases):
50 databases √ó 4 vCores √ó $720/month = $36,000/month

With Elastic Pool (shared resources):
1 pool √ó 50 vCores √ó $3,600/month = $3,600/month
(assuming databases share resources efficiently)

With Elastic Pool + Azure Hybrid Benefit:
1 pool √ó 50 vCores √ó $1,620/month = $1,620/month

Savings: Up to 95% compared to individual databases
```

**Key Considerations**:
- ‚úÖ Elastic pools are ideal for multiple databases with varying usage patterns
- ‚úÖ vCore model within elastic pools supports Azure Hybrid Benefit
- ‚úÖ Resources are shared and automatically allocated based on demand
- ‚úÖ Simplified management compared to individual databases or managed instances
- ‚ùå Managed Instance is overkill for simple database hosting
- ‚ùå SQL Server on VMs requires manual scaling and higher operational overhead

**Deployment Option Comparison**:

| Feature | Elastic Pool | Managed Instance | SQL Server Always On (VMs) |
|---------|-------------|------------------|---------------------------|
| License Mobility | ‚úÖ Yes (vCore) | ‚úÖ Yes | ‚úÖ Yes |
| Azure Hybrid Benefit | ‚úÖ Yes (vCore) | ‚úÖ Yes | ‚úÖ Yes |
| Automatic Scaling | ‚úÖ Pool-level | ‚ùå Manual | ‚ùå Manual |
| Resource Sharing | ‚úÖ Yes | ‚ùå No | ‚ùå No |
| Cost for 50 DBs | üí∞ Low | üí∞üí∞üí∞ High | üí∞üí∞üí∞üí∞ Highest |
| Management Overhead | Low (PaaS) | Low (PaaS) | High (IaaS) |
| Best for Variable Workloads | ‚úÖ Ideal | ‚ùå Not optimal | ‚ùå Not optimal |

**Reference Links**:
- [Azure SQL Database Elastic Pool Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview)
- [SQL Managed Instance Overview](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview?view=azuresql)
- [SQL Server Always On Availability Groups on Azure VMs](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/availability-group-overview?view=azuresql)

**Domain**: Design data storage solutions

---

## Exam Practice Questions

### Question 1: Enterprise Database Migration with Resiliency Requirements (Litware Inc.)

**Scenario**: Refer to the Litware Inc. case study. You plan to migrate DB1 and DB2 to Azure. You need to ensure that the Azure database and the service tier meet the resiliency and business requirements.

**Question**: What should you configure for the Database?

**Options**:
- A) A single Azure SQL database
- B) Azure SQL managed instance
- C) An Azure SQL Database elastic pool

**Correct Answer**: **B) Azure SQL managed instance**

**Explanation**:

Azure SQL managed instance is correct because it allows you to bring your on-premises SQL Server databases to Azure with minimal administrative effort while maintaining high compatibility with existing SQL Server features. It supports advanced capabilities like automatic failover, zone-redundant configuration, and geo-replication, which satisfy the requirement to maintain availability if two availability zones in the region fail. Additionally, it is optimized for minimal I/O latency, meeting the performance expectations of DB1 and DB2. As a fully managed service, it also offloads tasks such as backups, patching, and high availability setup, directly supporting the business requirement to minimize both administrative effort and operational complexity.

**Why Other Options Are Incorrect**:

**A single Azure SQL database** is incorrect because it is best suited for standalone or less complex databases and may not support all SQL Server features. While it can be made zone-redundant in premium tiers, it lacks the broad feature compatibility and scalability required for enterprise-grade workloads like DB1 and DB2.

**An Azure SQL Database elastic pool** is incorrect because it is designed to manage multiple databases with varying usage patterns in a cost-effective manner. It does not offer the resiliency features or low-latency performance needed for critical databases that must remain available and performant during zone failures.

**Reference Links**:
- [Azure SQL High Availability SLA](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla)
- [Azure SQL Managed Instance PaaS Overview](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview?view=azuresql)

**Domain**: Design data storage solutions

---

### Question 2: Service Tier Selection for High-Availability Requirements (Litware Inc.)

**Scenario**: Refer to the Litware Inc. case study. You plan to migrate DB1 and DB2 to Azure. You need to ensure that the Azure database and the service tier meet the resiliency and business requirements.

**Question**: What should you configure for the Service tier?

**Options**:
- A) Hyperscale
- B) Business Critical
- C) General Purpose

**Correct Answer**: **B) Business Critical**

**Explanation**:

**Business Critical** is correct because it is the only tier that meets all three resiliency requirements specified in the case study: 
1. Maintain availability even if two availability zones fail
2. Fail over automatically
3. Minimize I/O latency

The Business Critical tier is built for high-performance transactional workloads and provides zone-redundant configurations through synchronous replication across availability zones. It also uses local SSD-based storage, which ensures low latency and high throughput. Furthermore, since it is a platform-as-a-service (PaaS) offering, it also meets the requirement to minimize administrative effort.

**Why Other Options Are Incorrect**:

**General Purpose** is incorrect because although it offers zone redundancy, it relies on remote storage, which introduces higher I/O latency and lower throughput. This violates the requirement to minimize I/O latency. Additionally, the resiliency through zone redundancy in this tier is not generally available in all regions, making it less reliable for mission-critical workloads that demand proven zone failure resilience.

**Hyperscale** is incorrect because it is designed for extremely large databases and scale-out read scenarios, not high-availability transactional systems. Importantly, Hyperscale does not currently support zone redundancy, so it cannot meet the requirement to maintain availability if two availability zones fail, making it an unsuitable choice in this scenario.

**Reference Links**:
- [Azure SQL Database Service Tiers - When to Choose Business Critical](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql#when-to-choose-this-service-tier-1)
- [Azure SQL High Availability SLA](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla)

**Domain**: Design data storage solutions

---

### Question 3: Azure SQL Database Auditing Storage Requirements

**Scenario**: You have an Azure subscription that contains the following resources:

**SQL Servers**:
| Name | Resource Group | Location |
|------|----------------|----------|
| SQLsvr1 | RG1 | East US |
| SQLsvr2 | RG2 | West US |

**Storage Accounts**:
| Name | Resource Group | Location | Account Kind |
|------|----------------|----------|--------------|
| storage1 | RG1 | East US | StorageV2 (general purpose v2) |
| storage2 | RG2 | Central US | BlobStorage |

**Azure SQL Databases**:
| Name | Resource Group | Server | Pricing Tier |
|------|----------------|--------|--------------|
| SQLdb1 | RG1 | SQLsvr1 | Standard |
| SQLdb2 | RG1 | SQLsvr1 | Standard |
| SQLdb3 | RG2 | SQLsvr2 | Premium |

**Question**: When you enable auditing for SQLdb1, can you store the audit information to storage1?

**Options**:
- A) Yes
- B) No

**Correct Answer**: **A) Yes**

**Explanation**:

Yes is correct because SQLdb1 is a Standard tier Azure SQL Database hosted on SQLsvr1, which is deployed in the East US region. The target storage account storage1 is also located in East US, which meets the requirement for storing audit logs ‚Äî **the storage account must be in the same region as the SQL server**.

Additionally, SQL auditing is supported for databases in the Standard tier, and since both the database and the storage account reside in the same region, storing audit logs from SQLdb1 to storage1 is fully supported.

**Key Requirements for Azure SQL Database Auditing**:
1. ‚úÖ Storage account must be in the **same region** as the SQL server
2. ‚úÖ Auditing is supported across **all service tiers** (Basic, Standard, Premium)
3. ‚úÖ Both DTU-based and vCore-based purchasing models support auditing

**Analysis of the Scenario**:
- SQLdb1 is on SQLsvr1 (East US) ‚Üí storage1 (East US): ‚úÖ **Supported** (same region)
- SQLdb1 is on SQLsvr1 (East US) ‚Üí storage2 (Central US): ‚ùå **Not supported** (different regions)
- SQLdb3 is on SQLsvr2 (West US) ‚Üí storage1 (East US): ‚ùå **Not supported** (different regions)

**Reference**: [Azure SQL Database Auditing Overview](https://learn.microsoft.com/azure/azure-sql/database/auditing-overview)

**Domain**: Design data storage solutions

---

### Question 4: Elastic Pool for Multiple Databases with Varying Usage

**Scenario**: You are designing a SQL database solution. The solution will include 20 databases that will be 20 GB each and have varying usage patterns.

**Requirements**:
- The solution must meet a Service Level Agreement (SLA) of 99.99% uptime
- The compute resources allocated to the databases must scale dynamically
- The solution must have reserved capacity
- Compute charges must be minimized

**Question**: What should you include in the recommendation?

**Options**:
- A) An elastic pool that contains 20 Azure SQL databases
- B) 20 databases on a Microsoft SQL server that runs on an Azure virtual machine in an availability set
- C) 20 instances of Azure SQL Database serverless
- D) 20 databases on Microsoft SQL server that runs on an Azure virtual machine

**Correct Answer**: **A) An elastic pool that contains 20 Azure SQL databases**

**Explanation**:

**An elastic pool that contains 20 Azure SQL databases** is correct because it allows multiple Azure SQL databases to share a pool of compute and storage resources, making it ideal for scenarios with varying usage patterns across databases. Elastic pools provide built-in high availability with a 99.99% SLA, support reserved capacity through vCore purchasing models, and help minimize compute charges by pooling resources instead of provisioning separate resources for each database. Additionally, elastic pools can automatically scale compute within the pool, addressing the need for dynamic resource usage. When some databases in the pool are experiencing high demand, they can consume more resources, while idle databases use minimal resources, resulting in optimal cost efficiency.

**Why Other Options Are Incorrect**:

**20 databases on a Microsoft SQL server that runs on an Azure virtual machine in an availability set** is incorrect because while it gives you full control and high availability through availability sets, it does not support dynamic compute scaling and typically results in higher operational overhead and cost compared to PaaS offerings like Azure SQL. Managing SQL Server on VMs requires infrastructure management, patching, and maintenance, which increases complexity and cost.

**20 instances of Azure SQL Database serverless** is incorrect because although serverless can scale compute automatically and pause during inactivity to save costs, it does **not support reserved capacity pricing**, which is required in this scenario. Serverless is best suited for intermittent workloads but not when reserved compute is needed. Additionally, provisioning 20 separate serverless databases would lack the cost efficiency of a shared resource pool and could result in higher overall costs compared to an elastic pool.

**20 databases on Microsoft SQL server that runs on an Azure virtual machine** is incorrect for similar reasons. Although it provides control and compatibility, it lacks dynamic scaling, reserved PaaS-level pricing benefits, and requires managing infrastructure, which goes against the goal of minimizing compute charges and ensuring simplicity. You would need to manually scale VMs and manage all operational aspects.

**Key Advantages of Elastic Pools**:
- ‚úÖ Share resources across multiple databases with varying patterns
- ‚úÖ Built-in 99.99% SLA for high availability
- ‚úÖ Dynamic scaling at the pool level
- ‚úÖ Supports reserved capacity with vCore model
- ‚úÖ Significantly lower cost than individual databases
- ‚úÖ Minimal administrative overhead (fully managed PaaS)

**Reference Links**:
- [Elastic Pool Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview)
- [Azure SQL Database Reserved Capacity](https://learn.microsoft.com/en-us/azure/azure-sql/database/reserved-capacity-overview)
- [Serverless Tier Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview)

**Domain**: Design data storage solutions

---

### Question 5: SQL Server on Azure VM Disk Caching Configuration

**Scenario**: You are designing a virtual machine that will run Microsoft SQL Server and will contain two data disks. The first data disk will store log files, and the second data disk will store data. Both disks are P40 managed disks.

**Requirements**:
- Provide the best overall performance for the virtual machine
- Preserve integrity of the SQL data and logs

**Question**: Which host caching method should you recommend for the **Data disk**?

**Options**:
- A) None
- B) ReadOnly
- C) ReadWrite

**Correct Answer**: **B) ReadOnly**

**Explanation**:

**ReadOnly** is correct for the data disk because it offers performance benefits through host caching while maintaining data integrity. SQL Server workloads typically involve frequent read operations from data files. Enabling ReadOnly caching allows Azure to cache data in the host machine's memory for faster retrieval, improving overall read performance. This setting is commonly recommended by Microsoft for SQL Server data disks as it optimizes read-intensive operations without risking corruption or integrity issues.

**How ReadOnly Caching Works**:
- Reads are cached in the host machine's memory (faster access)
- Writes bypass the cache and go directly to disk (maintains integrity)
- Uncached IOPS and throughput **plus** Cached IOPS and throughput yield the total possible performance
- Actual performance varies based on the workload's cache hit ratio
- Typical cache hit ratios of 60-80% provide significant performance improvements

**Why Other Options Are Incorrect**:

**None** is incorrect because it disables host caching completely, resulting in slower read performance for database queries that access data files frequently. While it's safe in terms of data integrity, it doesn't provide the **best performance**. All read operations must go directly to the disk, missing out on the performance benefits of host-level caching.

**ReadWrite** is incorrect because it introduces the **risk of data corruption**, especially with transactional systems like SQL Server. Caching writes in host memory can lead to data loss during unexpected failures (such as VM crashes, host failures, or power outages), making it unsuitable for SQL data or log disks where write consistency and durability are critical. Even though it might seem to offer better performance, it violates the requirement to **preserve integrity** of the SQL data.

**Additional Context - Transaction Log Disk Recommendation**:

For the **Transaction log disk** in this scenario, the correct answer would be **None**:
- Set the caching policy to **None** for disks hosting the transaction log
- There's no performance benefit to enabling caching for transaction log disks (sequential writes don't benefit from caching)
- Having either ReadOnly or ReadWrite caching enabled on the log drive can **degrade performance** of the writes against the drive
- Caching also decreases the amount of cache available for reads on the data drive

**Complete Recommended Configuration**:

| Disk Type | Caching Policy | Reason |
|-----------|----------------|--------|
| **Data disk** | **ReadOnly** | Optimizes read performance, maintains integrity |
| **Transaction log disk** | **None** | Optimizes write performance, no caching benefit |
| **OS disk** | **ReadWrite** (default) | Don't change from default |
| **tempdb disk** | **ReadOnly** or **Ephemeral (D:\)** | Fast local storage |

**Performance Example**:
```
P40 Premium SSD Specifications:
- 7,500 IOPS (uncached)
- 250 MB/s throughput (uncached)

With ReadOnly Caching on Data Disk:
- 7,500 IOPS (uncached) + additional cached IOPS from host
- Cache hit ratio of 70% = 70% of reads served from fast host cache
- Result: Significantly improved query performance

With None Caching on Transaction Log Disk:
- 7,500 IOPS for sequential writes
- No caching overhead
- Maximum write throughput maintained
```

**Key Takeaways**:
- ‚úÖ Data disks: ReadOnly caching for optimal read performance
- ‚úÖ Transaction log disks: None caching for optimal write performance  
- ‚ùå Never use ReadWrite caching for SQL Server data or log disks
- ‚úÖ Cache hit ratio determines the actual performance benefit
- ‚úÖ This configuration provides the best **balance of performance and integrity**

**Reference Links**:
- [SQL Server on Azure VMs - Storage Performance Best Practices](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices-storage?view=azuresql#data-file-caching-policies)
- [Azure Managed Disks - Host Caching](https://learn.microsoft.com/en-us/azure/virtual-machines/premium-storage-performance#disk-caching)
- [SQL Server on Azure VMs - Performance Guidelines](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices-checklist?view=azuresql)

**Domain**: Design data storage solutions

---

### Question 6: Purchase Model for License Mobility Customer (Contoso, Ltd.)

**Scenario**: You manage a database environment for a Microsoft Volume Licensing customer named Contoso, Ltd. Contoso uses License Mobility through Software Assurance.

You need to deploy 50 databases. The solution must meet the following requirements:
- Support automatic scaling
- Minimize Microsoft SQL Server licensing costs

**Question**: Which type of purchase model should you include in the recommendation?

**Options**:
- A) DTU
- B) vCore
- C) Azure reserved virtual machine instances

**Correct Answer**: **B) vCore**

**Explanation**:

**vCore** is correct because the vCore (virtual core) purchase model in Azure SQL Database offers greater flexibility in terms of performance scaling and licensing options, including the ability to use **License Mobility through Software Assurance**. Since Contoso is a Microsoft Volume Licensing customer and participates in Software Assurance, they are eligible for the **Azure Hybrid Benefit**, allowing them to bring their own SQL Server licenses to reduce overall SQL licensing costs.

Additionally, the vCore model supports **automatic scaling** (particularly with the serverless tier) and offers **transparent control** over compute, memory, and I/O ‚Äî making it ideal for deploying and managing a large number of databases efficiently.

**Key Benefits of vCore for This Scenario**:
- ‚úÖ Supports Azure Hybrid Benefit (BYOL)
- ‚úÖ License Mobility through Software Assurance compatible
- ‚úÖ Automatic scaling via serverless compute tier
- ‚úÖ Transparent resource control (CPU, memory, I/O)
- ‚úÖ Up to 55% savings with Azure Hybrid Benefit
- ‚úÖ Up to 80% savings with Hybrid Benefit + Reserved Capacity

**Why Other Options Are Incorrect**:

**DTU** is incorrect because the DTU (Database Transaction Unit) model is a bundled compute model with less flexibility. It **does not support Azure Hybrid Benefit or License Mobility through Software Assurance**, and it lacks detailed visibility and control over resource consumption. This leads to higher licensing costs for customers eligible for bring-your-own-license models. For a customer like Contoso with existing SQL Server licenses, the DTU model would result in paying for SQL licensing twice ‚Äî once for their existing licenses and again through the bundled DTU pricing.

**Azure reserved virtual machine instances** is incorrect because while they help reduce costs for SQL Server on Azure VMs, they apply to **IaaS-based deployments, not PaaS-based Azure SQL Databases**. Also, managing 50 databases on VMs would significantly increase operational overhead (patching, backups, high availability configuration, etc.) and not support automatic scaling as efficiently as the PaaS model under vCore. The requirement to minimize SQL Server licensing costs and support automatic scaling points clearly to a PaaS solution with vCore.

**Purchase Model Comparison**:

| Feature | vCore | DTU | Azure VMs |
|---------|-------|-----|----------|
| Azure Hybrid Benefit | ‚úÖ Yes | ‚ùå No | ‚úÖ Yes |
| License Mobility | ‚úÖ Yes | ‚ùå No | ‚úÖ Yes |
| Automatic Scaling | ‚úÖ Serverless | ‚ùå Manual | ‚ùå Manual |
| Resource Transparency | ‚úÖ Full | ‚ùå Bundled | ‚úÖ Full |
| Operational Overhead | Low (PaaS) | Low (PaaS) | High (IaaS) |
| Best for BYOL | ‚úÖ Ideal | ‚ùå No | ‚ö†Ô∏è Complex |

**Reference Links**:
- [vCore-based Purchasing Model](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)
- [Azure SQL Database Purchasing Models](https://learn.microsoft.com/en-us/azure/azure-sql/database/purchasing-models?view=azuresql)
- [Azure SQL Database Reserved Capacity](https://learn.microsoft.com/en-us/azure/azure-sql/database/reserved-capacity-overview)

**Domain**: Design data storage solutions

---

### Question 7: Data Warehouse Selection for Reporting Solution

**Scenario**: You are designing a data storage solution to support reporting. The solution will ingest high volumes of data in JSON format by using Azure Event Hubs. As the data arrives, Event Hubs will write the data to storage.

The solution must meet the following requirements:
- Organize data in directories by date and time
- Allow stored data to be queried directly, transformed into summarized tables, and then stored in a data warehouse
- Ensure that the data warehouse can store **50 TB of relational data** and support **200-300 concurrent read operations**

**Question**: Which service should you recommend for the data store for the data warehouse?

**Options**:
- A) Azure Cosmos DB Cassandra API
- B) Azure Cosmos DB SQL API
- C) Azure SQL Database Hyperscale
- D) Azure Synapse Analytics dedicated SQL pools

**Correct Answer**: **C) Azure SQL Database Hyperscale**

**Explanation**:

**Azure SQL Database Hyperscale** is correct because it is specifically designed to handle large relational datasets, supporting up to **100 TB of storage**, and offers independent compute and storage scaling, which provides flexibility and cost efficiency. The Hyperscale tier delivers high performance and fast backups, making it suitable for enterprise-grade data workloads. The scenario states that the data warehouse must support 50 TB of relational data and between 200‚Äì300 concurrent read operations, which falls well within the performance and scalability limits of Hyperscale.

**Why Other Options Are Incorrect**:

**Azure Cosmos DB Cassandra API** is incorrect because it is intended for NoSQL column-family workloads and not designed for storing or querying large volumes of relational, structured data. It also does not support analytical queries efficiently and is not cost-effective for 50 TB data storage.

**Azure Cosmos DB SQL API** is incorrect because although it supports JSON-based document storage with flexible schema, it is not a relational database and is unsuitable for warehouse-scale storage or complex SQL-based analytics at the scale described.

**Azure Synapse Analytics dedicated SQL pools** is incorrect in this context because while it is designed for large-scale analytical processing, it is more suitable when working with pre-aggregated or summarized data from a data lake rather than being used as a primary data warehouse for high-volume relational storage with frequent concurrent read access. In this scenario, where relational consistency, transactional support, and high concurrent access are required, Hyperscale is more appropriate.

**Reference Links**:
- [Azure SQL Database Hyperscale](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)
- [Azure SQL Managed Instance Resource Limits](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/resource-limits?view=azuresql)
- [Azure Cosmos DB Introduction](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)
- [Azure Synapse Analytics Compute Management](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-compute-overview)

**Domain**: Design data storage solutions

---

### Question 8: Deployment Option for License Mobility Customer (Contoso, Ltd.)

**Scenario**: You manage a database environment for a Microsoft Volume Licensing customer named Contoso, Ltd. Contoso uses License Mobility through Software Assurance.

You need to deploy 50 databases. The solution must meet the following requirements:
- Support automatic scaling
- Minimize Microsoft SQL Server licensing costs

**Question**: Which type of deployment option should you include in the recommendation?

**Options**:
- A) An Azure SQL managed instance
- B) An Azure SQL Database elastic pool
- C) A SQL Server Always On availability group

**Correct Answer**: **B) An Azure SQL Database elastic pool**

**Explanation**:

**An Azure SQL Database elastic pool** is correct because it allows you to deploy and manage multiple databases (such as 50 in this case) that share the same compute and storage resources, enabling cost efficiency and automatic scaling based on individual database usage patterns. Elastic pools are designed specifically for scenarios where databases have varying and unpredictable usage, which helps minimize overall licensing and compute costs by pooling resources.

Since Contoso uses License Mobility through Software Assurance, they can take advantage of the **Azure Hybrid Benefit** under the vCore-based model within elastic pools, further reducing SQL Server licensing costs.

**Key Benefits of Elastic Pools for This Scenario**:
- ‚úÖ Shared compute and storage resources across 50 databases
- ‚úÖ Automatic scaling based on individual database usage
- ‚úÖ Azure Hybrid Benefit support (vCore model)
- ‚úÖ License Mobility through Software Assurance compatible
- ‚úÖ Cost-effective for varying and unpredictable workloads
- ‚úÖ Low management overhead (PaaS)

**Why Other Options Are Incorrect**:

**An Azure SQL managed instance** is incorrect because while it supports license mobility and is suitable for migrating on-premises SQL Server instances, it is **not optimized for managing a large number of small or variable-load databases**. Deploying 50 separate databases on managed instances would be more complex and expensive compared to using an elastic pool. Managed Instance excels at lift-and-shift migrations requiring instance-scoped features (SQL Agent, Service Broker, cross-database queries), not at hosting many independent databases with varying usage patterns.

**A SQL Server Always On availability group** is incorrect because it applies to **IaaS-based SQL Server deployments** running on Azure VMs, which involve higher licensing costs, manual scaling, and increased management overhead. It does not natively support automatic scaling or cost optimization across multiple databases like elastic pools do. Always On availability groups are designed for high availability and disaster recovery, not for managing multiple databases cost-effectively.

**Deployment Option Comparison**:

| Feature | Elastic Pool | Managed Instance | Always On (VMs) |
|---------|-------------|------------------|------------------|
| License Mobility | ‚úÖ Yes (vCore) | ‚úÖ Yes | ‚úÖ Yes |
| Azure Hybrid Benefit | ‚úÖ Yes (vCore) | ‚úÖ Yes | ‚úÖ Yes |
| Automatic Scaling | ‚úÖ Pool-level | ‚ùå Manual | ‚ùå Manual |
| Resource Sharing | ‚úÖ Yes | ‚ùå No | ‚ùå No |
| Best for 50+ DBs | ‚úÖ Ideal | ‚ùå Complex/Expensive | ‚ùå Highest cost |
| Management Overhead | Low (PaaS) | Low (PaaS) | High (IaaS) |
| Use Case | Variable workloads | Lift-and-shift | HA/DR for SQL VMs |

**Reference Links**:
- [Azure SQL Database Elastic Pool Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview)
- [SQL Managed Instance Overview](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview?view=azuresql)
- [SQL Server Always On Availability Groups on Azure VMs](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/availability-group-overview?view=azuresql)

**Domain**: Design data storage solutions

---

### Question 9: SQL Server Migration - Replication Mechanism Selection

**Scenario**: You have an on-premises Microsoft SQL Server database named SQL1.

You plan to migrate SQL1 to Azure.

You need to recommend a hosting solution for SQL1. The solution must meet the following requirements:
- Support the deployment of **multiple secondary, read-only replicas**
- Support **automatic replication** between primary and secondary replicas
- Support failover between primary and secondary replicas within a **15-minute recovery time objective (RTO)**

**Question**: Which replication mechanism should you include in the recommendation?

**Options**:
- A) Active geo-replication
- B) Auto-failover groups
- C) Standard geo-replication

**Correct Answer**: **A) Active geo-replication**

**Explanation**:

**Active Geo-replication** is correct because it meets all three requirements specified in the scenario:

1. **Multiple secondary replicas**: Active geo-replication supports up to 4 readable secondary replicas in different Azure regions
2. **Automatic replication**: It provides automatic asynchronous replication between primary and secondary replicas
3. **RTO within 15 minutes**: The recovery time objective (RTO) for active geo-replication is less than 30 seconds, which is well within the 15-minute requirement

Additionally, all secondary replicas are readable, making them available for read scale-out scenarios.

**Why Other Options Are Incorrect**:

**Auto-failover groups** is incorrect because auto-failover groups do **not support multiple replicas**. Auto-failover groups only support a single secondary replica. While they provide automatic failover with DNS endpoint management, the requirement for multiple secondary read-only replicas cannot be met. Additionally, the RTO for auto-failover groups is typically around 1 hour, which exceeds the 15-minute requirement.

**Standard geo-replication** is incorrect because:
- It supports only **one secondary replica** (not multiple)
- The secondary replica is kept **offline and not readable**
- It has a higher RTO and RPO compared to active geo-replication
- It is essentially a legacy option that should be avoided for new deployments

**Replication Mechanism Comparison**:

| Feature | Active Geo-Replication | Auto-Failover Groups | Standard Geo-Replication |
|---------|----------------------|---------------------|-------------------------|
| **Multiple Secondaries** | ‚úÖ Up to 4 | ‚ùå Only 1 | ‚ùå Only 1 |
| **Readable Secondaries** | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No (offline) |
| **Automatic Replication** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **RTO** | < 30 seconds | ~1 hour | Higher |
| **Meets 15-min RTO** | ‚úÖ Yes | ‚ùå No | ‚ùå No |

**Key Takeaways**:
- ‚úÖ Choose **Active Geo-Replication** when you need multiple readable secondaries and low RTO
- ‚ö†Ô∏è Choose **Auto-Failover Groups** when you need automatic DNS failover but only need one secondary
- ‚ùå Avoid **Standard Geo-Replication** for new deployments

**Reference Links**:
- [Active Geo-Replication Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql&tabs=tsql)
- [Active Geo-Replication Configuration](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-configure-portal?view=azuresql&tabs=portal)
- [Auto-Failover Groups Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/auto-failover-group-sql-db?view=azuresql)

**Domain**: Design data storage solutions

---

### Question 10: Protecting Sensitive Employee Data in Azure SQL Managed Instance

**Scenario**: You have an Azure subscription. The subscription contains an Azure SQL managed instance that stores employee details, including social security numbers and phone numbers.

You need to configure the managed instance to meet the following requirements:
- The helpdesk team must see only the **last four digits** of an employee's phone number
- Cloud administrators must be **prevented from seeing** the employees' social security numbers

**Question**: What should you enable for the Social security numbers column in the managed instance?

**Options**:
- A) Always Encrypted
- B) Column encryption
- C) Dynamic data masking
- D) Transparent Data Encryption (TDE)

**Correct Answer**: **A) Always Encrypted**

**Explanation**:

**Always Encrypted** is correct because:
- Ensures that sensitive data like social security numbers **remain encrypted at rest, in transit, AND during query processing**
- Even administrators with access to the underlying database **cannot view the plain text** social security numbers
- Encryption and decryption are handled **transparently by the client application** using keys managed by the application
- If a cloud administrator gains access to the database, they **won't be able to see** the actual social security numbers without the appropriate keys
- Keys should be securely managed using a service like **Azure Key Vault**

**Why Other Options Are Incorrect**:

**Column encryption** is incorrect because:
- While it can encrypt specific columns, the encryption process involves **key management at the database level**
- Does NOT provide the same level of protection against privileged database users
- Always Encrypted is preferred because the **application manages the keys**, preventing cloud administrators from accessing unencrypted data
- Column Encryption may lack the necessary **granularity for access control** when preventing specific users from seeing data

**Dynamic data masking** is incorrect because:
- Designed to **mask data in query results**, not encrypt it
- Does NOT provide encryption at rest or during query processing
- Privileged users (including administrators) **can still see the unmasked data**
- Main purpose is to **limit exposure** to non-privileged users, not prevent administrator access
- **Important**: DDM is the correct answer for the **phone number requirement** (showing last 4 digits), but NOT for preventing admin access to SSN

**Transparent Data Encryption (TDE)** is incorrect because:
- Operates at the **page level**, encrypting the **entire database**, not specific columns
- Does NOT allow selective encryption of specific columns like social security numbers
- While it protects data at rest from physical theft, it **does NOT prevent** cloud administrators from viewing data when querying
- Data is automatically decrypted when queried by any user with database access

**Complete Solution for Both Requirements**:

| Requirement | Solution | Why |
|-------------|----------|-----|
| **SSN - Prevent admin access** | **Always Encrypted** | Client-side encryption; keys not accessible to DBAs |
| **Phone - Show last 4 digits** | **Dynamic Data Masking** | Masks data at query level; can use partial masking |

**Implementation Example**:

```sql
-- For Social Security Numbers: Configure Always Encrypted
-- This requires setting up column master key in Azure Key Vault
-- and column encryption key in the database

CREATE COLUMN MASTER KEY [CMK_Auto1]
WITH (
    KEY_STORE_PROVIDER_NAME = N'AZURE_KEY_VAULT',
    KEY_PATH = N'https://mykeyvault.vault.azure.net/keys/CMK/abc123'
);

CREATE COLUMN ENCRYPTION KEY [CEK_Auto1]
WITH VALUES (
    COLUMN_MASTER_KEY = [CMK_Auto1],
    ALGORITHM = 'RSA_OAEP',
    ENCRYPTED_VALUE = 0x...
);

-- Alter table to encrypt SSN column
ALTER TABLE Employees
ALTER COLUMN SocialSecurityNumber NVARCHAR(11) 
    ENCRYPTED WITH (
        ENCRYPTION_TYPE = DETERMINISTIC,
        ALGORITHM = 'AEAD_AES_256_CBC_HMAC_SHA_256',
        COLUMN_ENCRYPTION_KEY = [CEK_Auto1]
    );

-- For Phone Numbers: Configure Dynamic Data Masking (partial)
ALTER TABLE Employees
ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'partial(0,"XXX-XXX-",4)');
-- Result: XXX-XXX-1234 (only last 4 digits visible)
```

**Key Comparison - Data Protection Methods**:

| Feature | Always Encrypted | Column Encryption | Dynamic Data Masking | TDE |
|---------|-----------------|-------------------|---------------------|-----|
| **Encryption Level** | Column | Column | None (masking only) | Database |
| **Protects from DBAs** | ‚úÖ Yes | ‚ùå No | ‚ùå No | ‚ùå No |
| **Key Location** | Client/Key Vault | Database | N/A | Server-side |
| **Data at Rest** | ‚úÖ Encrypted | ‚úÖ Encrypted | ‚ùå Unencrypted | ‚úÖ Encrypted |
| **During Query** | ‚úÖ Encrypted | ‚ùå Decrypted | Masked (not encrypted) | ‚ùå Decrypted |
| **Partial Visibility** | ‚ùå No | ‚ùå No | ‚úÖ Yes | ‚ùå No |
| **Best For** | Preventing admin access | General encryption | Limiting non-privileged access | At-rest protection |

> **Exam Tip:** When a scenario mentions **preventing cloud administrators or DBAs from seeing sensitive data**, **Always Encrypted** is the correct answer because the encryption keys are managed by the application, not stored in the database. TDE and Column Encryption don't prevent privileged users from seeing data. Dynamic Data Masking only masks data for non-privileged users.

**Reference Links**:
- [Always Encrypted](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine)
- [Encrypt a Column of Data](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/encrypt-a-column-of-data)
- [Transparent Data Encryption](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/transparent-data-encryption)
- [Dynamic Data Masking](https://learn.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking)
- [Configure Always Encrypted](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine#configure-always-encrypted)

**Domain**: Design data storage solutions

---

## Key Insights for Exams

### Critical Points

1. **Serverless = Auto-scaling + Per-second billing**
   > Only Single Database supports true per-second billing with auto-scaling via Serverless compute tier

2. **Elastic Pool ‚â† Auto-scaling per database**
   > Elastic pools scale at pool level, not individual database level. Resources are shared.

2a. **Elastic Pool + Reserved Capacity = Cost Optimization**
   > Elastic pools support reserved capacity through vCore model, providing up to 80% savings for predictable workloads. Ideal for multiple databases with varying usage patterns.

2b. **Serverless ‚â† Reserved Capacity**
   > Serverless compute tier does NOT support reserved capacity. It's designed for intermittent workloads with auto-pause and per-second billing.

2c. **SQL Server VM Disk Caching**
   > Data disk = ReadOnly caching (optimizes reads) | Transaction log disk = None caching (optimizes writes) | Never use ReadWrite for SQL disks (data corruption risk)

3. **Managed Instance = Near 100% SQL Server compatibility**
   > Choose for lift-and-shift migrations requiring instance-scoped features (SQL Agent, Service Broker)

4. **Business Critical = Local SSD + Read Replica**
   > Business Critical uses local SSD for low latency and includes one free read replica

5. **Hyperscale = Large databases + Fast scaling**
   > Use Hyperscale for databases >4 TB or when rapid scaling is required

6. **Azure Hybrid Benefit = Up to 71% savings**
   > Bring existing SQL Server licenses for significant cost reduction

7. **Zone Redundancy = Higher availability**
   > Zone redundant increases SLA to 99.995% (vs 99.99%)

8. **Single Database vs Elastic Pool**
   > Single DB = dedicated resources | Elastic Pool = shared resources across multiple DBs

9. **Managed Instance for High Resiliency Requirements**
   > When requirements include availability across multiple availability zone failures, minimal I/O latency, and minimal administrative effort, Azure SQL Managed Instance is the correct choice. It supports zone-redundant configuration, automatic failover, geo-replication, and uses local SSD storage (Business Critical tier) for optimal performance while being fully managed.

10. **Auditing Storage Account = Same Region as SQL Server**
   > Azure SQL Database auditing requires the storage account to be in the same region as the SQL server. Cross-region storage is not supported for audit logs. Auditing is supported across all service tiers (Basic, Standard, Premium).

11. **Zero Data Loss Failover = Premium (DTU) or Business Critical (vCore)**
   > Only Premium (DTU) and Business Critical (vCore) tiers provide zero data loss (RPO = 0) during failover. They use Always On availability groups with synchronous replication. For cost optimization with zero data loss and zone redundancy requirements, Premium (DTU) is the most cost-effective option.

12. **Zone Redundancy for High Availability**
   > Basic and Standard (DTU) tiers do NOT support zone redundancy. For zone outage protection, use Premium (DTU), General Purpose (vCore), Business Critical (vCore), or Hyperscale. However, only Premium and Business Critical guarantee zero data loss during failover.

13. **Hyperscale for Large-Scale Relational Data Warehouses**
   > When you need a data warehouse that stores 50+ TB of relational data with 200-300+ concurrent read operations, Azure SQL Database Hyperscale is ideal. It supports up to 100 TB, offers independent compute/storage scaling, and is better suited than Synapse dedicated SQL pools when relational consistency and high concurrent access are required.

14. **vCore Model for License Mobility and Azure Hybrid Benefit**
   > For Microsoft Volume Licensing customers with License Mobility through Software Assurance, the vCore purchasing model is required to leverage Azure Hybrid Benefit. DTU model does NOT support Azure Hybrid Benefit or License Mobility. vCore also supports automatic scaling via serverless tier and provides transparent resource control. Savings can reach up to 80% when combining Azure Hybrid Benefit with reserved capacity.

15. **Elastic Pool for Multiple Databases with License Mobility**
   > When deploying 50+ databases for a License Mobility customer requiring automatic scaling and cost minimization, Elastic Pool is the correct deployment option. It shares resources across databases with varying usage patterns, supports Azure Hybrid Benefit (vCore), and provides automatic scaling at pool level. Managed Instance is overkill for simple database hosting, and SQL Server Always On (VMs) has higher costs and manual scaling.

16. **Long-Term Retention (LTR) for Compliance Backup Requirements**
   > When backup retention must exceed 35 days (e.g., 7 years for compliance), configure a **Long-Term Retention (LTR) policy**. Azure SQL Database automatic backups have a default retention of 7-35 days, which is insufficient for long-term compliance. LTR stores full database backups in Azure Blob storage for up to 10 years. **Do NOT confuse with**: Azure Site Recovery (for disaster recovery, not backup retention), Geo-replication (for high availability, not backup retention), or standard automatic backups (max 35 days).

17. **Auto-Failover Groups for Cross-Region Business Continuity with Minimal Admin Effort**
   > When requirements include: (1) user-initiated backups, (2) multiple automatically replicated instances across Azure regions, and (3) minimal administrative effort for business continuity, choose **Auto-Failover Groups**. 
   > - **Active Geo-Replication** requires **manual failover**, increasing administrative overhead
   > - **Zone-Redundant Deployment** only provides HA within a **single region** (no cross-region support)
   > - Auto-Failover Groups provide **automatic failover** with **DNS endpoint management**, eliminating manual intervention

18. **Always Encrypted = Prevent DBA/Admin Access to Sensitive Data**
   > When you need to **prevent cloud administrators from seeing sensitive data** (SSN, credit cards, etc.), use **Always Encrypted**. Keys are managed client-side (Azure Key Vault), so DBAs cannot decrypt data. TDE and Column Encryption do NOT prevent privileged user access.

19. **Dynamic Data Masking = Partial Data Visibility for Non-Privileged Users**
   > Use **Dynamic Data Masking** when you need to **show partial data** (e.g., last 4 digits of phone/SSN) to non-privileged users. Note: DDM does NOT encrypt data and does NOT prevent privileged users (admins) from seeing full data.

20. **TDE vs Always Encrypted vs DDM**
   > - **TDE**: Encrypts **entire database** at rest; does NOT prevent query-time access
   > - **Always Encrypted**: Encrypts **specific columns**; prevents even DBAs from seeing data
   > - **DDM**: **Masks** data in query results; no encryption; admins can still see full data

21. **Column Encryption vs Always Encrypted**
   > Both encrypt columns, but **Always Encrypted** stores keys **outside the database** (client-side), while Column Encryption stores keys in the database. For preventing admin access, always choose **Always Encrypted**.

---

### Exam Scenario: Cross-Region Resiliency for SQL Server Migration

**Scenario**: You plan to migrate on-premises Microsoft SQL Server databases to Azure. You need to determine a resiliency solution that meets the following requirements:
- Supports user-initiated backups
- Supports multiple automatically replicated instances across Azure regions
- Minimizes administrative effort to implement and maintain business continuity

**Question**: What should you recommend as the solution?

**Options**:
- A. Auto-failover group
- B. Active geo-replication
- C. Zone-redundant deployment

**Answer**: **A. Auto-failover group**

**Why Correct Answer**:
Auto-failover group is correct because it:
- ‚úÖ Supports **user-initiated backups**
- ‚úÖ Provides **automatic replication across Azure regions**
- ‚úÖ **Minimizes administrative effort** with automatic failover capability
- ‚úÖ Integrates with both Azure SQL Database and Azure SQL Managed Instance
- ‚úÖ DNS endpoint management eliminates need for manual connection string updates
- ‚úÖ Reduces manual intervention, ensuring high availability and business continuity

**Why Other Options Are Incorrect**:

| Option | Why Incorrect |
|--------|---------------|
| **B. Active geo-replication** | While it supports multiple automatically replicated instances across Azure regions, it **requires manual failover**. This increases administrative overhead, making it less suitable for minimizing effort. |
| **C. Zone-redundant deployment** | Provides high availability **within a single Azure region** by distributing replicas across availability zones. It does **NOT support replication across multiple regions**, failing the cross-region requirement. |

**Key Comparison**:

| Requirement | Auto-Failover Groups | Active Geo-Replication | Zone-Redundant |
|-------------|---------------------|----------------------|----------------|
| User-initiated backups | ‚úÖ | ‚úÖ | ‚úÖ |
| Cross-region replication | ‚úÖ | ‚úÖ | ‚ùå |
| Minimal admin effort | ‚úÖ (automatic failover) | ‚ùå (manual failover) | ‚úÖ |

**Reference Links**:
- [Auto-Failover Group Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/auto-failover-group-overview)
- [Failover Groups for Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/failover-group-sql-mi?view=azuresql&tabs=azure-powershell)
- [Active Geo-Replication Overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview)
- [High Availability SLA](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla)

**Domain**: Design Data Storage Solutions

---

### Exam Scenario: Database Backup Retention Requirements

**Scenario**: A company requires database backups to be retained for a minimum of seven years to meet compliance requirements.

**Question**: What should you recommend to meet the database backup retention requirements?

**Options**:
- A. Use automatic Azure SQL Database backups
- B. Configure Azure Site Recovery
- C. Configure geo-replication of the database
- D. Configure a long-term retention policy for the database

**Answer**: **D. Configure a long-term retention policy for the database**

**Why Correct Answer**:
Azure SQL Database offers **Long-Term Retention (LTR)** for automated backups. This allows you to keep the backups in Azure Blob storage for up to **10 years**, which aligns with the requirement to retain database backups for a minimum of seven years.

**Why Other Options Are Incorrect**:

| Option | Why Incorrect |
|--------|---------------|
| **A. Automatic Azure SQL Database backups** | Default retention period is **7-35 days**, not seven years. You need LTR policy for extended retention. |
| **B. Azure Site Recovery** | Designed for **disaster recovery** (replication, failover, recovery of workloads), NOT for long-term backup retention. |
| **C. Geo-replication** | Used for **high availability and disaster recovery** by creating readable secondary replicas in different regions. Does NOT address backup retention requirements. |

**Key Comparison**:

| Feature | Purpose | Retention Period |
|---------|---------|------------------|
| **Automatic Backups** | Point-in-time restore | 7-35 days |
| **Long-Term Retention (LTR)** | Compliance, archival | Up to 10 years |
| **Geo-replication** | High availability, DR | N/A (real-time sync) |
| **Azure Site Recovery** | Disaster recovery | N/A (workload recovery) |

**Reference Links**:
- [Azure SQL Database Long-Term Retention](https://learn.microsoft.com/en-us/azure/azure-sql/database/long-term-retention-overview)
- [Directory Synchronization Overview](https://learn.microsoft.com/en-us/entra/architecture/sync-directory)

**Domain**: Design Data Storage Solutions

---

## Quick Reference Cheat Sheet

### When Requirements Say...

| Requirement | Answer |
|-------------|---------|
| "Auto-scales based on workload" | **Serverless** compute tier |
| "Per-second billing" | **Serverless** compute tier |
| "Multiple databases, cost-effective" | **Elastic Pool** |
| "Multiple databases with varying usage patterns" | **Elastic Pool** |
| "Reserved capacity + multiple databases" | **Elastic Pool (vCore)** |
| "Dynamic scaling + multiple databases" | **Elastic Pool** |
| "SQL Agent, Service Broker" | **Managed Instance** or **SQL VM** |
| "Lift-and-shift from SQL Server" | **Managed Instance** |
| "SQL Server VM data disk caching" | **ReadOnly caching** |
| "SQL Server VM transaction log caching" | **None caching** |
| "Best performance + data integrity for SQL VM" | **Data: ReadOnly, Log: None** |
| "Full OS control" | **SQL Server on Azure VMs** |
| "Mission-critical, low latency" | **Business Critical** tier |
| "Database > 4 TB" | **Hyperscale** tier |
| "Development/test environment" | **Serverless General Purpose** |
| "Modern cloud application" | **Single Database** |
| "Availability if two zones fail" | **Managed Instance** with zone redundancy |
| "Minimal I/O latency + minimal admin" | **Managed Instance Business Critical** |
| "High resiliency + enterprise features" | **Managed Instance** with geo-replication |
| "Store audit logs to storage account" | Storage account must be in **same region** as SQL server |
| "Zero data loss + zone redundancy + minimize cost" | **Premium (DTU)** tier |
| "No data loss during failover" | **Premium (DTU)** or **Business Critical (vCore)** |
| "Zone outage protection" | **Premium**, **General Purpose**, **Business Critical**, or **Hyperscale** (NOT Basic/Standard) |
| "RPO = 0 (zero data loss)" | **Premium (DTU)** or **Business Critical (vCore)** only |
| "50+ TB relational data warehouse" | **Hyperscale** tier |
| "200-300 concurrent read operations" | **Hyperscale** tier |
| "Data warehouse + relational consistency" | **Hyperscale** (not Synapse dedicated pools) |
| "License Mobility through Software Assurance" | **vCore** model (supports Azure Hybrid Benefit) |
| "Minimize SQL licensing costs + BYOL" | **vCore** model with Azure Hybrid Benefit |
| "Volume Licensing customer" | **vCore** model (DTU doesn't support Hybrid Benefit) |
| "50+ databases + License Mobility + auto-scaling" | **Elastic Pool** (vCore) |
| "Multiple databases + minimize licensing costs" | **Elastic Pool** with Azure Hybrid Benefit |
| "Variable workload databases + cost optimization" | **Elastic Pool** (not Managed Instance or VMs) |
| "Backup retention > 35 days" | **Long-Term Retention (LTR)** policy |
| "Backup retention for compliance (7+ years)" | **Long-Term Retention (LTR)** policy |
| "Archive database backups for years" | **Long-Term Retention (LTR)** policy (up to 10 years) |
| "Cross-region replication + auto failover" | **Auto-Failover Groups** |
| "Cross-region replication + minimal admin effort" | **Auto-Failover Groups** |
| "Multiple replicas across regions + manual control" | **Active Geo-Replication** |
| "HA within single region only" | **Zone-Redundant Deployment** |
| "Automatic failover across regions" | **Auto-Failover Groups** (not Active Geo-Replication) |
| "Business continuity + minimize admin overhead" | **Auto-Failover Groups** |
| "Prevent DBAs from seeing sensitive columns" | **Always Encrypted** |
| "Protect SSN/credit cards from cloud admins" | **Always Encrypted** |
| "Show only last 4 digits to helpdesk" | **Dynamic Data Masking** (partial function) |
| "Mask PII for non-privileged users" | **Dynamic Data Masking** |
| "Encrypt specific columns, app manages keys" | **Always Encrypted** |
| "Encrypt entire database at rest" | **Transparent Data Encryption (TDE)** |
| "Encrypt columns but DBAs can still read" | **Column Encryption** (not Always Encrypted) |
| "Client-side encryption with Key Vault" | **Always Encrypted** |
| "Maximum protection for sensitive columns" | **Always Encrypted** |

## References

### Microsoft Documentation

- [Azure SQL Database Overview](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview)
- [Serverless Compute Tier](https://learn.microsoft.com/azure/azure-sql/database/serverless-tier-overview)
- [Azure SQL Managed Instance](https://learn.microsoft.com/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview)
- [SQL Server on Azure VMs](https://learn.microsoft.com/azure/azure-sql/virtual-machines/windows/sql-server-on-azure-vm-iaas-what-is-overview)
- [vCore Model](https://learn.microsoft.com/azure/azure-sql/database/service-tiers-vcore)
- [DTU Model](https://learn.microsoft.com/azure/azure-sql/database/service-tiers-dtu)
- [Elastic Pools](https://learn.microsoft.com/azure/azure-sql/database/elastic-pool-overview)
- [Service Tiers](https://learn.microsoft.com/azure/azure-sql/database/service-tiers-general-purpose-business-critical)
- [High Availability and Zone Redundancy](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla-local-zone-redundancy?view=azuresql&tabs=azure-powershell)
- [Service Tiers - DTU Model](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)
- [Hyperscale Service Tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)
- [Azure Hybrid Benefit](https://learn.microsoft.com/azure/azure-sql/azure-hybrid-benefit)
- [Azure SQL Database Auditing](https://learn.microsoft.com/azure/azure-sql/database/auditing-overview)
- [Long-Term Retention (LTR)](https://learn.microsoft.com/en-us/azure/azure-sql/database/long-term-retention-overview)

### Pricing Calculators

- [Azure Pricing Calculator](https://azure.microsoft.com/pricing/calculator/)
- [Azure SQL Database Pricing](https://azure.microsoft.com/pricing/details/azure-sql-database/)
- [Azure SQL Managed Instance Pricing](https://azure.microsoft.com/pricing/details/azure-sql-managed-instance/)

---

**Last Updated**: December 2025  
**Document Version**: 1.0
