# 7.1 Reliability Architecture

Reliability architecture focuses on ensuring systems remain operational, can recover from failures gracefully, and meet defined service level objectives. This document covers the key concepts, patterns, and practices for building reliable distributed systems.

---

## Table of Contents

- [Overview](#overview)
- [High Availability (HA)](#high-availability-ha)
- [Disaster Recovery (DR)](#disaster-recovery-dr)
- [Fault-Tolerant Architecture](#fault-tolerant-architecture)
- [Chaos Engineering Architecture](#chaos-engineering-architecture)
- [Resilience Patterns](#resilience-patterns)
- [Key Metrics and SLOs](#key-metrics-and-slos)
- [Best Practices](#best-practices)
- [Related Topics](#related-topics)

---

## Overview

Reliability is the ability of a system to perform its intended function correctly and consistently over time, even in the face of failures. A reliable system:

- **Continues to work correctly** even when things go wrong (hardware/software faults, human error)
- **Tolerates faults** in hardware, software, and operations
- **Meets user expectations** for functionality and performance
- **Prevents unauthorized access** and abuse

### The Reliability Equation

```
Reliability = Availability × Performance × Correctness
```

### Key Reliability Concepts

| Concept | Definition |
|---------|------------|
| **Availability** | Percentage of time a system is operational and accessible |
| **Durability** | Data is not lost, even across failures |
| **Consistency** | System behaves correctly according to specification |
| **Resilience** | Ability to recover from failures and adapt to change |
| **Fault Tolerance** | Ability to continue operating when components fail |

---

## High Availability (HA)

High Availability ensures systems remain accessible with minimal downtime, typically expressed as a percentage (e.g., 99.99% uptime).

### Availability Tiers

| Availability | Downtime/Year | Downtime/Month | Downtime/Week | Common Name |
|--------------|---------------|----------------|---------------|-------------|
| 99% | 3.65 days | 7.31 hours | 1.68 hours | Two nines |
| 99.9% | 8.77 hours | 43.83 minutes | 10.08 minutes | Three nines |
| 99.95% | 4.38 hours | 21.92 minutes | 5.04 minutes | Three and a half nines |
| 99.99% | 52.60 minutes | 4.38 minutes | 1.01 minutes | Four nines |
| 99.999% | 5.26 minutes | 26.30 seconds | 6.05 seconds | Five nines |

### HA Deployment Patterns

#### Active-Active

Multiple instances handle traffic simultaneously, providing maximum availability and capacity.

```
                    ┌─────────────────┐
                    │  Load Balancer  │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
    ┌─────────────┐   ┌─────────────┐   ┌─────────────┐
    │  Instance 1 │   │  Instance 2 │   │  Instance 3 │
    │   (Active)  │   │   (Active)  │   │   (Active)  │
    └─────────────┘   └─────────────┘   └─────────────┘
```

| Aspect | Details |
|--------|---------|
| **Benefits** | Maximum throughput, no wasted resources, seamless scaling |
| **Challenges** | Data synchronization, session management, conflict resolution |
| **Use Cases** | Global applications, stateless services, read-heavy workloads |

#### Active-Passive (Active-Standby)

A standby instance takes over when the primary fails.

```
    ┌─────────────────────────────────────────────────┐
    │                 Normal Operation                 │
    │  ┌─────────────┐              ┌─────────────┐   │
    │  │  Instance 1 │  ──sync──▶  │  Instance 2 │   │
    │  │   (Active)  │              │  (Standby)  │   │
    │  └─────────────┘              └─────────────┘   │
    └─────────────────────────────────────────────────┘

    ┌─────────────────────────────────────────────────┐
    │                  After Failover                  │
    │  ┌─────────────┐              ┌─────────────┐   │
    │  │  Instance 1 │              │  Instance 2 │   │
    │  │   (Failed)  │              │   (Active)  │   │
    │  └─────────────┘              └─────────────┘   │
    └─────────────────────────────────────────────────┘
```

| Aspect | Details |
|--------|---------|
| **Benefits** | Simpler data consistency, lower cost than active-active |
| **Challenges** | Failover time, wasted standby resources |
| **Use Cases** | Databases, stateful applications, cost-sensitive HA |

#### N+1 Redundancy

One extra instance beyond the minimum required for normal operation.

| Scenario | Instances | Description |
|----------|-----------|-------------|
| **N+1** | 3 (if N=2) | One spare for any single failure |
| **N+2** | 4 (if N=2) | Two spares for maintenance + failure |
| **2N** | 4 (if N=2) | Full duplicate for maximum redundancy |

#### Geographic Redundancy

Instances distributed across multiple geographic regions or availability zones.

```
              ┌─────────────────────────────────────────────────────┐
              │               Global Load Balancer                   │
              └─────────────────────────┬───────────────────────────┘
                                        │
        ┌───────────────────────────────┼───────────────────────────────┐
        ▼                               ▼                               ▼
┌───────────────┐               ┌───────────────┐               ┌───────────────┐
│   Region A    │               │   Region B    │               │   Region C    │
│  ┌─────────┐  │               │  ┌─────────┐  │               │  ┌─────────┐  │
│  │ Zone 1  │  │               │  │ Zone 1  │  │               │  │ Zone 1  │  │
│  └─────────┘  │               │  └─────────┘  │               │  └─────────┘  │
│  ┌─────────┐  │               │  ┌─────────┐  │               │  ┌─────────┐  │
│  │ Zone 2  │  │               │  │ Zone 2  │  │               │  │ Zone 2  │  │
│  └─────────┘  │               └─────────┘  │               │  └─────────┘  │
└───────────────┘               └───────────────┘               └───────────────┘
```

### HA Design Principles

1. **Eliminate single points of failure (SPOF)** – Every critical component should have redundancy
2. **Design for failure** – Assume components will fail; plan for it
3. **Implement automated failover** – Minimize human intervention during failures
4. **Test failover regularly** – Validate failover mechanisms work as expected
5. **Monitor everything** – Detect failures quickly to minimize impact

---

## Disaster Recovery (DR)

Disaster Recovery ensures business continuity after catastrophic failures such as natural disasters, data center outages, or major security incidents.

### Key DR Metrics

| Metric | Definition | Question It Answers |
|--------|------------|---------------------|
| **RTO (Recovery Time Objective)** | Maximum acceptable downtime | How quickly must we recover? |
| **RPO (Recovery Point Objective)** | Maximum acceptable data loss | How much data can we afford to lose? |
| **MTTR (Mean Time to Recovery)** | Average time to restore service | How long does recovery typically take? |
| **MTBF (Mean Time Between Failures)** | Average time between failures | How often do failures occur? |

### DR Strategies Comparison

```
Cost/Complexity                                                    Recovery Speed
     ▲                                                                    ▲
     │                                          ┌──────────────────┐      │
     │                                          │  Hot Standby /   │      │
     │                                          │   Multi-Site     │      │
     │                                    ┌─────┴──────────────────┘      │
     │                                    │                               │
     │                         ┌──────────┴──────────┐                    │
     │                         │   Warm Standby      │                    │
     │               ┌─────────┴──────────────────────┘                   │
     │               │                                                    │
     │    ┌──────────┴──────────┐                                         │
     │    │    Pilot Light      │                                         │
     │    └──────────┬──────────┘                                         │
     │               │                                                    │
     │ ┌─────────────┴───────────────┐                                    │
     │ │     Backup & Restore        │                                    │
     │ └─────────────────────────────┘                                    │
     └────────────────────────────────────────────────────────────────────▶
```

### DR Strategy Details

#### Backup & Restore

| Aspect | Details |
|--------|---------|
| **RTO** | Hours to days |
| **RPO** | Hours (based on backup frequency) |
| **Cost** | $ (Lowest) |
| **Description** | Regularly backup data and configurations; restore when disaster occurs |
| **Best For** | Non-critical systems, development environments |

#### Pilot Light

| Aspect | Details |
|--------|---------|
| **RTO** | Minutes to hours |
| **RPO** | Minutes |
| **Cost** | $$ |
| **Description** | Minimal core infrastructure always running (databases replicated); scale up on disaster |
| **Best For** | Cost-conscious DR for critical systems |

#### Warm Standby

| Aspect | Details |
|--------|---------|
| **RTO** | Minutes |
| **RPO** | Seconds to minutes |
| **Cost** | $$$ |
| **Description** | Scaled-down but fully functional replica; scale up during disaster |
| **Best For** | Business-critical applications |

#### Hot Standby / Multi-Site Active-Active

| Aspect | Details |
|--------|---------|
| **RTO** | Seconds (near-zero) |
| **RPO** | Near-zero |
| **Cost** | $$$$ (Highest) |
| **Description** | Full-scale replica running continuously; instant automatic failover |
| **Best For** | Mission-critical, zero-downtime requirements |

### DR Planning Checklist

- [ ] Define RTO and RPO for each system/service
- [ ] Identify critical systems and dependencies
- [ ] Choose appropriate DR strategy for each tier
- [ ] Document runbooks for failover and failback
- [ ] Implement data replication (sync vs async)
- [ ] Set up automated failover where possible
- [ ] Plan for DNS and networking changes
- [ ] Test DR procedures regularly (quarterly recommended)
- [ ] Train team members on DR procedures
- [ ] Review and update DR plan annually

---

## Fault-Tolerant Architecture

Fault tolerance allows systems to continue operating correctly despite component failures, preventing single failures from causing system-wide outages.

### Fault Tolerance Approaches

| Approach | Description | Example |
|----------|-------------|---------|
| **Redundancy** | Duplicate critical components | Multiple database replicas |
| **Replication** | Copy data across multiple nodes | Distributed file systems |
| **Graceful Degradation** | Reduce functionality rather than fail completely | Show cached data when API unavailable |
| **Fail-Safe Defaults** | Default to safe state when failures occur | Deny access on auth service failure |
| **Isolation** | Contain failures to prevent cascade | Microservices with bulkheads |

### Redundancy Patterns

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        Types of Redundancy                               │
├───────────────────┬─────────────────────┬───────────────────────────────┤
│      Hardware     │       Software      │            Data               │
├───────────────────┼─────────────────────┼───────────────────────────────┤
│ • Multiple servers│ • Multiple instances│ • Database replication        │
│ • RAID storage    │ • Service replicas  │ • Backup systems              │
│ • Redundant power │ • Load balancers    │ • Cross-region data copy      │
│ • Dual network    │ • Container replicas│ • Write-ahead logging         │
└───────────────────┴─────────────────────┴───────────────────────────────┘
```

### Graceful Degradation Strategy

Design systems to provide reduced functionality when components fail:

| Component Failure | Graceful Response |
|-------------------|-------------------|
| Recommendation service down | Show popular items instead |
| Search service degraded | Return cached results |
| Payment service slow | Queue orders for processing |
| Image service unavailable | Show placeholder images |
| Analytics service down | Continue without tracking |

### Failure Isolation

```
┌────────────────────────────────────────────────────────────────┐
│                     Without Isolation                           │
│                                                                 │
│  Service A ─▶ Service B ─▶ Service C ─▶ Service D              │
│      ✓           ✗ (fails)    ✗ (cascade)  ✗ (cascade)         │
└────────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────────┐
│                      With Isolation                             │
│                                                                 │
│  Service A ─▶ [Circuit Breaker] ─▶ Service B (failed)          │
│      ✓              │                   ✗                       │
│                     └─▶ Fallback ─▶ Continue                   │
│                              ✓                                  │
│                                                                 │
│  Service C ✓    Service D ✓   (unaffected)                     │
└────────────────────────────────────────────────────────────────┘
```

---

## Chaos Engineering Architecture

Chaos Engineering is the discipline of experimenting on distributed systems to build confidence in the system's capability to withstand turbulent conditions in production.

### Chaos Engineering Principles

| Principle | Description |
|-----------|-------------|
| **Build a Hypothesis** | Define expected system behavior before experiments |
| **Vary Real-World Events** | Simulate realistic failures that could happen |
| **Run Experiments in Production** | Real environment reveals real issues |
| **Automate Experiments** | Continuous resilience validation |
| **Minimize Blast Radius** | Start small, gradually increase scope |

### Chaos Engineering Process

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Chaos Engineering Cycle                               │
│                                                                          │
│    ┌──────────────┐     ┌──────────────┐     ┌──────────────┐          │
│    │   1. Define  │────▶│  2. Create   │────▶│  3. Execute  │          │
│    │  Steady State│     │  Hypothesis  │     │  Experiment  │          │
│    └──────────────┘     └──────────────┘     └──────┬───────┘          │
│           ▲                                          │                   │
│           │                                          ▼                   │
│    ┌──────┴───────┐     ┌──────────────┐     ┌──────────────┐          │
│    │  5. Improve  │◀────│  4. Analyze  │◀────│   Observe    │          │
│    │    System    │     │   Results    │     │   Behavior   │          │
│    └──────────────┘     └──────────────┘     └──────────────┘          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Common Chaos Experiments

| Category | Experiments |
|----------|-------------|
| **Infrastructure** | Instance termination, zone failure, region failover |
| **Network** | Latency injection, packet loss, DNS failure, partition |
| **Application** | Service failure, dependency timeout, memory leak |
| **Resource** | CPU stress, memory exhaustion, disk full |
| **Time** | Clock skew, time jumps |

### Chaos Experiment Example: Service Failure

```yaml
# Example Chaos Experiment Definition
experiment:
  name: "Payment Service Failure"
  hypothesis: "When payment service fails, orders are queued and users see appropriate message"
  
  steady_state:
    - metric: "order_success_rate"
      value: ">= 99%"
    - metric: "user_error_rate"  
      value: "< 1%"
  
  method:
    - action: "terminate_service"
      target: "payment-service"
      duration: "5m"
  
  rollback:
    - action: "restart_service"
      target: "payment-service"
```

### Chaos Engineering Tools

| Tool | Description | Platform |
|------|-------------|----------|
| **Chaos Monkey** | Random instance termination | Netflix/AWS |
| **Gremlin** | Enterprise chaos platform | Multi-cloud |
| **Litmus** | Kubernetes-native chaos | Kubernetes |
| **Azure Chaos Studio** | Azure chaos experiments | Azure |
| **AWS Fault Injection Simulator** | AWS chaos experiments | AWS |
| **Chaos Mesh** | Cloud-native chaos platform | Kubernetes |

### Chaos Engineering Maturity Model

| Level | Description | Practices |
|-------|-------------|-----------|
| **Level 1** | Ad-hoc | Manual failure injection in test |
| **Level 2** | Repeatable | Documented experiments, staging environment |
| **Level 3** | Defined | Regular chaos days, production experiments |
| **Level 4** | Managed | Automated experiments, integrated in CI/CD |
| **Level 5** | Optimized | Continuous chaos, self-healing systems |

---

## Resilience Patterns

Resilience patterns help systems recover from failures and adapt to changing conditions.

### Circuit Breaker Pattern

Prevents cascading failures by stopping calls to failing services.

```
         ┌─────────────────────────────────────────────────────┐
         │                  Circuit Breaker States              │
         │                                                      │
         │   ┌─────────┐   failure threshold   ┌──────────┐    │
         │   │ CLOSED  │ ────────────────────▶ │   OPEN   │    │
         │   └─────────┘                        └──────────┘    │
         │        ▲                                   │         │
         │        │                                   │         │
         │        │                          timeout │         │
         │        │                          expires │         │
         │        │                                   ▼         │
         │        │       success            ┌───────────┐     │
         │        └────────────────────────  │ HALF-OPEN │     │
         │                                   └───────────┘     │
         │                                                      │
         └─────────────────────────────────────────────────────┘
```

| State | Behavior | Transition |
|-------|----------|------------|
| **Closed** | Requests flow normally; failures are counted | → Open: when failure count exceeds threshold |
| **Open** | Requests fail immediately (fail fast) | → Half-Open: after timeout period |
| **Half-Open** | Limited requests allowed to test recovery | → Closed: if test succeeds; → Open: if test fails |

**Configuration Parameters:**
- Failure threshold (e.g., 5 failures)
- Success threshold for recovery (e.g., 3 successes)
- Timeout duration (e.g., 30 seconds)
- Monitoring window (e.g., 10 seconds)

---

### Retry Pattern

Automatically retry failed operations with configurable strategies.

| Strategy | Description | Use Case | Example Delays |
|----------|-------------|----------|----------------|
| **Immediate Retry** | Retry instantly | Transient glitches | 0, 0, 0 |
| **Fixed Interval** | Wait fixed time between retries | Simple scenarios | 1s, 1s, 1s |
| **Linear Backoff** | Increase wait linearly | Moderate load | 1s, 2s, 3s |
| **Exponential Backoff** | Double wait time each retry | Overloaded services | 1s, 2s, 4s, 8s |
| **Exponential with Jitter** | Backoff + random delay | Prevent thundering herd | 1.2s, 2.7s, 4.1s |

**Retry Best Practices:**
- Set maximum retry count (typically 3-5)
- Only retry idempotent operations
- Use circuit breaker to stop retrying persistently failing services
- Log retry attempts for observability
- Consider operation timeout in total retry duration

```
┌────────────────────────────────────────────────────────────────────────┐
│               Exponential Backoff with Jitter                           │
│                                                                         │
│   Attempt 1     Attempt 2     Attempt 3     Attempt 4     Give Up      │
│       │             │             │             │             │         │
│       ▼             ▼             ▼             ▼             ▼         │
│   ────●────────────●────────────●────────────●────────────✗           │
│       ◀──1s+rand──▶◀───2s+rand──▶◀───4s+rand──▶◀───8s+rand──▶         │
│                                                                         │
└────────────────────────────────────────────────────────────────────────┘
```

---

### Bulkhead Pattern

Isolates components to prevent failure propagation, inspired by ship bulkheads that contain flooding.

```
┌───────────────────────────────────────────────────────────────┐
│                      Application                               │
│                                                                │
│  ┌─────────────────┐  ┌─────────────────┐  ┌───────────────┐  │
│  │   Bulkhead A    │  │   Bulkhead B    │  │  Bulkhead C   │  │
│  │  ┌───────────┐  │  │  ┌───────────┐  │  │ ┌───────────┐ │  │
│  │  │ Thread    │  │  │  │ Thread    │  │  │ │ Thread    │ │  │
│  │  │ Pool (20) │  │  │  │ Pool (20) │  │  │ │ Pool (10) │ │  │
│  │  └───────────┘  │  │  └───────────┘  │  │ └───────────┘ │  │
│  │       ▼         │  │       ▼         │  │      ▼        │  │
│  │  Service A      │  │  Service B      │  │  Service C    │  │
│  │  (Critical)     │  │  (Normal)       │  │  (Background) │  │
│  └─────────────────┘  └─────────────────┘  └───────────────┘  │
│                                                                │
│  If Service B becomes slow/fails, only Bulkhead B is affected │
│  Services A and C continue to operate normally                 │
│                                                                │
└───────────────────────────────────────────────────────────────┘
```

| Isolation Type | Description | Overhead |
|----------------|-------------|----------|
| **Thread Pool** | Separate thread pools per dependency | Medium |
| **Semaphore** | Limit concurrent calls without separate threads | Low |
| **Connection Pool** | Dedicated connection pools per service | Medium |
| **Process** | Separate processes/containers | High |
| **Service** | Separate service instances per customer/tenant | Highest |

---

### Timeout Pattern

Bound the time spent waiting for responses to prevent resource exhaustion.

| Timeout Type | Purpose | Typical Range |
|--------------|---------|---------------|
| **Connection Timeout** | Time to establish connection | 1-5 seconds |
| **Read Timeout** | Time to receive response | 5-30 seconds |
| **Write Timeout** | Time to send request | 5-30 seconds |
| **Idle Timeout** | Max time connection stays idle | 30-300 seconds |
| **Total/Request Timeout** | End-to-end operation time | Varies |

**Cascading Timeout Rule:**
```
Upstream timeout > Sum of downstream timeouts + processing time

Example:
  API Gateway (30s) > Auth Service (5s) + Business Service (15s) + DB (5s) + Buffer (5s)
```

---

### Fallback Pattern

Provide alternative responses when primary operation fails.

| Strategy | Description | Example |
|----------|-------------|---------|
| **Default Value** | Return safe static value | Show "0 items" instead of error |
| **Cached Response** | Return last known good value | Serve stale data from cache |
| **Alternative Service** | Call backup service | Use secondary API provider |
| **Graceful Degradation** | Reduced functionality | Show basic UI without personalization |
| **Queue for Later** | Store request for retry | Accept order, process later |
| **Fail Silent** | Return empty result | Skip non-critical feature |

---

### Rate Limiting Pattern

Protect services from overload by limiting request rates.

| Algorithm | Description | Characteristics |
|-----------|-------------|-----------------|
| **Token Bucket** | Tokens replenish at fixed rate; requests consume tokens | Allows bursts up to bucket size |
| **Leaky Bucket** | Requests queue and process at fixed rate | Smooths traffic, constant output |
| **Fixed Window** | Count requests in fixed time windows | Simple, but allows edge bursts |
| **Sliding Window** | Rolling window for smoother limiting | More accurate, higher memory |
| **Sliding Window Counter** | Combines fixed windows with weighted rolling | Good accuracy/memory balance |

```
┌────────────────────────────────────────────────────────────────────────┐
│                        Token Bucket Algorithm                           │
│                                                                         │
│   Bucket Capacity: 10 tokens    Refill Rate: 2 tokens/second           │
│                                                                         │
│       ┌───┐                                                            │
│       │ T │  ← Tokens added at fixed rate                              │
│       └─┬─┘                                                            │
│         ▼                                                               │
│   ┌───────────┐                                                        │
│   │ ● ● ● ● ● │  ← Bucket (max 10 tokens)                             │
│   │ ● ● ● ○ ○ │                                                        │
│   └─────┬─────┘                                                        │
│         ▼                                                               │
│   ┌───────────┐                                                        │
│   │  Request  │ → If token available: process, remove token            │
│   └───────────┘ → If no token: reject/queue                            │
│                                                                         │
└────────────────────────────────────────────────────────────────────────┘
```

---

### Health Check Pattern

Enable systems to monitor component health for automated recovery.

| Health Check Type | Purpose | Failure Action |
|-------------------|---------|----------------|
| **Liveness** | Is the process running? | Restart container/process |
| **Readiness** | Can service accept traffic? | Remove from load balancer |
| **Startup** | Has initialization completed? | Wait before other probes |
| **Deep Health** | Are dependencies healthy? | Alert, possibly failover |

**Health Check Implementation:**

```
┌────────────────────────────────────────────────────────────────┐
│                    Health Check Endpoints                       │
│                                                                 │
│  GET /health/live                                               │
│  └─▶ Returns: { "status": "UP" }                               │
│       Purpose: Basic process liveness                           │
│                                                                 │
│  GET /health/ready                                              │
│  └─▶ Returns: { "status": "UP", "checks": [...] }              │
│       Purpose: Ready to accept traffic                          │
│                                                                 │
│  GET /health (Deep Health)                                      │
│  └─▶ Returns: {                                                │
│         "status": "UP",                                         │
│         "components": {                                         │
│           "database": { "status": "UP", "latency": "5ms" },    │
│           "cache": { "status": "UP", "latency": "1ms" },       │
│           "queue": { "status": "UP", "depth": 42 }             │
│         }                                                       │
│       }                                                         │
│                                                                 │
└────────────────────────────────────────────────────────────────┘
```

---

## Key Metrics and SLOs

### Service Level Terminology

| Term | Definition | Example |
|------|------------|---------|
| **SLI (Service Level Indicator)** | Quantitative measure of service | 99.5% of requests < 200ms |
| **SLO (Service Level Objective)** | Target value for an SLI | Response time SLO: p99 < 200ms |
| **SLA (Service Level Agreement)** | Contract with consequences | 99.9% uptime or credit issued |

### Common Reliability SLIs

| Category | SLI | Measurement |
|----------|-----|-------------|
| **Availability** | Uptime percentage | (Total time - Downtime) / Total time |
| **Latency** | Response time percentiles | p50, p95, p99 response times |
| **Error Rate** | Percentage of failed requests | Failed requests / Total requests |
| **Throughput** | Requests processed | Requests per second/minute |
| **Durability** | Data loss rate | Lost records / Total records |
| **Correctness** | Accurate responses | Correct responses / Total responses |

### Error Budget

```
┌────────────────────────────────────────────────────────────────────────┐
│                         Error Budget Concept                            │
│                                                                         │
│   SLO: 99.9% availability                                              │
│   Error Budget: 0.1% = ~43 minutes/month                               │
│                                                                         │
│   Month Progress: ████████████████████░░░░░░░░░░ 60%                   │
│   Budget Used:    ██████░░░░░░░░░░░░░░░░░░░░░░░░ 20% (8.6 min)        │
│   Budget Left:    80% (34.4 min remaining)                              │
│                                                                         │
│   Decision Framework:                                                   │
│   ├─ Budget healthy → Deploy new features, experiment                  │
│   ├─ Budget low → Prioritize reliability work                          │
│   └─ Budget exhausted → Freeze deployments, focus on stability         │
│                                                                         │
└────────────────────────────────────────────────────────────────────────┘
```

---

## Best Practices

### Design Principles

1. **Assume failure** – Design for failure at every level
2. **Automate recovery** – Minimize human intervention in failure scenarios
3. **Test resilience** – Regularly test failure scenarios in production-like environments
4. **Monitor and alert** – Implement comprehensive observability
5. **Document runbooks** – Prepare procedures for common failure scenarios

### Implementation Checklist

#### High Availability
- [ ] No single points of failure identified
- [ ] Redundancy implemented for critical components
- [ ] Load balancing configured
- [ ] Health checks implemented
- [ ] Auto-scaling configured

#### Disaster Recovery
- [ ] RTO and RPO defined for each system
- [ ] DR strategy selected and implemented
- [ ] Data replication configured
- [ ] Failover procedures documented
- [ ] DR drills scheduled regularly

#### Resilience
- [ ] Circuit breakers implemented for external calls
- [ ] Retry policies configured with backoff
- [ ] Timeouts set for all external calls
- [ ] Fallback mechanisms defined
- [ ] Rate limiting implemented

#### Chaos Engineering
- [ ] Steady state metrics defined
- [ ] Initial chaos experiments designed
- [ ] Blast radius controls in place
- [ ] Experiment schedule established

### Anti-Patterns to Avoid

| Anti-Pattern | Problem | Solution |
|--------------|---------|----------|
| **Retry Storm** | Aggressive retries overwhelm recovering service | Exponential backoff, circuit breaker |
| **Cascading Failure** | One failure brings down entire system | Bulkheads, isolation, circuit breakers |
| **Thundering Herd** | All clients retry simultaneously | Jitter in retry delays |
| **Tight Coupling** | Failures propagate through dependencies | Async communication, loose coupling |
| **Missing Timeouts** | Requests hang indefinitely | Always set timeouts |
| **Silent Failures** | Errors go unnoticed | Comprehensive logging and alerting |

---

## Related Topics

- [Reliability Performance Operations Patterns](./reliability-performance-operations-patterns.md) - Comprehensive patterns reference
- [7.2 Performance Architecture](./7.2-performance-architecture.md) - Performance optimization
- [7.3 Observability Architecture](./7.3-observability-architecture/) - Monitoring and observability
- [Azure Site Recovery](../../architecture-azure/governance/azure-site-recovery-backup.md) - Azure DR implementation
- [Azure Monitor](../../architecture-azure/observability/) - Azure monitoring solutions

---

## References

- Microsoft Azure Well-Architected Framework - Reliability Pillar
- AWS Well-Architected Framework - Reliability Pillar
- Google SRE Book - Site Reliability Engineering
- Release It! - Michael Nygard
- Chaos Engineering - Casey Rosenthal & Nora Jones
