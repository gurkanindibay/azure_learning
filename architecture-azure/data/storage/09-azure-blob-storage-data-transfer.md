# Azure Blob Storage - Data Transfer Solutions

## Table of Contents

- [Overview](#overview)
- [Data Transfer Methods](#data-transfer-methods)
  - [Azure Import/Export Service](#azure-importexport-service)
    - [Required Files for Import/Export Jobs](#required-files-for-importexport-jobs)
  - [Azure Data Factory](#azure-data-factory)
  - [AzCopy](#azcopy)
  - [Azure Storage Explorer](#azure-storage-explorer)
  - [Azure Storage Mover](#azure-storage-mover)
- [Comparison Matrix](#comparison-matrix)
- [Exam Question Analysis](#exam-question-analysis)
  - [Question 1: Required Files for Azure Import/Export Service](#question-1-required-files-for-azure-importexport-service)
  - [Question 2: On-Premises File Server to Blob Storage Migration](#question-2-on-premises-file-server-to-blob-storage-migration)
  - [Question 3: Creating a Blob Container for VM Images](#question-3-creating-a-blob-container-for-vm-images)
  - [Question 4: Azure Storage Explorer Capabilities](#question-4-azure-storage-explorer-capabilities)
- [Best Practices](#best-practices)
- [References](#references)

---

## Overview

Transferring data from on-premises infrastructure to Azure Blob Storage is a common requirement for cloud migration scenarios. Azure provides multiple services and tools to facilitate data transfer, each optimized for different scenarios, data volumes, and network conditions.

This document covers the primary methods for transferring large amounts of data to Azure Blob Storage, with a focus on enterprise-scale migrations.

---

## Data Transfer Methods

### Azure Import/Export Service

**Azure Import/Export** is a physical data transfer service that allows you to securely transfer large amounts of data to Azure by shipping physical disk drives to Microsoft data centers.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Physical disk shipment to Azure data center |
| **Supported Services** | Azure Blob Storage, Azure Files |
| **Data Volume** | Optimized for large datasets (typically > 40 TB) |
| **Network Dependency** | Minimal - no internet bandwidth required for data transfer |
| **Transfer Speed** | Depends on shipping time + data copy time at Azure data center |
| **Use Cases** | Bandwidth-limited environments, initial bulk migrations, disaster recovery |

#### How It Works

1. **Prepare Disk Drives**: Use WAImportExport tool to prepare and encrypt disks (BitLocker)
2. **Ship to Azure**: Send drives to designated Azure data center via carrier
3. **Data Import**: Microsoft imports data to your Azure Storage account
4. **Return Shipment**: Microsoft returns drives to you (optional)

#### When to Use Azure Import/Export

âœ… **Use When:**
- Uploading hundreds of gigabytes or terabytes of data
- Network bandwidth is limited, expensive, or unreliable
- Internet upload would take several days or weeks
- Initial bulk migration to Azure
- Regulatory requirements prevent data transfer over public internet
- Cost of bandwidth exceeds shipping costs

âŒ **Don't Use When:**
- Data volume is small (< 40 TB can often be uploaded via network more efficiently)
- You need real-time or scheduled automated transfers
- Continuous data synchronization is required
- Time-to-cloud is critical (shipping adds days)

#### Required Files for Import/Export Jobs

Before preparing drives for an Azure Import/Export job, you must create **two critical CSV files**:

##### 1. Dataset CSV File

The **dataset CSV file** specifies the list of files and directories to be imported to the storage account. It provides details about the data you want to transfer.

**Purpose:**
- Defines the source data (files/directories) to be imported
- Specifies the destination path in Azure Storage
- Maps local data to blob containers or file shares

**Format:**
```csv
BasePath,DstBlobPathOrPrefix,BlobType,Disposition,MetadataFile,PropertiesFile
C:\Data\Documents\,documents/,BlockBlob,rename,metadata.txt,properties.txt
C:\Data\Images\,images/,BlockBlob,no-overwrite,,
C:\Data\Archive.zip,archive/Archive.zip,BlockBlob,overwrite,,
```

**Column Descriptions:**

| Column | Description | Required |
|--------|-------------|----------|
| `BasePath` | Source directory or file path on local system | Yes |
| `DstBlobPathOrPrefix` | Destination path/prefix in Azure Storage | Yes |
| `BlobType` | Type of blob: `BlockBlob`, `PageBlob`, or `AppendBlob` | Yes |
| `Disposition` | Action if blob exists: `rename`, `no-overwrite`, `overwrite` | Yes |
| `MetadataFile` | Optional metadata file for blobs | No |
| `PropertiesFile` | Optional properties file for blobs | No |

**Example Dataset CSV:**
```csv
BasePath,DstBlobPathOrPrefix,BlobType,Disposition,MetadataFile,PropertiesFile
C:\myfiles\documents\,import/documents/,BlockBlob,rename,,
C:\myfiles\videos\,import/videos/,BlockBlob,no-overwrite,,
```

##### 2. Driveset CSV File

The **driveset CSV file** contains information about the drives you will use for the import job. It maps the physical drives and provides Azure with the necessary information to identify and process them.

**Purpose:**
- Identifies the physical drives to be used
- Maps drive letters to their usage in the job
- Specifies BitLocker encryption keys for each drive

**Format:**
```csv
DriveLetter,FormatOption,SilentOrPromptOnFormat,Encryption,ExistingBitLockerKey
X:,Format,SilentMode,Encrypt,
Y:,AlreadyFormatted,SilentMode,AlreadyEncrypted,123456-789012-345678-901234-567890-123456-789012-345678
```

**Column Descriptions:**

| Column | Description | Required |
|--------|-------------|----------|
| `DriveLetter` | Drive letter of the disk to prepare | Yes |
| `FormatOption` | `Format` or `AlreadyFormatted` | Yes |
| `SilentOrPromptOnFormat` | `SilentMode` or `PromptOnFormat` | Yes |
| `Encryption` | `Encrypt` or `AlreadyEncrypted` | Yes |
| `ExistingBitLockerKey` | BitLocker key if already encrypted (48 digits) | Conditional |

**Example Driveset CSV:**
```csv
DriveLetter,FormatOption,SilentOrPromptOnFormat,Encryption,ExistingBitLockerKey
G:,Format,SilentMode,Encrypt,
H:,Format,SilentMode,Encrypt,
```

##### Using CSV Files with WAImportExport Tool

Once you've created both CSV files, use them with the WAImportExport tool:

```bash
# Prepare drives using both CSV files
WAImportExport.exe PrepImport \
  /j:JournalFile.jrn \
  /id:session1 \
  /sk:StorageAccountKey \
  /InitialDriveSet:driveset.csv \
  /DataSet:dataset.csv \
  /logdir:C:\Logs
```

**Parameters Explained:**
- `/j` - Journal file to track progress
- `/id` - Session identifier
- `/sk` - Storage account key for authentication
- `/InitialDriveSet` - Path to driveset CSV file
- `/DataSet` - Path to dataset CSV file
- `/logdir` - Directory for log files

#### Implementation Example

```bash
# 1. Install WAImportExport tool
# Download from: https://aka.ms/waiev2

# 2. Create dataset.csv
echo "BasePath,DstBlobPathOrPrefix,BlobType,Disposition,MetadataFile,PropertiesFile" > dataset.csv
echo "C:\\Data\\,imports/,BlockBlob,rename,," >> dataset.csv

# 3. Create driveset.csv
echo "DriveLetter,FormatOption,SilentOrPromptOnFormat,Encryption,ExistingBitLockerKey" > driveset.csv
echo "X:,Format,SilentMode,Encrypt," >> driveset.csv

# 4. Prepare drives with data
WAImportExport.exe PrepImport /j:FirstDrive.jrn /id:session1 /sk:YourStorageAccountKey /InitialDriveSet:driveset.csv /DataSet:dataset.csv /logdir:C:\Logs

# 5. Create Azure Import Job via Azure Portal
# - Specify storage account
# - Upload journal files
# - Provide shipping information

# 6. Ship drives to Azure data center
# 7. Monitor job status in Azure Portal
# 8. Verify data after import completes
```

#### Cost Considerations

- **Per-device handling fee**: ~$80-$150 per drive
- **Shipping costs**: Varies by carrier and distance
- **Storage costs**: Standard Azure Blob Storage pricing after import
- **Return shipping**: Optional, additional cost

---

### Azure Data Factory

**Azure Data Factory (ADF)** is a cloud-based ETL (Extract, Transform, Load) and data integration service that enables automated, scheduled data movement and transformation across various sources.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Network-based data pipelines |
| **Supported Sources** | 90+ connectors (on-premises, cloud, SaaS) |
| **Data Volume** | Suitable for any size, optimized for regular transfers |
| **Network Dependency** | Requires stable internet connectivity |
| **Automation** | Fully automated, scheduled, event-triggered pipelines |
| **Use Cases** | Ongoing data synchronization, scheduled backups, hybrid integration |

#### How It Works

1. **Install Self-Hosted Integration Runtime (SHIR)**: Installed on on-premises server to access local data
2. **Create Linked Services**: Define connections to source (on-premises) and destination (Azure Blob)
3. **Build Pipeline**: Configure copy activity to move data
4. **Schedule & Monitor**: Set triggers and monitor execution

#### When to Use Azure Data Factory

âœ… **Use When:**
- Need automated, scheduled data transfers
- Require data transformation during transfer
- Multiple data sources need to be integrated
- Ongoing synchronization between on-premises and cloud
- Want to orchestrate complex data workflows
- Need monitoring, logging, and alerting capabilities
- Data volume changes regularly and predictably

âŒ **Don't Use When:**
- One-time bulk transfer of massive data (> 1 PB) with poor bandwidth
- No need for automation or scheduling
- Simple file copy without transformation is sufficient
- Network bandwidth is severely limited or unavailable

#### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   On-Premises File Server   â”‚
â”‚         (Server1)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Self-Hosted Integration Runtime (SHIR)
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Azure Data Factory      â”‚
â”‚         (Pipeline)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Copy Activity
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Azure Blob Storage        â”‚
â”‚         (store1)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation Example

**Step 1: Install Self-Hosted Integration Runtime**
```powershell
# Download and install SHIR on Server1
# From Azure Portal: Azure Data Factory > Connections > Integration Runtimes
# Copy authentication key and register SHIR
```

**Step 2: Create Linked Services**
```json
// Source: File System Linked Service
{
  "name": "OnPremFileSystem",
  "type": "Microsoft.DataFactory/factories/linkedservices",
  "properties": {
    "type": "FileServer",
    "typeProperties": {
      "host": "\\\\Server1\\CompanyFiles",
      "userId": "domain\\username",
      "password": {
        "type": "SecureString",
        "value": "**********"
      }
    },
    "connectVia": {
      "referenceName": "SelfHostedIR",
      "type": "IntegrationRuntimeReference"
    }
  }
}

// Destination: Azure Blob Storage Linked Service
{
  "name": "AzureBlobStorage",
  "type": "Microsoft.DataFactory/factories/linkedservices",
  "properties": {
    "type": "AzureBlobStorage",
    "typeProperties": {
      "connectionString": "DefaultEndpointsProtocol=https;AccountName=store1;AccountKey=***"
    }
  }
}
```

**Step 3: Create Copy Pipeline**
```json
{
  "name": "CopyOnPremFilesToBlob",
  "properties": {
    "activities": [
      {
        "name": "CopyFiles",
        "type": "Copy",
        "inputs": [
          {
            "referenceName": "OnPremFiles",
            "type": "DatasetReference"
          }
        ],
        "outputs": [
          {
            "referenceName": "BlobStorageDestination",
            "type": "DatasetReference"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "FileSystemSource",
            "recursive": true
          },
          "sink": {
            "type": "BlobSink",
            "copyBehavior": "PreserveHierarchy"
          },
          "enableStaging": false,
          "parallelCopies": 32,
          "dataIntegrationUnits": 4
        }
      }
    ]
  }
}
```

**Step 4: Schedule & Execute**
```json
// Trigger: Daily scheduled execution
{
  "name": "DailyTrigger",
  "properties": {
    "type": "ScheduleTrigger",
    "typeProperties": {
      "recurrence": {
        "frequency": "Day",
        "interval": 1,
        "startTime": "2025-01-01T02:00:00Z"
      }
    },
    "pipelines": [
      {
        "pipelineReference": {
          "referenceName": "CopyOnPremFilesToBlob"
        }
      }
    ]
  }
}
```

#### Cost Considerations

- **Data Integration Units (DIU)**: ~$0.25 per DIU-hour
- **Self-Hosted IR**: No charge, but requires on-premises compute resources
- **Pipeline Orchestration**: ~$0.001 per 1,000 pipeline activities
- **Data Movement**: Included with DIU charges
- **Storage**: Standard Azure Blob Storage pricing

---

### AzCopy

**AzCopy** is a command-line utility for high-performance data transfer to and from Azure Storage.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Network-based command-line tool |
| **Supported Services** | Azure Blob Storage, Azure Files, Azure Data Lake Storage Gen2 |
| **Data Volume** | Suitable for any size |
| **Network Dependency** | Requires internet connectivity |
| **Automation** | Script-based automation possible |
| **Use Cases** | One-time transfers, scripted backups, developer workflows |

#### When to Use AzCopy

âœ… **Use When:**
- Need quick, one-time data transfer
- Comfortable with command-line tools
- Want simple, lightweight solution without infrastructure setup
- Need cross-platform support
- Require resumable uploads for reliability

âŒ **Don't Use When:**
- Need GUI-based management
- Require complex orchestration or transformation
- Want built-in monitoring and alerting dashboards
- Need to integrate with enterprise scheduling systems

#### Implementation Example

```bash
# Authenticate with Azure AD
azcopy login

# Copy entire directory to Blob Storage
azcopy copy 'C:\CompanyFiles\*' 'https://store1.blob.core.windows.net/data/' --recursive=true

# With SAS token authentication
azcopy copy 'C:\CompanyFiles\*' 'https://store1.blob.core.windows.net/data/?sv=2021-06-08&ss=b&srt=sco&sp=rwdlac&se=...' --recursive=true

# Monitor progress
# AzCopy provides real-time progress and can resume interrupted transfers
```

#### AzCopy Authentication Methods

AzCopy supports different authentication methods depending on the Azure Storage service you're targeting.

##### Blob Storage Authentication

**Supported Methods:**
- âœ… **Azure AD (Microsoft Entra ID)** - Recommended for security
- âœ… **Shared Access Signatures (SAS)** - For temporary, limited access

**Example with Azure AD:**
```bash
# Authenticate with Azure AD
azcopy login

# Copy to Blob Storage using Azure AD credentials
azcopy copy 'C:\LocalData\*' 'https://storage1.blob.core.windows.net/container/' --recursive=true
```

**Example with SAS:**
```bash
# Copy to Blob Storage using SAS token
azcopy copy 'C:\LocalData\*' 'https://storage1.blob.core.windows.net/container/?sv=2021-06-08&ss=b&srt=sco&sp=rwdlac&se=...' --recursive=true
```

##### File Storage Authentication

**Supported Methods:**
- âœ… **Shared Access Signatures (SAS) only** - Only SAS tokens are supported for Azure Files

**Important:** Azure AD authentication is **NOT supported** for Azure Files when using AzCopy.

**Example with SAS:**
```bash
# Copy to Azure Files using SAS token (ONLY option)
azcopy copy 'C:\LocalData\*' 'https://storage1.file.core.windows.net/share/?sv=2021-06-08&ss=f&srt=sco&sp=rwdlac&se=...' --recursive=true
```

**Authentication Method Summary:**

| Storage Service | Azure AD | SAS Token |
|----------------|----------|-----------|
| **Blob Storage** | âœ… Supported | âœ… Supported |
| **File Storage** | âŒ Not Supported | âœ… Supported (Only option) |

**Exam Takeaway:**
- ğŸ”‘ **Blob Storage**: Use Azure AD or SAS for AzCopy
- ğŸ”‘ **File Storage**: Use SAS only for AzCopy (Azure AD not supported)

---

### Azure Storage Explorer

**Azure Storage Explorer** is a standalone GUI application that enables easy management and interaction with Azure Storage resources. It provides a user-friendly interface for working with existing storage accounts but does not support creating storage accounts.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Interface** | Cross-platform desktop application (Windows, macOS, Linux) |
| **Supported Services** | Blob Storage, Azure Files, Queue Storage, Table Storage, Cosmos DB |
| **Authentication** | Azure AD, SAS tokens, storage account keys |
| **Primary Use** | Manage existing storage account resources |
| **Limitations** | Cannot create storage accounts (only manage existing ones) |

#### Capabilities

âœ… **What Storage Explorer CAN Do:**
- Upload and download blobs
- Create and manage blob containers
- Create and manage file shares
- Add data to tables and queues
- View and modify blob properties and metadata
- Copy blobs between storage accounts
- Generate SAS tokens for delegated access
- Manage access policies
- Browse and search storage resources
- Set blob access tiers (Hot, Cool, Archive)

âŒ **What Storage Explorer CANNOT Do:**
- Create new storage accounts (must use Azure Portal, Azure CLI, PowerShell, or ARM templates)
- Deploy Azure resources
- Configure storage account settings like redundancy type or networking rules
- Manage Azure subscriptions or resource groups

#### When to Use Azure Storage Explorer

âœ… **Use When:**
- Need GUI-based management of storage resources
- Working with existing storage accounts across multiple subscriptions
- Performing bulk operations (upload/download multiple files)
- Managing file shares, containers, and tables visually
- Developers need quick access to storage without using command-line tools
- Testing and debugging storage-related applications

âŒ **Don't Use When:**
- Need to create new storage accounts (use Azure Portal, CLI, or PowerShell)
- Require automation or scripting (use AzCopy, Azure CLI, or SDKs)
- Need programmatic access in applications (use Azure Storage SDKs)
- Working with very large data transfers (AzCopy or Import/Export may be more efficient)

#### Authentication Methods

Storage Explorer supports multiple authentication methods:

**1. Azure AD (Microsoft Entra ID)** - Recommended
```
Sign in with your Azure account credentials
Provides access to all resources you have permissions for
Supports multi-factor authentication (MFA)
```

**2. Shared Access Signature (SAS)**
```
Connect using a SAS token for limited-scope access
Useful for accessing specific containers or resources
Time-limited and permission-scoped access
```

**3. Storage Account Key**
```
Full access to storage account resources
Less secure than Azure AD or SAS
Should be avoided in production scenarios
```

#### Common Operations

**Upload Blobs to Container:**
1. Open Storage Explorer
2. Navigate to Storage Accounts â†’ [account name] â†’ Blob Containers
3. Select the target container (e.g., container1)
4. Click "Upload" button
5. Select files or folders
6. Choose blob type (Block Blob, Page Blob, Append Blob)
7. Click "Upload"

**Create File Share:**
1. Navigate to Storage Accounts â†’ [account name] â†’ File Shares
2. Click "Create File Share"
3. Enter share name
4. Set quota (optional)
5. Click "OK"

**Add Data to Table:**
1. Navigate to Storage Accounts â†’ [account name] â†’ Tables
2. Select the target table (e.g., table1)
3. Click "Add Entity"
4. Enter entity properties
5. Click "Insert"

---

### Question 4: Azure Storage Explorer Capabilities

#### Scenario

You have an Azure subscription that contains the following resources:

| Name | Type |
|------|------|
| storage1 | Storage account |
| container1 | Blob container |
| table1 | Storage table |

You need to perform the following tasks:

| Name | Type |
|------|------|
| Task1 | Create a new storage account |
| Task2 | Upload an append blob to container1 |
| Task3 | Create a file share in storage1 |
| Task4 | Add data to table1 |

#### Question

Which tasks can you perform by using Azure Storage Explorer?

**Answer Options:**

A. Task1, Task2, Task3, and Task4  
B. Task1, Task2, and Task3 only  
C. Task2, Task3, and Task4 only âœ… **CORRECT**  
D. Task1, Task3, and Task4 only  
E. Task1 and Task3 only

---

#### âœ… Correct Answer: C - Task2, Task3, and Task4 only

**Explanation:**

Azure Storage Explorer is designed to **manage and interact with existing Azure storage accounts and their resources** but **cannot create storage accounts**.

**Task Analysis:**

| Task | Can Storage Explorer Perform? | Explanation |
|------|------------------------------|-------------|
| **Task1: Create storage account** | âŒ No | Storage Explorer cannot create storage accounts. Storage accounts must be created using Azure Portal, Azure CLI, PowerShell, ARM templates, or Bicep. |
| **Task2: Upload append blob** | âœ… Yes | Storage Explorer supports uploading all blob types (Block, Append, Page) to existing containers. |
| **Task3: Create file share** | âœ… Yes | Storage Explorer can create and manage file shares within existing storage accounts. |
| **Task4: Add data to table** | âœ… Yes | Storage Explorer provides full support for Azure Table Storage operations, including adding, editing, and deleting entities. |

**Why Task1 Cannot Be Performed:**

Azure Storage Explorer is a **resource management tool**, not a **resource provisioning tool**. Creating a storage account is an Azure Resource Manager (ARM) operation that requires:
- Subscription-level permissions
- Resource group assignment
- Configuration of properties (region, redundancy, performance tier, etc.)
- Deployment through ARM templates, Azure Portal, CLI, or PowerShell

Storage Explorer operates at the **data plane** (managing storage contents), not the **control plane** (creating Azure resources).

**How to Create a Storage Account Instead:**

```bash
# Azure CLI
az storage account create \
  --name mystorageaccount \
  --resource-group myresourcegroup \
  --location eastus \
  --sku Standard_LRS

# Azure PowerShell
New-AzStorageAccount `
  -ResourceGroupName "myresourcegroup" `
  -Name "mystorageaccount" `
  -Location "eastus" `
  -SkuName "Standard_LRS"
```

**After Creating the Storage Account:**
Once the storage account exists, you can connect to it in Storage Explorer and perform Tasks 2, 3, and 4.

---

#### Key Takeaways

**Azure Storage Explorer Capabilities:**

| Operation Category | Supported? | Examples |
|-------------------|-----------|----------|
| **Data Operations** | âœ… Yes | Upload/download blobs, add table entities, create queues |
| **Container Management** | âœ… Yes | Create containers, file shares, queues, tables |
| **Access Management** | âœ… Yes | Generate SAS tokens, manage access policies |
| **Resource Creation** | âŒ No | Cannot create storage accounts |
| **Subscription Management** | âŒ No | Cannot manage Azure subscriptions |

**Exam Strategy:**

ğŸ¯ **Remember:**
- Storage Explorer = Management tool for **existing** storage accounts
- Storage Explorer â‰  Provisioning tool for **new** storage accounts
- Use Portal/CLI/PowerShell to create storage accounts
- Use Storage Explorer to manage storage account contents

ğŸ¯ **Decision Matrix:**
- Need to **create storage account** â†’ Azure Portal, CLI, PowerShell, ARM/Bicep
- Need to **upload blobs** â†’ Storage Explorer, AzCopy, Portal, SDKs
- Need to **create containers/shares** â†’ Storage Explorer, CLI, PowerShell, SDKs
- Need to **manage table data** â†’ Storage Explorer, SDKs, REST API

---

### Azure Storage Mover

**Azure Storage Mover** is a fully managed migration service for moving on-premises file shares to Azure Storage.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Managed service with on-premises agent |
| **Supported Services** | Azure Blob Storage, Azure Files |
| **Data Volume** | Suitable for large-scale migrations |
| **Network Dependency** | Requires internet connectivity |
| **Automation** | Fully managed with built-in scheduling |
| **Use Cases** | Large-scale file server migrations, NAS migrations |

#### When to Use Azure Storage Mover

âœ… **Use When:**
- Migrating file servers or NAS devices to Azure
- Need fully managed migration service
- Want minimal setup and maintenance
- Require migration project management features

âŒ **Don't Use When:**
- Need to transfer non-file-system data
- Require custom transformation logic
- Budget constraints (may be more expensive than AzCopy)

---

## Comparison Matrix

### Decision Matrix: Choosing the Right Data Transfer Method

| Factor | Azure Import/Export | Azure Data Factory | AzCopy | Azure Storage Explorer | Azure Storage Mover |
|--------|---------------------|-------------------|---------|------------------------|---------------------|
| **Interface** | Physical + CLI | Web-based + CLI | Command-line | Desktop GUI app | Web-based + Agent |
| **Data Volume** | > 40 TB optimal | Any size | Any size | Small to medium | Large datasets |
| **Transfer Speed** | Days (shipping) | Hours to days | Hours to days | Varies by network | Hours to days |
| **Bandwidth Required** | None | Moderate to high | Moderate to high | Depends on upload size | Moderate to high |
| **Automation** | Manual process | Fully automated | Script-based | Manual | Managed service |
| **Scheduling** | âŒ No | âœ… Yes | âš ï¸ Via external scheduler | âŒ No | âœ… Yes |
| **Transformation** | âŒ No | âœ… Yes | âŒ No | âŒ No | âŒ Limited |
| **Monitoring** | Azure Portal | Built-in ADF monitoring | Command-line logs | Built-in GUI | Built-in monitoring |
| **Create Storage Account** | âŒ No | âŒ No | âŒ No | âŒ No | âŒ No |
| **Cost for 500 GB** | ~$150 (one-time) | ~$5-20 (depending on DIUs) | Egress costs only | Free tool + bandwidth | Variable |
| **Setup Complexity** | High | Medium | Low | Very low | Low to medium |
| **Ongoing Use** | âŒ Not suitable | âœ… Excellent | âš ï¸ Manual | âœ… Good for ad-hoc | âœ… Good |
| **Use Case** | Initial bulk migration | Scheduled syncs, ETL | Quick transfers | GUI-based management | File server migrations |

### Quick Selection Guide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          HOW MUCH DATA DO YOU NEED TO TRANSFER?         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚
    < 40 TB                    > 40 TB
         â”‚                         â”‚
         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NEED AUTOMATION?â”‚      â”‚ LIMITED BANDWIDTH?   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚       â”‚                 â”‚           â”‚
    Yes     No                Yes         No
     â”‚       â”‚                 â”‚           â”‚
     â–¼       â–¼                 â–¼           â–¼
   ADF    GUI or CLI?  Import/Export    ADF + AzCopy
         â”Œâ”€â”€â”´â”€â”€â”
         â”‚     â”‚
       GUI    CLI
         â”‚     â”‚
         â–¼     â–¼
    Storage  AzCopy
    Explorer
```

---

## Exam Question Analysis

### Question 1: Required Files for Azure Import/Export Service

#### Scenario

You plan to use the **Azure Import/Export service** to copy files to a storage account.

#### Question

Which **TWO files** should you create before you prepare the drives for the import job?

Each correct answer presents part of the solution.

**NOTE**: Each correct selection is worth one point.

#### Answer Options

##### âŒ Option A: A PowerShell PS1 File

**Why This Is Wrong:**

While PowerShell can be used to **automate parts of the process**, it is **not mandatory** for creating or preparing drives for the import job.

**Key Points:**
- âŒ PowerShell scripts are optional for automation
- âŒ Not a required file for the WAImportExport tool
- âŒ The tool uses CSV files for configuration, not PowerShell scripts
- âš ï¸ PowerShell could be used to generate CSV files or automate tool execution, but it's not required

**What It Could Do (Optional):**
You might use PowerShell to automate CSV creation or call WAImportExport.exe, but the tool itself requires CSV files, not PS1 files.

---

##### âœ… Option B: A Dataset CSV File (CORRECT)

**Why This Is Correct:**

The **dataset CSV file** specifies the list of files and directories to be imported to the storage account. It provides details about the data you want to transfer.

**Key Points:**
- âœ… **Mandatory File**: Required by WAImportExport tool
- âœ… **Defines Source Data**: Lists files/directories to import
- âœ… **Specifies Destination**: Maps local paths to Azure Storage paths
- âœ… **Configuration Format**: CSV format with specific columns

**What It Contains:**
- Source directory or file paths on local system (`BasePath`)
- Destination path in Azure Storage (`DstBlobPathOrPrefix`)
- Blob type specification (`BlockBlob`, `PageBlob`, `AppendBlob`)
- Disposition rules (rename, no-overwrite, overwrite)
- Optional metadata and properties files

**Example:**
```csv
BasePath,DstBlobPathOrPrefix,BlobType,Disposition,MetadataFile,PropertiesFile
C:\myfiles\documents\,import/documents/,BlockBlob,rename,,
C:\myfiles\videos\,import/videos/,BlockBlob,no-overwrite,,
```

**Critical Purpose:**
This file outlines **what data** to import, **where** to import it from, and **where** in Azure Storage it should go.

---

##### âŒ Option C: An XML Manifest File

**Why This Is Wrong:**

**Not required** for the Azure Import/Export service. The service uses **CSV files**, not XML manifests.

**Key Points:**
- âŒ Azure Import/Export uses CSV format, not XML
- âŒ WAImportExport tool generates **journal (.jrn) files**, not XML manifests
- âŒ No XML configuration is needed for this service

**Confusion Point:**
Other Azure services may use XML for configuration, but Import/Export specifically requires CSV files for dataset and driveset configurations.

---

##### âŒ Option D: A JSON Configuration File

**Why This Is Wrong:**

**Not relevant** to Azure Import/Export jobs. The service requires CSV files, not JSON.

**Key Points:**
- âŒ Import/Export does not use JSON for configuration
- âŒ CSV is the required format for dataset and driveset files
- âŒ JSON might be used in other Azure services (ARM templates, etc.), but not here

**Confusion Point:**
While JSON is commonly used in Azure (ARM templates, Azure CLI output), the Import/Export service specifically requires CSV files.

---

##### âœ… Option E: A Driveset CSV File (CORRECT)

**Why This Is Correct:**

The **driveset CSV file** contains information about the drives you will use for the import job, such as drive IDs and mapping details.

**Key Points:**
- âœ… **Mandatory File**: Required by WAImportExport tool
- âœ… **Identifies Physical Drives**: Specifies which drives to prepare
- âœ… **Drive Configuration**: Format options, encryption settings
- âœ… **BitLocker Management**: Handles encryption key information

**What It Contains:**
- Drive letter of disks to prepare (`DriveLetter`)
- Format options (`Format` or `AlreadyFormatted`)
- Silent or prompt mode for formatting
- Encryption settings (`Encrypt` or `AlreadyEncrypted`)
- Existing BitLocker keys if already encrypted

**Example:**
```csv
DriveLetter,FormatOption,SilentOrPromptOnFormat,Encryption,ExistingBitLockerKey
X:,Format,SilentMode,Encrypt,
Y:,Format,SilentMode,Encrypt,
```

**Critical Purpose:**
This file maps the physical drives and provides Azure with the necessary information to identify and process them during the import job.

---

#### Summary: Two Required Files

When using the Azure Import/Export service, you need to prepare **two specific CSV files** before preparing the drives:

| File | Purpose | Required Columns |
|------|---------|------------------|
| **Dataset CSV** | Defines **what data** to import and **where** it goes in Azure | `BasePath`, `DstBlobPathOrPrefix`, `BlobType`, `Disposition` |
| **Driveset CSV** | Defines **which drives** to use and **how** to prepare them | `DriveLetter`, `FormatOption`, `SilentOrPromptOnFormat`, `Encryption` |

#### Complete Workflow

```
1. Create dataset.csv      â”€â”€â”
                             â”‚
2. Create driveset.csv     â”€â”€â”¤
                             â”œâ”€â”€â–º 3. Run WAImportExport.exe PrepImport
                             â”‚         (References both CSV files)
                             â”‚
                             â”œâ”€â”€â–º 4. Drives prepared and encrypted
                             â”‚
                             â””â”€â”€â–º 5. Ship drives to Azure data center

6. Microsoft imports data to storage account

7. Verify data in Azure Storage
```

#### Key Takeaway

**Always create both CSV files before running WAImportExport tool:**

âœ… **Dataset CSV** - Specifies the data to import  
âœ… **Driveset CSV** - Specifies the drives to use

âŒ PowerShell, XML, and JSON files are **not required** for Azure Import/Export jobs.

---

### Question 2: On-Premises File Server to Blob Storage Migration

#### Scenario

You have an Azure subscription that contains an Azure Blob storage account named **store1**.

You have an on-premises file server named **Server1** that runs Windows Server 2016. Server1 stores **500 GB** of company files.

**Requirement**: You need to store a copy of the company files from Server1 in store1.

**Question**: Which TWO Azure services can achieve this goal?

---

#### Answer Options

##### âœ… Option 1: Azure Import/Export Job (CORRECT)

**Why This Is Correct:**

Azure Import/Export allows you to securely transfer large amounts of data (like 500 GB) to Azure Blob Storage by shipping physical disk drives to Microsoft data centers.

**Key Points:**
- âœ… **Handles Large Data Volumes**: 500 GB is well within the service's capabilities
- âœ… **Bandwidth-Independent**: Ideal when internet bandwidth is limited, expensive, or unreliable
- âœ… **Secure Transfer**: Data is encrypted with BitLocker before shipping
- âœ… **Cost-Effective for Large Volumes**: When network transfer would take too long or be too expensive
- âœ… **Directly Supports Blob Storage**: Native integration with Azure Blob Storage

**Use Case Fit:**
When uploading 500 GB over the internet would take too long or exceed bandwidth costs, Import/Export provides a reliable alternative by physically shipping encrypted drives to Azure.

**Implementation Steps:**
1. Prepare drives using WAImportExport tool
2. Encrypt data with BitLocker
3. Create import job in Azure Portal
4. Ship drives to Azure data center
5. Microsoft imports data to store1
6. Drives returned to you

---

##### âœ… Option 2: Azure Data Factory (CORRECT)

**Why This Is Correct:**

Azure Data Factory supports the creation of data pipelines that can move files from an on-premises server to Azure Blob Storage using the **Self-hosted Integration Runtime (SHIR)**.

**Key Points:**
- âœ… **On-Premises to Cloud**: SHIR enables secure connectivity from Server1 to Azure
- âœ… **Automated Transfers**: Can schedule regular file synchronization
- âœ… **Enterprise ETL Service**: Cloud-based service designed for data movement
- âœ… **No Manual Intervention**: Fully automated after initial setup
- âœ… **Supports File Systems**: Native support for Windows file servers

**Use Case Fit:**
Perfect for automated, scheduled transfers from on-premises file servers to Azure Blob Storage. The Self-hosted Integration Runtime enables secure, reliable data movement without exposing the file server directly to the internet.

**Architecture:**
```
Server1 (On-Premises)
    â†“
Self-Hosted Integration Runtime
    â†“
Azure Data Factory Pipeline
    â†“
store1 (Azure Blob Storage)
```

**Implementation Steps:**
1. Install Self-hosted Integration Runtime on Server1 (or a machine with access to it)
2. Create linked service for on-premises file system
3. Create linked service for Azure Blob Storage (store1)
4. Create copy pipeline with source and destination datasets
5. Schedule or trigger pipeline execution

---

#### âŒ Incorrect Options

##### âŒ Option 3: Azure Analysis Services On-Premises Data Gateway

**Why This Is Wrong:**

Azure Analysis Services On-premises Data Gateway is designed for **live query connections** between on-premises data sources and Azure Analysis Services for business intelligence and analytics workloads.

**Key Points:**
- âŒ **Not a Data Transfer Tool**: Designed for query passthrough, not file copying
- âŒ **Purpose Mismatch**: Used for connecting tabular models to on-premises databases
- âŒ **No File Support**: Works with databases (SQL Server, Oracle, etc.), not file systems
- âŒ **Different Use Case**: Enables BI reports to query on-premises data in real-time

**What It Actually Does:**
Allows Azure Analysis Services to query on-premises databases (e.g., SQL Server) for live data refresh in BI models. It does NOT copy or move files.

---

##### âŒ Option 4: Azure Batch Account

**Why This Is Wrong:**

Azure Batch is designed for **large-scale parallel compute jobs** and high-performance computing (HPC) workloads, not data movement.

**Key Points:**
- âŒ **Not a Data Movement Tool**: Designed for running compute-intensive applications
- âŒ **Wrong Purpose**: Used for parallel job execution (rendering, simulations, data processing)
- âŒ **No Direct File Transfer**: Does not provide native support for transferring files from on-premises to Blob Storage
- âŒ **Requires Custom Code**: Would need significant custom development to use for file transfer

**What It Actually Does:**
Executes large-scale parallel and batch compute jobs across a pool of virtual machines. While Batch jobs can read/write from Blob Storage, it's not designed or optimized for simple file transfers.

**Example Use Cases:**
- Video rendering
- Financial risk modeling
- Scientific simulations
- Data processing pipelines

---

##### âŒ Option 5: Azure Logic Apps Integration Account

**Why This Is Wrong:**

Azure Logic Apps Integration Account is specifically designed for **B2B (Business-to-Business) integrations** with support for EDI, XML, and AS2 protocols.

**Key Points:**
- âŒ **Not for Bulk File Transfer**: Designed for enterprise integration patterns (EDI, AS2, X12, EDIFACT)
- âŒ **Different Purpose**: Used for B2B message exchanges between trading partners
- âŒ **Requires Logic Apps**: Integration accounts are add-ons for Logic Apps, not standalone data transfer services
- âŒ **Overkill for Simple Copy**: Not designed for simple file server to blob storage scenarios

**What It Actually Does:**
Provides artifacts (schemas, maps, agreements, certificates) for B2B integration scenarios in Logic Apps. Used when exchanging EDI messages or XML documents with external partners.

**When You Would Use It:**
- Trading partner EDI document exchange (X12, EDIFACT)
- XML transformations with XSLT maps
- AS2 protocol message exchange
- B2B certificate management

**Note:** While Logic Apps (without integration account) CAN copy files, it's not listed as an option and would be less efficient than ADF or Import/Export for 500 GB.

---

#### Key Takeaways

| Service | Valid for 500 GB File Transfer? | Primary Purpose |
|---------|----------------------------------|-----------------|
| **Azure Import/Export** | âœ… Yes | Physical disk-based bulk data transfer |
| **Azure Data Factory** | âœ… Yes | Automated cloud-based ETL and data movement |
| **Analysis Services Gateway** | âŒ No | Live query connections for BI |
| **Azure Batch** | âŒ No | Parallel compute job execution |
| **Logic Apps Integration Account** | âŒ No | B2B/EDI enterprise integrations |

---
### Question 3: Creating a Blob Container for VM Images

#### Scenario

You have an Azure subscription that contains an Azure Storage account.

You plan to copy an on-premises virtual machine image to a container named **vmimages**.

**Requirement**: You need to create the container for the planned image.

**Question**: Which command should you run?

---

#### Command Format

```
azcopy [command] 'https://[storageaccount].[service].core.windows.net/[containername]'
```

---

#### Answer

The correct command is:

```bash
azcopy make 'https://mystorageaccount.blob.core.windows.net/vmimages'
```

**Components:**
- **Command**: `make` - Creates a new container or file share
- **Service**: `blob` - Specifies Azure Blob Storage service
- **Container Name**: `vmimages` - The container to be created

---

#### Explanation

##### âœ… Correct Components

**1. Command: `make`**

The `azcopy make` command is specifically designed to create containers and file shares in Azure Storage.

**Key Points:**
- âœ… **Creates containers**: Specifically used for creating new blob containers
- âœ… **Creates file shares**: Can also create Azure Files shares
- âœ… **Idempotent**: Safe to run multiple times (won't error if container exists)
- âœ… **Simple syntax**: Requires only the container URL

**Usage:**
```bash
# Create a blob container
azcopy make 'https://storageaccount.blob.core.windows.net/containername'

# Create a file share
azcopy make 'https://storageaccount.file.core.windows.net/sharename'
```

**2. Service: `blob`**

The blob service endpoint is required for creating blob containers.

**Key Points:**
- âœ… **Blob Storage**: Uses `.blob.core.windows.net` endpoint
- âœ… **Container Storage**: Blob containers are part of Azure Blob Storage
- âœ… **VM Image Storage**: VM images are typically stored as blobs (VHD/VHDX files)

**Service Endpoint Patterns:**
- **Blob Storage**: `https://[account].blob.core.windows.net/`
- **File Storage**: `https://[account].file.core.windows.net/`
- **Queue Storage**: `https://[account].queue.core.windows.net/`
- **Table Storage**: `https://[account].table.core.windows.net/`

---

##### âŒ Incorrect Options

**Command Alternatives:**

**âŒ `azcopy copy`**
- **Purpose**: Copies data between locations
- **Why Wrong**: Used for copying files/blobs, not creating containers
- **Use Case**: `azcopy copy 'source' 'destination'`

**âŒ `azcopy sync`**
- **Purpose**: Synchronizes source and destination locations
- **Why Wrong**: Syncs content between existing locations, doesn't create containers
- **Use Case**: `azcopy sync 'source' 'destination'` (one-way sync)

**Service Alternatives:**

**âŒ `file`**
- **Purpose**: Azure Files service endpoint
- **Why Wrong**: Creates file shares, not blob containers
- **Example**: `https://storageaccount.file.core.windows.net/sharename`

**âŒ `queue`**
- **Purpose**: Azure Queue Storage service endpoint
- **Why Wrong**: Queue storage is for message queues, not container storage
- **Example**: `https://storageaccount.queue.core.windows.net/queuename`

**âŒ `table`**
- **Purpose**: Azure Table Storage service endpoint
- **Why Wrong**: Table storage is for NoSQL data, not blob containers
- **Example**: `https://storageaccount.table.core.windows.net/tablename`

**âŒ `dfs`**
- **Purpose**: Azure Data Lake Storage Gen2 endpoint
- **Why Wrong**: While ADLS Gen2 uses blob storage underneath, the question specifies creating a container for VM images, which uses the blob endpoint
- **Example**: `https://storageaccount.dfs.core.windows.net/filesystem`

**âŒ `images`**
- **Purpose**: Not a valid Azure Storage service
- **Why Wrong**: There is no "images" service in Azure Storage
- **Note**: Images are stored as blobs in blob containers

---

#### Complete Workflow Example

After creating the container, you would copy the VM image:

```bash
# Step 1: Authenticate
azcopy login

# Step 2: Create the container
azcopy make 'https://mystorageaccount.blob.core.windows.net/vmimages'

# Step 3: Copy VM image to the container
azcopy copy 'C:\VMs\myvm.vhd' 'https://mystorageaccount.blob.core.windows.net/vmimages/myvm.vhd'

# Step 4: Verify the upload
# Check in Azure Portal or use Azure CLI
az storage blob list --account-name mystorageaccount --container-name vmimages
```

---

#### Key Takeaways

**AzCopy Commands:**

| Command | Purpose | Example |
|---------|---------|---------|
| **`azcopy make`** | Create container/file share | `azcopy make 'https://account.blob.core.windows.net/container'` |
| **`azcopy copy`** | Copy files/blobs | `azcopy copy 'source' 'destination'` |
| **`azcopy sync`** | Synchronize locations | `azcopy sync 'source' 'destination'` |
| **`azcopy remove`** | Delete blobs | `azcopy remove 'https://account.blob.core.windows.net/container/*'` |
| **`azcopy list`** | List blobs | `azcopy list 'https://account.blob.core.windows.net/container'` |

**Azure Storage Service Endpoints:**

| Service | Endpoint Pattern | Use Case |
|---------|------------------|----------|
| **Blob Storage** | `https://[account].blob.core.windows.net/` | Object storage, VM images, backups |
| **File Storage** | `https://[account].file.core.windows.net/` | SMB file shares |
| **Queue Storage** | `https://[account].queue.core.windows.net/` | Message queues |
| **Table Storage** | `https://[account].table.core.windows.net/` | NoSQL key-value storage |
| **Data Lake Gen2** | `https://[account].dfs.core.windows.net/` | Big data analytics |

---

#### Exam Strategy

**For the Exam, Remember:**

ğŸ¯ **Container Creation:**
- Use `azcopy make` to create new containers
- Blob containers use `.blob.core.windows.net` endpoint
- File shares use `.file.core.windows.net` endpoint

ğŸ¯ **Common AzCopy Commands:**
- `make` = Create container/share
- `copy` = Copy data
- `sync` = Synchronize (one-way)
- `remove` = Delete blobs
- `list` = List contents

ğŸ¯ **VM Image Storage:**
- VM images (VHD/VHDX) are stored in blob containers
- Use blob service endpoint, not file or other services
- Container must exist before uploading VM images

---
#### Exam Strategy

**For the Exam, Remember:**

ğŸ¯ **Data Transfer Services:**
- Azure Import/Export: Physical disk shipment for bulk data
- Azure Data Factory: Automated, scheduled data pipelines
- AzCopy: Command-line tool for network-based transfers

ğŸ¯ **NOT Data Transfer Services:**
- Analysis Services Gateway: BI live queries only
- Azure Batch: Compute jobs, not data movement
- Logic Apps Integration Account: B2B/EDI scenarios

ğŸ¯ **Decision Factors:**
- **Large volume + Limited bandwidth** â†’ Import/Export
- **Automation + Scheduling needed** â†’ Data Factory
- **One-time quick transfer** â†’ AzCopy
- **B2B integrations** â†’ Logic Apps Integration Account (but NOT for file transfers)
- **Live BI queries** â†’ Analysis Services Gateway

---

## Best Practices

### Planning Your Data Transfer

1. **Assess Data Volume and Frequency**
   - One-time bulk migration â†’ Import/Export or AzCopy
   - Ongoing synchronization â†’ Azure Data Factory
   - Regular scheduled backups â†’ Azure Data Factory

2. **Evaluate Network Bandwidth**
   - Limited/expensive bandwidth â†’ Import/Export
   - Sufficient bandwidth â†’ ADF or AzCopy

3. **Consider Automation Requirements**
   - Need scheduling/monitoring â†’ Azure Data Factory
   - Manual/scripted transfers â†’ AzCopy

4. **Budget Considerations**
   - Compare network egress costs vs. Import/Export fees
   - Factor in time-to-cloud and business requirements

5. **Security and Compliance**
   - Data encryption in transit (ADF, AzCopy: TLS)
   - Data encryption at rest (Import/Export: BitLocker)
   - Network isolation (ADF with private endpoints)

### Performance Optimization

#### Azure Data Factory
- Use appropriate Data Integration Units (DIUs)
- Enable parallel copies
- Optimize file sizes (avoid many small files)
- Use staging when needed

#### AzCopy
- Use `--block-size-mb` for large files
- Adjust `--cap-mbps` to control bandwidth usage
- Enable `--recursive` for directory copies
- Use `--overwrite=false` to skip existing files

#### Import/Export
- Use multiple drives for faster processing at Azure
- Optimize data organization before preparing drives
- Use SATA II/III or SSD drives for faster copying

---

## References

### Microsoft Learn Documentation

- [Azure Import/Export Service Overview](https://learn.microsoft.com/en-us/azure/import-export/storage-import-export-service)
- [Azure Data Factory Introduction](https://learn.microsoft.com/en-us/azure/data-factory/introduction)
- [Azure Data Factory Copy Activity](https://learn.microsoft.com/en-us/azure/data-factory/copy-data-tool)
- [Get Started with AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)
- [Azure Storage Mover Overview](https://learn.microsoft.com/en-us/azure/storage-mover/service-overview)

### Related Topics

- [Azure Storage Redundancy Options](./01-azure-storage-redundancy-options.md)
- [Azure Storage Secure Access](./02-azure-storage-secure-access.md)
- [Azure Blob Storage API](./03-azure-blob-storage-api.md)
- [Azure Data Lake Storage Gen2 Migration](./08-azure-data-lake-storage-gen2-migration.md)

---

**Domain**: Design data storage solutions  
**Last Updated**: December 2025
