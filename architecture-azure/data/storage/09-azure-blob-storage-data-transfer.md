# Azure Blob Storage - Data Transfer Solutions

## Table of Contents

- [Overview](#overview)
- [Data Transfer Methods](#data-transfer-methods)
  - [Azure Import/Export Service](#azure-importexport-service)
  - [Azure Data Factory](#azure-data-factory)
  - [AzCopy](#azcopy)
  - [Azure Storage Mover](#azure-storage-mover)
- [Comparison Matrix](#comparison-matrix)
- [Exam Question Analysis](#exam-question-analysis)
  - [Question: On-Premises File Server to Blob Storage Migration](#question-on-premises-file-server-to-blob-storage-migration)
- [Best Practices](#best-practices)
- [References](#references)

---

## Overview

Transferring data from on-premises infrastructure to Azure Blob Storage is a common requirement for cloud migration scenarios. Azure provides multiple services and tools to facilitate data transfer, each optimized for different scenarios, data volumes, and network conditions.

This document covers the primary methods for transferring large amounts of data to Azure Blob Storage, with a focus on enterprise-scale migrations.

---

## Data Transfer Methods

### Azure Import/Export Service

**Azure Import/Export** is a physical data transfer service that allows you to securely transfer large amounts of data to Azure by shipping physical disk drives to Microsoft data centers.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Physical disk shipment to Azure data center |
| **Supported Services** | Azure Blob Storage, Azure Files |
| **Data Volume** | Optimized for large datasets (typically > 40 TB) |
| **Network Dependency** | Minimal - no internet bandwidth required for data transfer |
| **Transfer Speed** | Depends on shipping time + data copy time at Azure data center |
| **Use Cases** | Bandwidth-limited environments, initial bulk migrations, disaster recovery |

#### How It Works

1. **Prepare Disk Drives**: Use WAImportExport tool to prepare and encrypt disks (BitLocker)
2. **Ship to Azure**: Send drives to designated Azure data center via carrier
3. **Data Import**: Microsoft imports data to your Azure Storage account
4. **Return Shipment**: Microsoft returns drives to you (optional)

#### When to Use Azure Import/Export

âœ… **Use When:**
- Uploading hundreds of gigabytes or terabytes of data
- Network bandwidth is limited, expensive, or unreliable
- Internet upload would take several days or weeks
- Initial bulk migration to Azure
- Regulatory requirements prevent data transfer over public internet
- Cost of bandwidth exceeds shipping costs

âŒ **Don't Use When:**
- Data volume is small (< 40 TB can often be uploaded via network more efficiently)
- You need real-time or scheduled automated transfers
- Continuous data synchronization is required
- Time-to-cloud is critical (shipping adds days)

#### Implementation Example

```bash
# 1. Install WAImportExport tool
# Download from: https://aka.ms/waiev2

# 2. Prepare drives with data
WAImportExport.exe PrepImport /j:FirstDrive.jrn /id:session1 /srcdir:C:\Data /dstdir:imports/ /bk:489A35C73217F582DDD22B82456E82BB7C1E5B06EA8F1B3E8DD6F75E8D7DA15B /encrypt /logdir:C:\Logs

# 3. Create Azure Import Job via Azure Portal
# - Specify storage account
# - Upload journal files
# - Provide shipping information

# 4. Ship drives to Azure data center
# 5. Monitor job status in Azure Portal
# 6. Verify data after import completes
```

#### Cost Considerations

- **Per-device handling fee**: ~$80-$150 per drive
- **Shipping costs**: Varies by carrier and distance
- **Storage costs**: Standard Azure Blob Storage pricing after import
- **Return shipping**: Optional, additional cost

---

### Azure Data Factory

**Azure Data Factory (ADF)** is a cloud-based ETL (Extract, Transform, Load) and data integration service that enables automated, scheduled data movement and transformation across various sources.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Network-based data pipelines |
| **Supported Sources** | 90+ connectors (on-premises, cloud, SaaS) |
| **Data Volume** | Suitable for any size, optimized for regular transfers |
| **Network Dependency** | Requires stable internet connectivity |
| **Automation** | Fully automated, scheduled, event-triggered pipelines |
| **Use Cases** | Ongoing data synchronization, scheduled backups, hybrid integration |

#### How It Works

1. **Install Self-Hosted Integration Runtime (SHIR)**: Installed on on-premises server to access local data
2. **Create Linked Services**: Define connections to source (on-premises) and destination (Azure Blob)
3. **Build Pipeline**: Configure copy activity to move data
4. **Schedule & Monitor**: Set triggers and monitor execution

#### When to Use Azure Data Factory

âœ… **Use When:**
- Need automated, scheduled data transfers
- Require data transformation during transfer
- Multiple data sources need to be integrated
- Ongoing synchronization between on-premises and cloud
- Want to orchestrate complex data workflows
- Need monitoring, logging, and alerting capabilities
- Data volume changes regularly and predictably

âŒ **Don't Use When:**
- One-time bulk transfer of massive data (> 1 PB) with poor bandwidth
- No need for automation or scheduling
- Simple file copy without transformation is sufficient
- Network bandwidth is severely limited or unavailable

#### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   On-Premises File Server   â”‚
â”‚         (Server1)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Self-Hosted Integration Runtime (SHIR)
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Azure Data Factory      â”‚
â”‚         (Pipeline)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Copy Activity
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Azure Blob Storage        â”‚
â”‚         (store1)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation Example

**Step 1: Install Self-Hosted Integration Runtime**
```powershell
# Download and install SHIR on Server1
# From Azure Portal: Azure Data Factory > Connections > Integration Runtimes
# Copy authentication key and register SHIR
```

**Step 2: Create Linked Services**
```json
// Source: File System Linked Service
{
  "name": "OnPremFileSystem",
  "type": "Microsoft.DataFactory/factories/linkedservices",
  "properties": {
    "type": "FileServer",
    "typeProperties": {
      "host": "\\\\Server1\\CompanyFiles",
      "userId": "domain\\username",
      "password": {
        "type": "SecureString",
        "value": "**********"
      }
    },
    "connectVia": {
      "referenceName": "SelfHostedIR",
      "type": "IntegrationRuntimeReference"
    }
  }
}

// Destination: Azure Blob Storage Linked Service
{
  "name": "AzureBlobStorage",
  "type": "Microsoft.DataFactory/factories/linkedservices",
  "properties": {
    "type": "AzureBlobStorage",
    "typeProperties": {
      "connectionString": "DefaultEndpointsProtocol=https;AccountName=store1;AccountKey=***"
    }
  }
}
```

**Step 3: Create Copy Pipeline**
```json
{
  "name": "CopyOnPremFilesToBlob",
  "properties": {
    "activities": [
      {
        "name": "CopyFiles",
        "type": "Copy",
        "inputs": [
          {
            "referenceName": "OnPremFiles",
            "type": "DatasetReference"
          }
        ],
        "outputs": [
          {
            "referenceName": "BlobStorageDestination",
            "type": "DatasetReference"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "FileSystemSource",
            "recursive": true
          },
          "sink": {
            "type": "BlobSink",
            "copyBehavior": "PreserveHierarchy"
          },
          "enableStaging": false,
          "parallelCopies": 32,
          "dataIntegrationUnits": 4
        }
      }
    ]
  }
}
```

**Step 4: Schedule & Execute**
```json
// Trigger: Daily scheduled execution
{
  "name": "DailyTrigger",
  "properties": {
    "type": "ScheduleTrigger",
    "typeProperties": {
      "recurrence": {
        "frequency": "Day",
        "interval": 1,
        "startTime": "2025-01-01T02:00:00Z"
      }
    },
    "pipelines": [
      {
        "pipelineReference": {
          "referenceName": "CopyOnPremFilesToBlob"
        }
      }
    ]
  }
}
```

#### Cost Considerations

- **Data Integration Units (DIU)**: ~$0.25 per DIU-hour
- **Self-Hosted IR**: No charge, but requires on-premises compute resources
- **Pipeline Orchestration**: ~$0.001 per 1,000 pipeline activities
- **Data Movement**: Included with DIU charges
- **Storage**: Standard Azure Blob Storage pricing

---

### AzCopy

**AzCopy** is a command-line utility for high-performance data transfer to and from Azure Storage.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Network-based command-line tool |
| **Supported Services** | Azure Blob Storage, Azure Files, Azure Data Lake Storage Gen2 |
| **Data Volume** | Suitable for any size |
| **Network Dependency** | Requires internet connectivity |
| **Automation** | Script-based automation possible |
| **Use Cases** | One-time transfers, scripted backups, developer workflows |

#### When to Use AzCopy

âœ… **Use When:**
- Need quick, one-time data transfer
- Comfortable with command-line tools
- Want simple, lightweight solution without infrastructure setup
- Need cross-platform support
- Require resumable uploads for reliability

âŒ **Don't Use When:**
- Need GUI-based management
- Require complex orchestration or transformation
- Want built-in monitoring and alerting dashboards
- Need to integrate with enterprise scheduling systems

#### Implementation Example

```bash
# Authenticate with Azure AD
azcopy login

# Copy entire directory to Blob Storage
azcopy copy 'C:\CompanyFiles\*' 'https://store1.blob.core.windows.net/data/' --recursive=true

# With SAS token authentication
azcopy copy 'C:\CompanyFiles\*' 'https://store1.blob.core.windows.net/data/?sv=2021-06-08&ss=b&srt=sco&sp=rwdlac&se=...' --recursive=true

# Monitor progress
# AzCopy provides real-time progress and can resume interrupted transfers
```

#### AzCopy Authentication Methods

AzCopy supports different authentication methods depending on the Azure Storage service you're targeting.

##### Blob Storage Authentication

**Supported Methods:**
- âœ… **Azure AD (Microsoft Entra ID)** - Recommended for security
- âœ… **Shared Access Signatures (SAS)** - For temporary, limited access

**Example with Azure AD:**
```bash
# Authenticate with Azure AD
azcopy login

# Copy to Blob Storage using Azure AD credentials
azcopy copy 'C:\LocalData\*' 'https://storage1.blob.core.windows.net/container/' --recursive=true
```

**Example with SAS:**
```bash
# Copy to Blob Storage using SAS token
azcopy copy 'C:\LocalData\*' 'https://storage1.blob.core.windows.net/container/?sv=2021-06-08&ss=b&srt=sco&sp=rwdlac&se=...' --recursive=true
```

##### File Storage Authentication

**Supported Methods:**
- âœ… **Shared Access Signatures (SAS) only** - Only SAS tokens are supported for Azure Files

**Important:** Azure AD authentication is **NOT supported** for Azure Files when using AzCopy.

**Example with SAS:**
```bash
# Copy to Azure Files using SAS token (ONLY option)
azcopy copy 'C:\LocalData\*' 'https://storage1.file.core.windows.net/share/?sv=2021-06-08&ss=f&srt=sco&sp=rwdlac&se=...' --recursive=true
```

**Authentication Method Summary:**

| Storage Service | Azure AD | SAS Token |
|----------------|----------|-----------|
| **Blob Storage** | âœ… Supported | âœ… Supported |
| **File Storage** | âŒ Not Supported | âœ… Supported (Only option) |

**Exam Takeaway:**
- ğŸ”‘ **Blob Storage**: Use Azure AD or SAS for AzCopy
- ğŸ”‘ **File Storage**: Use SAS only for AzCopy (Azure AD not supported)

---

### Azure Storage Mover

**Azure Storage Mover** is a fully managed migration service for moving on-premises file shares to Azure Storage.

#### Key Characteristics

| Feature | Description |
|---------|-------------|
| **Transfer Method** | Managed service with on-premises agent |
| **Supported Services** | Azure Blob Storage, Azure Files |
| **Data Volume** | Suitable for large-scale migrations |
| **Network Dependency** | Requires internet connectivity |
| **Automation** | Fully managed with built-in scheduling |
| **Use Cases** | Large-scale file server migrations, NAS migrations |

#### When to Use Azure Storage Mover

âœ… **Use When:**
- Migrating file servers or NAS devices to Azure
- Need fully managed migration service
- Want minimal setup and maintenance
- Require migration project management features

âŒ **Don't Use When:**
- Need to transfer non-file-system data
- Require custom transformation logic
- Budget constraints (may be more expensive than AzCopy)

---

## Comparison Matrix

### Decision Matrix: Choosing the Right Data Transfer Method

| Factor | Azure Import/Export | Azure Data Factory | AzCopy | Azure Storage Mover |
|--------|---------------------|-------------------|---------|---------------------|
| **Data Volume** | > 40 TB optimal | Any size | Any size | Large datasets |
| **Transfer Speed** | Days (shipping) | Hours to days | Hours to days | Hours to days |
| **Bandwidth Required** | None | Moderate to high | Moderate to high | Moderate to high |
| **Automation** | Manual process | Fully automated | Script-based | Managed service |
| **Scheduling** | âŒ No | âœ… Yes | âš ï¸ Via external scheduler | âœ… Yes |
| **Transformation** | âŒ No | âœ… Yes | âŒ No | âŒ Limited |
| **Monitoring** | Azure Portal | Built-in ADF monitoring | Command-line logs | Built-in monitoring |
| **Cost for 500 GB** | ~$150 (one-time) | ~$5-20 (depending on DIUs) | Egress costs only | Variable |
| **Setup Complexity** | High | Medium | Low | Low to medium |
| **Ongoing Use** | âŒ Not suitable | âœ… Excellent | âš ï¸ Manual | âœ… Good |
| **Use Case** | Initial bulk migration | Scheduled syncs, ETL | Quick transfers | File server migrations |

### Quick Selection Guide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          HOW MUCH DATA DO YOU NEED TO TRANSFER?         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚
    < 40 TB                    > 40 TB
         â”‚                         â”‚
         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NEED AUTOMATION?â”‚      â”‚ LIMITED BANDWIDTH?   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚       â”‚                 â”‚           â”‚
    Yes     No                Yes         No
     â”‚       â”‚                 â”‚           â”‚
     â–¼       â–¼                 â–¼           â–¼
   ADF    AzCopy      Import/Export    ADF + AzCopy
```

---

## Exam Question Analysis

### Question: On-Premises File Server to Blob Storage Migration

#### Scenario

You have an Azure subscription that contains an Azure Blob storage account named **store1**.

You have an on-premises file server named **Server1** that runs Windows Server 2016. Server1 stores **500 GB** of company files.

**Requirement**: You need to store a copy of the company files from Server1 in store1.

**Question**: Which TWO Azure services can achieve this goal?

---

#### Answer Options

##### âœ… Option 1: Azure Import/Export Job (CORRECT)

**Why This Is Correct:**

Azure Import/Export allows you to securely transfer large amounts of data (like 500 GB) to Azure Blob Storage by shipping physical disk drives to Microsoft data centers.

**Key Points:**
- âœ… **Handles Large Data Volumes**: 500 GB is well within the service's capabilities
- âœ… **Bandwidth-Independent**: Ideal when internet bandwidth is limited, expensive, or unreliable
- âœ… **Secure Transfer**: Data is encrypted with BitLocker before shipping
- âœ… **Cost-Effective for Large Volumes**: When network transfer would take too long or be too expensive
- âœ… **Directly Supports Blob Storage**: Native integration with Azure Blob Storage

**Use Case Fit:**
When uploading 500 GB over the internet would take too long or exceed bandwidth costs, Import/Export provides a reliable alternative by physically shipping encrypted drives to Azure.

**Implementation Steps:**
1. Prepare drives using WAImportExport tool
2. Encrypt data with BitLocker
3. Create import job in Azure Portal
4. Ship drives to Azure data center
5. Microsoft imports data to store1
6. Drives returned to you

---

##### âœ… Option 2: Azure Data Factory (CORRECT)

**Why This Is Correct:**

Azure Data Factory supports the creation of data pipelines that can move files from an on-premises server to Azure Blob Storage using the **Self-hosted Integration Runtime (SHIR)**.

**Key Points:**
- âœ… **On-Premises to Cloud**: SHIR enables secure connectivity from Server1 to Azure
- âœ… **Automated Transfers**: Can schedule regular file synchronization
- âœ… **Enterprise ETL Service**: Cloud-based service designed for data movement
- âœ… **No Manual Intervention**: Fully automated after initial setup
- âœ… **Supports File Systems**: Native support for Windows file servers

**Use Case Fit:**
Perfect for automated, scheduled transfers from on-premises file servers to Azure Blob Storage. The Self-hosted Integration Runtime enables secure, reliable data movement without exposing the file server directly to the internet.

**Architecture:**
```
Server1 (On-Premises)
    â†“
Self-Hosted Integration Runtime
    â†“
Azure Data Factory Pipeline
    â†“
store1 (Azure Blob Storage)
```

**Implementation Steps:**
1. Install Self-hosted Integration Runtime on Server1 (or a machine with access to it)
2. Create linked service for on-premises file system
3. Create linked service for Azure Blob Storage (store1)
4. Create copy pipeline with source and destination datasets
5. Schedule or trigger pipeline execution

---

#### âŒ Incorrect Options

##### âŒ Option 3: Azure Analysis Services On-Premises Data Gateway

**Why This Is Wrong:**

Azure Analysis Services On-premises Data Gateway is designed for **live query connections** between on-premises data sources and Azure Analysis Services for business intelligence and analytics workloads.

**Key Points:**
- âŒ **Not a Data Transfer Tool**: Designed for query passthrough, not file copying
- âŒ **Purpose Mismatch**: Used for connecting tabular models to on-premises databases
- âŒ **No File Support**: Works with databases (SQL Server, Oracle, etc.), not file systems
- âŒ **Different Use Case**: Enables BI reports to query on-premises data in real-time

**What It Actually Does:**
Allows Azure Analysis Services to query on-premises databases (e.g., SQL Server) for live data refresh in BI models. It does NOT copy or move files.

---

##### âŒ Option 4: Azure Batch Account

**Why This Is Wrong:**

Azure Batch is designed for **large-scale parallel compute jobs** and high-performance computing (HPC) workloads, not data movement.

**Key Points:**
- âŒ **Not a Data Movement Tool**: Designed for running compute-intensive applications
- âŒ **Wrong Purpose**: Used for parallel job execution (rendering, simulations, data processing)
- âŒ **No Direct File Transfer**: Does not provide native support for transferring files from on-premises to Blob Storage
- âŒ **Requires Custom Code**: Would need significant custom development to use for file transfer

**What It Actually Does:**
Executes large-scale parallel and batch compute jobs across a pool of virtual machines. While Batch jobs can read/write from Blob Storage, it's not designed or optimized for simple file transfers.

**Example Use Cases:**
- Video rendering
- Financial risk modeling
- Scientific simulations
- Data processing pipelines

---

##### âŒ Option 5: Azure Logic Apps Integration Account

**Why This Is Wrong:**

Azure Logic Apps Integration Account is specifically designed for **B2B (Business-to-Business) integrations** with support for EDI, XML, and AS2 protocols.

**Key Points:**
- âŒ **Not for Bulk File Transfer**: Designed for enterprise integration patterns (EDI, AS2, X12, EDIFACT)
- âŒ **Different Purpose**: Used for B2B message exchanges between trading partners
- âŒ **Requires Logic Apps**: Integration accounts are add-ons for Logic Apps, not standalone data transfer services
- âŒ **Overkill for Simple Copy**: Not designed for simple file server to blob storage scenarios

**What It Actually Does:**
Provides artifacts (schemas, maps, agreements, certificates) for B2B integration scenarios in Logic Apps. Used when exchanging EDI messages or XML documents with external partners.

**When You Would Use It:**
- Trading partner EDI document exchange (X12, EDIFACT)
- XML transformations with XSLT maps
- AS2 protocol message exchange
- B2B certificate management

**Note:** While Logic Apps (without integration account) CAN copy files, it's not listed as an option and would be less efficient than ADF or Import/Export for 500 GB.

---

#### Key Takeaways

| Service | Valid for 500 GB File Transfer? | Primary Purpose |
|---------|----------------------------------|-----------------|
| **Azure Import/Export** | âœ… Yes | Physical disk-based bulk data transfer |
| **Azure Data Factory** | âœ… Yes | Automated cloud-based ETL and data movement |
| **Analysis Services Gateway** | âŒ No | Live query connections for BI |
| **Azure Batch** | âŒ No | Parallel compute job execution |
| **Logic Apps Integration Account** | âŒ No | B2B/EDI enterprise integrations |

---

#### Exam Strategy

**For the Exam, Remember:**

ğŸ¯ **Data Transfer Services:**
- Azure Import/Export: Physical disk shipment for bulk data
- Azure Data Factory: Automated, scheduled data pipelines
- AzCopy: Command-line tool for network-based transfers

ğŸ¯ **NOT Data Transfer Services:**
- Analysis Services Gateway: BI live queries only
- Azure Batch: Compute jobs, not data movement
- Logic Apps Integration Account: B2B/EDI scenarios

ğŸ¯ **Decision Factors:**
- **Large volume + Limited bandwidth** â†’ Import/Export
- **Automation + Scheduling needed** â†’ Data Factory
- **One-time quick transfer** â†’ AzCopy
- **B2B integrations** â†’ Logic Apps Integration Account (but NOT for file transfers)
- **Live BI queries** â†’ Analysis Services Gateway

---

## Best Practices

### Planning Your Data Transfer

1. **Assess Data Volume and Frequency**
   - One-time bulk migration â†’ Import/Export or AzCopy
   - Ongoing synchronization â†’ Azure Data Factory
   - Regular scheduled backups â†’ Azure Data Factory

2. **Evaluate Network Bandwidth**
   - Limited/expensive bandwidth â†’ Import/Export
   - Sufficient bandwidth â†’ ADF or AzCopy

3. **Consider Automation Requirements**
   - Need scheduling/monitoring â†’ Azure Data Factory
   - Manual/scripted transfers â†’ AzCopy

4. **Budget Considerations**
   - Compare network egress costs vs. Import/Export fees
   - Factor in time-to-cloud and business requirements

5. **Security and Compliance**
   - Data encryption in transit (ADF, AzCopy: TLS)
   - Data encryption at rest (Import/Export: BitLocker)
   - Network isolation (ADF with private endpoints)

### Performance Optimization

#### Azure Data Factory
- Use appropriate Data Integration Units (DIUs)
- Enable parallel copies
- Optimize file sizes (avoid many small files)
- Use staging when needed

#### AzCopy
- Use `--block-size-mb` for large files
- Adjust `--cap-mbps` to control bandwidth usage
- Enable `--recursive` for directory copies
- Use `--overwrite=false` to skip existing files

#### Import/Export
- Use multiple drives for faster processing at Azure
- Optimize data organization before preparing drives
- Use SATA II/III or SSD drives for faster copying

---

## References

### Microsoft Learn Documentation

- [Azure Import/Export Service Overview](https://learn.microsoft.com/en-us/azure/import-export/storage-import-export-service)
- [Azure Data Factory Introduction](https://learn.microsoft.com/en-us/azure/data-factory/introduction)
- [Azure Data Factory Copy Activity](https://learn.microsoft.com/en-us/azure/data-factory/copy-data-tool)
- [Get Started with AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)
- [Azure Storage Mover Overview](https://learn.microsoft.com/en-us/azure/storage-mover/service-overview)

### Related Topics

- [Azure Storage Redundancy Options](./01-azure-storage-redundancy-options.md)
- [Azure Storage Secure Access](./02-azure-storage-secure-access.md)
- [Azure Blob Storage API](./03-azure-blob-storage-api.md)
- [Azure Data Lake Storage Gen2 Migration](./08-azure-data-lake-storage-gen2-migration.md)

---

**Domain**: Design data storage solutions  
**Last Updated**: December 2025
